<head><title>Backendjs Documentation</title><link rel="shortcut icon" href="img/logo.png" type="image/png" /><link rel="icon" href="img/logo.png" type="image/png" /><link href="css/font-awesome.css" rel="stylesheet"><script src="js/jquery.js"></script><link href="css/bootstrap.css" rel="stylesheet"><script src="js/bootstrap.js"></script><link rel="stylesheet" href="css/doc.css"></head><body class="pages">
<div class="container"><nav class="navbar"><div class="container-fluid"><div class="navbar-header"><button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar"><span class="sr-only">Toggle navigation</span><span class="icon-bar"></span><span class="icon-bar"></span><span class="icon-bar"></span></button><a href="/"><img class="logo" style="max-height:30px;" src="img/logo.png"></a><span>Backendjs Documentation</span></div><div id="navbar" class="navbar-collapse collapse"><ul class="nav navbar-nav navbar-right"><li><a href="/"><span class="fa fa-gears fa-fw"></span> Home</a></li><li><a href="http://github.com/vseryakov/backendjs"><span class="fa fa-github fa-fw"></span> Github</a><li></ul></div></div></nav><h1 id="backendjs-documentation">Backendjs Documentation</h1>
<p>##Table of contents</p>
<ul>
<li><a href="#backend-library-for-node-js"> Backend library for Node.js</a></li>
<li><a href="#installation"> Installation</a></li>
<li><a href="#quick-start"> Quick start</a><ul>
<li><a href="#to-run-an-example"> To run an example</a></li>
</ul>
</li>
<li><a href="#configuration"> Configuration</a></li>
<li><a href="#backend-runtime"> Backend runtime</a></li>
<li><a href="#application-structure"> Application structure</a><ul>
<li><a href="#modules"> Modules</a></li>
<li><a href="#npm-packages-as-modules"> NPM packages as modules</a></li>
</ul>
</li>
<li><a href="#database-schema-definition"> Database schema definition</a></li>
<li><a href="#api-requests-handling"> API requests handling</a></li>
<li><a href="#example-of-todo-application"> Example of TODO application</a></li>
<li><a href="#backend-directory-structure"> Backend directory structure</a></li>
<li><a href="#cache-configurations"> Cache configurations</a><ul>
<li><a href="#local"> Local</a></li>
<li><a href="#memcached"> memcached</a></li>
<li><a href="#redis"> Redis</a></li>
<li><a href="#redis-sentinel"> Redis Sentinel</a></li>
</ul>
</li>
<li><a href="#pub-sub-or-queue-configurations"> PUB/SUB or Queue configurations</a><ul>
<li><a href="#redis"> Redis</a></li>
<li><a href="#redis-queue"> Redis Queue</a></li>
<li><a href="#rabbitmq"> RabbitMQ</a></li>
<li><a href="#db"> DB</a></li>
<li><a href="#sqs"> SQS</a></li>
<li><a href="#local"> Local</a></li>
</ul>
</li>
<li><a href="#security-configurations"> Security configurations</a><ul>
<li><a href="#api-only"> API only</a></li>
<li><a href="#secure-web-site-client-verification"> Secure Web site, client verification</a></li>
<li><a href="#secure-web-site-backend-verification"> Secure Web site, backend verification</a></li>
</ul>
</li>
<li><a href="#websockets-connections"> WebSockets connections</a></li>
<li><a href="#versioning"> Versioning</a></li>
<li><a href="#the-backend-provisioning-utility-bkjs"> The backend provisioning utility: bkjs</a></li>
<li><a href="#web-development-notes"> Web development notes</a></li>
<li><a href="#deployment-use-cases"> Deployment use cases</a><ul>
<li><a href="#aws-instance-setup-with-node-and-backendjs"> AWS instance setup with node and backendjs</a></li>
<li><a href="#aws-instance-as-an-appliance"> AWS instance as an appliance</a></li>
<li><a href="#aws-beanstalk-deployment"> AWS Beanstalk deployment</a></li>
<li><a href="#aws-provisioning-examples"> AWS Provisioning examples</a></li>
<li><a href="#make-an-ami"> Make an AMI</a></li>
<li><a href="#launch-instances-when-not-using-autoscaling-groups"> Launch instances when not using AutoScaling Groups</a></li>
<li><a href="#launch-configurations"> Launch Configurations</a></li>
<li><a href="#copy-autoscaling-launch-configs-after-new-ami-is-created"> Copy Autoscaling launch configs after new AMI is created</a></li>
<li><a href="#update-route53-with-all-ips-from-running-instances"> Update Route53 with all IPs from running instances</a></li>
<li><a href="#proxy-mode"> Proxy mode</a></li>
<li><a href="#configure-http-port"> Configure HTTP port</a></li>
</ul>
</li>
<li><a href="#backend-library-development-mac-os-x-developers-"> Backend library development (Mac OS X, developers)</a></li>
<li><a href="#design-considerations"> Design considerations</a></li>
<li><a href="#api-endpoints-provided-by-the-backend"> API endpoints provided by the backend</a><ul>
<li><a href="#authentication-and-sessions"> Authentication and sessions</a></li>
<li><a href="#signature"> Signature</a></li>
<li><a href="#authentication-api"> Authentication API</a></li>
<li><a href="#accounts"> Accounts</a></li>
<li><a href="#health-enquiry"> Health enquiry</a></li>
<li><a href="#public-images-endpoint"> Public Images endpoint</a></li>
<li><a href="#icons"> Icons</a></li>
<li><a href="#file-api"> File API</a></li>
<li><a href="#connections"> Connections</a></li>
<li><a href="#locations"> Locations</a></li>
<li><a href="#messages"> Messages</a></li>
<li><a href="#counters"> Counters</a></li>
<li><a href="#data"> Data</a></li>
<li><a href="#pages"> Pages</a></li>
<li><a href="#system-api"> System API</a></li>
</ul>
</li>
<li><a href="#author"> Author</a></li>
<li><a href="#configuration-parameters">Configuration parameters</a></li>
<li>Javascript API functions<ul>
<li><a href="#module-api">api</a></li>
<li><a href="#module-api_accounts">api_accounts</a></li>
<li><a href="#module-api_auth">api_auth</a></li>
<li><a href="#module-api_files">api_files</a></li>
<li><a href="#module-api_icons">api_icons</a></li>
<li><a href="#module-api_proxy">api_proxy</a></li>
<li><a href="#module-api_statistics">api_statistics</a></li>
<li><a href="#module-app">app</a></li>
<li><a href="#module-aws">aws</a></li>
<li><a href="#module-aws_dynamodb">aws_dynamodb</a></li>
<li><a href="#module-aws_dynamodbstreams">aws_dynamodbstreams</a></li>
<li><a href="#module-aws_ec2">aws_ec2</a></li>
<li><a href="#module-aws_s3">aws_s3</a></li>
<li><a href="#module-aws_sns">aws_sns</a></li>
<li><a href="#module-core">core</a></li>
<li><a href="#module-core_utils">core_utils</a></li>
<li><a href="#module-db">db</a></li>
<li><a href="#module-db_dynamodb">db_dynamodb</a></li>
<li><a href="#module-db_elasticsearch">db_elasticsearch</a></li>
<li><a href="#module-db_sql">db_sql</a></li>
<li><a href="#module-db_sqlite">db_sqlite</a></li>
<li><a href="#module-http_get">http_get</a></li>
<li><a href="#module-index">index</a></li>
<li><a href="#module-ipc">ipc</a></li>
<li><a href="#module-ipc_client">ipc_client</a></li>
<li><a href="#module-ipc_local">ipc_local</a></li>
<li><a href="#module-ipc_memcache">ipc_memcache</a></li>
<li><a href="#module-ipc_redis">ipc_redis</a></li>
<li><a href="#module-ipc_sqs">ipc_sqs</a></li>
<li><a href="#module-jobs">jobs</a></li>
<li><a href="#module-lib">lib</a></li>
<li><a href="#module-logger">logger</a></li>
<li><a href="#module-metrics">metrics</a></li>
<li><a href="#module-msg">msg</a></li>
<li><a href="#module-msg_apn">msg_apn</a></li>
<li><a href="#module-msg_fcm">msg_fcm</a></li>
<li><a href="#module-msg_gcm">msg_gcm</a></li>
<li><a href="#module-server">server</a></li>
<li><a href="#module-bk_account">bk_account</a></li>
<li><a href="#module-bk_data">bk_data</a></li>
<li><a href="#module-bk_dynamodbstreams">bk_dynamodbstreams</a></li>
<li><a href="#module-bk_file">bk_file</a></li>
<li><a href="#module-bk_icon">bk_icon</a></li>
<li><a href="#module-bk_message">bk_message</a></li>
<li><a href="#module-bk_shell">bk_shell</a></li>
<li><a href="#module-bk_shell_aws">bk_shell_aws</a></li>
<li><a href="#module-bk_shell_db">bk_shell_db</a></li>
<li><a href="#module-bk_shell_test">bk_shell_test</a></li>
<li><a href="#module-bk_status">bk_status</a></li>
<li><a href="#module-bk_system">bk_system</a></li>
<li><a href="#module-db_cassandra">db_cassandra</a></li>
<li><a href="#module-db_couchdb">db_couchdb</a></li>
<li><a href="#module-db_lmdb">db_lmdb</a></li>
<li><a href="#module-db_mongodb">db_mongodb</a></li>
<li><a href="#module-db_mysql">db_mysql</a></li>
<li><a href="#module-db_pgsql">db_pgsql</a></li>
<li><a href="#module-db_redis">db_redis</a></li>
<li><a href="#module-db_riak">db_riak</a></li>
<li><a href="#module-ipc_amqp">ipc_amqp</a></li>
<li><a href="#module-ipc_db">ipc_db</a></li>
<li><a href="#module-ipc_hazelcast">ipc_hazelcast</a></li>
<li><a href="#module-msg_sns">msg_sns</a></li>
</ul>
</li>
</ul>
<h1 id="backend-library-for-node-js">Backend library for Node.js</h1>
<p>General purpose backend library. The primary goal is to have a scalable platform for running and managing Node.js
servers for Web services implementation.</p>
<p>This project only covers the lower portion of the Web services ecosystem:
Node.js processes, HTTP servers, basic API functionality, database access, caching, messaging between processes,
metrics and monitoring, a library of tools for developing Node.js servers.</p>
<p>For the UI and presentation layer there are no restrictions what to use as long as it can run on top of the Express server.</p>
<p>Features:</p>
<ul>
<li>Exposes a set of Web service APIs over HTTP(S) using Express framework.</li>
<li>Database API supports SQLite, PostgreSQL, MySQL, DynamoDB, Cassandra, MongoDB, Redis with all basic operations behaving the
same way allowing you to switch databases without changing the code.</li>
<li>Database drivers for LevelDB, LMDB, CouchDB, Riak, ElasticSearch support only a subset of all database operations</li>
<li>Database operations (Get, Put, Del, Update, Select) for all supported databases using the same DB API.</li>
<li>DynamoDB Streams processing in background worker processes</li>
<li>Easily extensible to support any kind of database, provides a database driver on top of Redis with all supported methods as an example.</li>
<li>Provides accounts, connections, locations, messaging and icons APIs with basic functionality for a quick start.</li>
<li>Supports crontab and queue job processing by separate worker processes.</li>
<li>Authentication is based on signed requests using API key and secret, similar to Amazon AWS signing requests.</li>
<li>Runs web server as separate processes to utilize multiple CPU cores.</li>
<li>Supports WebSockets connections and process them with the same Express routes as HTTP requests</li>
<li>Supports several cache modes(Redis, Memcache, Hazelcast, LRU) for the database operations, multiple hosts support
in the clients for failover.</li>
<li>Supports several PUB/SUB modes of operations using Redis, RabbitMQ, Hazelcast.</li>
<li>Supports async jobs processing using several work queue implementations on top of SQS, Redis, DB, RabbitMQ, Hazelcast.</li>
<li>ImageMagick as a separate C++ module for in-process image scaling, see bkjs-wand on NPM.</li>
<li>REPL (command line) interface for debugging and looking into server internals.</li>
<li>Supports push notifications for mobile devices, APN and GCM/FCM.</li>
<li>Supports HTTP(S) reverse proxy mode where multiple Web workers are load-balanced by the proxy
server running in the master process instead of relying on the OS scheduling between processes listening on the same port.</li>
<li>Can be used with any MVC, MVVC or other types of frameworks that work on top of, or with, the Express server.</li>
<li>AWS support is very well integrated including EC2, S3, DynamoDB, SQS and more.</li>
<li>Includes simple log watcher to monitor the log files including system errors.</li>
<li>Supports i18n hooks for request/response objects, easily overriden with any real i18n implementation.</li>
<li>Integrated very light unit testing facility which can be used to test modules and API requests</li>
<li>Support runtime metrics about the timing on database, requests, cache, memory and request rate limit control</li>
<li>Hosted on <a href="https://github.com/vseryakov/backendjs">github</a>, BSD licensed.</li>
</ul>
<p>Check out the <a href="http://bkjs.io">Documentation</a> for more details.</p>
<h1 id="installation">Installation</h1>
<p>To install the module with all optional dependencies if they are available in the system</p>
<pre><code>npm install backendjs</code></pre><p>This may take some time because of downloading and compiling required dependencies like ImageMagick. They are not required in all
applications but still part of the core of the system to be available once needed.</p>
<p>To install from the git</p>
<pre><code> npm install git+https://github.com/vseryakov/backendjs.git</code></pre><p>or simply</p>
<pre><code> npm install vseryakov/backendjs</code></pre><h1 id="quick-start">Quick start</h1>
<ul>
<li><p>Simplest way of using the backendjs, it will start the server listening on port 8000</p>
<pre><code>  $ node
  &gt; var bkjs = require(&#39;backendjs&#39;)
  &gt; bkjs.server.start()</code></pre></li>
<li><p>Access is allowed only with valid signature except urls that are explicitly allowed without it (see <code>api-allow</code> config parameter below)</p>
</li>
<li><p>Same but using the helper tool, by default it will use embedded SQLite database and listen on port 8000.</p>
<pre><code>  bkjs web</code></pre></li>
<li><p>To start the server and connect to the DynamoDB (command line parameters can be saved in the <code>etc/config file</code>, see below about config files)</p>
<pre><code>  bkjs web -db-pool dynamodb -db-dynamodb-pool default -aws-key XXXX -aws-secret XXXX</code></pre></li>
<li><p>If running on EC2 instance with IAM profile then no need to specify AWS credentials:</p>
<pre><code>  bkjs web -db-pool dynamodb -db-dynamodb-pool default</code></pre></li>
<li><p>or to the PostgreSQL server, database backend</p>
<pre><code>  bkjs web -db-pool pgsql -db-pgsql-pool postgresql://postgres@127.0.0.1/backend</code></pre></li>
<li><p>All commands above will behave exactly the same</p>
</li>
<li><p><strong>Tables are not created by default</strong>, in order to initialize the database, run the server or the shell with <code>-db-create-tables</code> flag,
it is called only inside a master process, a worker never creates tables on start</p>
<ul>
<li><p>prepare the tables in the shell</p>
<pre><code>bksh -db-pool pgsql -db-pgsql-pool postgresql://postgres@127.0.0.1/backend -db-create-tables</code></pre></li>
<li><p>run the server and create tables on start</p>
<pre><code>bkjs web -db-pool pgsql -db-pgsql-pool postgresql://postgres@127.0.0.1/backend -db-create-tables</code></pre></li>
</ul>
</li>
<li><p>While the local backendjs is runnning, the documentation is always available at <a href="http://localhost:8000/doc.html">http://localhost:8000/doc.html</a> (or whatever port is the server using)</p>
</li>
<li><p>To add users from the command line</p>
<pre><code>  bksh -account-add login test secret test name TestUser email test@test.com</code></pre></li>
<li><p>By default no external modules are loaded so it needs the accounts module with a
parameter <code>-allow-modules PATTERN</code>, this will load all modules that match the pattern, default modules start with <code>bk_</code>:</p>
<pre><code>  bkjs web -allow-modules bk_</code></pre></li>
<li><p>To start Node.js shell with backendjs loaded and initialized, all command line parameters apply to the shell as well</p>
<pre><code>  ./app.sh -shell</code></pre></li>
<li><p>To access the database while in the shell</p>
<pre><code>  &gt; db.select(&quot;bk_account&quot;, {}, function(err, rows, info) { console.log(err, rows) });
  &gt; db.select(&quot;bk_account&quot;, {}, lib.log);
  &gt; db.add(&quot;bk_account&quot;, { login: &#39;test2&#39;, secret: &#39;test2&#39;, name&#39; Test 2 name&#39;, gender: &#39;f&#39; }, lib.log);
  &gt; db.select(&quot;bk_account&quot;, { gender: &#39;m&#39; }, lib.log);</code></pre></li>
</ul>
<h2 id="to-run-an-example">To run an example</h2>
<ul>
<li><p>Go to <code>examples/api</code> directory:</p>
</li>
<li><p>Run the application, it will start the Web server on port 8000:</p>
<pre><code>  ./app.sh</code></pre></li>
<li><p>Now log in with the new account,</p>
</li>
<li><p>Go to <a href="http://localhost:8000/api.html">http://localhost:8000/api.html</a> and click on <em>Login</em> at the top-right corner, then enter &#39;test&#39; as login and &#39;test&#39; as secret in the login popup dialog.</p>
</li>
<li><p>To see your account details run the command in the console <code>/account/get</code></p>
</li>
<li><p>To see current metrics run the command in the console <code>/system/stats/get</code></p>
</li>
<li><p>To see charts about accumulated metrics go to <a href="http://localhost:8000/metrics.html">http://localhost:8000/metrics.html</a></p>
</li>
<li><p>When the web server is started with <code>-watch</code> parameters any change in the source files will make the server restart automatically
letting you focus on the source code and not server management, this mode is only enabled by default in development mode,
check <code>app.sh</code> for parameters before running it in production.</p>
</li>
</ul>
<h1 id="configuration">Configuration</h1>
<p>Almost everything in the backend is configurable using config files, a config database or DNS.
The whole principle behind it is that once deployed in production, even quick restarts are impossible to do so
there should be a way to push config changes to the processes without restarting.</p>
<p>Every module defines a set of config parameters that defines the behavior of the code, due to the single threaded
nature of the Node.js. It is simple to update any config parameter to a new value so the code can operate differently.
To achieve this the code must be written in a special way, like driven by configuration which can be changed at
any time.</p>
<p>All configuration goes through the configuration process that checks all inputs and produces valid output which
is applied to the module variables. Config file or database table with configuration can be loaded on demand or
periodically, for example all local config files are watched for modification and reloaded automatically, the
config database is loaded periodically which is defined by another config parameter.</p>
<h1 id="backend-runtime">Backend runtime</h1>
<p>When the backendjs server starts it spawns several processes that perform different tasks.</p>
<p>There are 2 major tasks of the backend that can be run at the same time or in any combination:</p>
<ul>
<li>a Web server (server) with Web workers (web)</li>
<li>a job scheduler (master)</li>
</ul>
<p>These features can be run standalone or under the guard of the monitor which tracks all running processes and restarted any failed ones.</p>
<p>This is the typical output from the ps command on Linux server:</p>
<pre><code>ec2-user    891  0.0  0.6 1071632 49504 ?  Ssl  14:33   0:01 bkjs: monitor
ec2-user    899  0.0  0.6 1073844 52892 ?  Sl   14:33   0:01 bkjs: master
ec2-user    908  0.0  0.8 1081020 68780 ?  Sl   14:33   0:02 bkjs: server
ec2-user    917  0.0  0.7 1072820 59008 ?  Sl   14:33   0:01 bkjs: web
ec2-user    919  0.0  0.7 1072820 60792 ?  Sl   14:33   0:02 bkjs: web
ec2-user    921  0.0  0.7 1072120 40721 ?  Sl   14:33   0:02 bkjs: worker</code></pre><p>To enable any task a command line parameter must be provided, it cannot be specified in the config file. The <code>bkjs</code> utility supports several
commands that simplify running the backend in different modes.</p>
<ul>
<li><code>bkjs start</code> - this command is supposed to be run at the server startup as a service, it runs in the background and the monitors all tasks,
 the env variable <code>BKJS_SERVER</code> can be set in the profile to one of the <code>master or monitor</code> to define which run mode to use, default mode is monitor</li>
<li><code>bkjs monitor</code> - this command is supposed to be run at the server startup, it runs in the background and the monitors all processes,
 the command line parameters are: <code>-daemon -monitor -master -syslog</code></li>
<li><code>bkjs master</code> - this command is supposed to be run at the server startup, it runs in the background and the monitors all processes,
 the command line parameters are: <code>-daemon -monitor -master -syslog</code></li>
<li><code>bkjs watch</code> - runs the master and Web server in wather mode checking all source files for changes, this is the common command to be used
 in development, it passes the command line switches: <code>-watch -master</code></li>
<li><code>bkjs web</code> - this command runs just web server process.</li>
<li><code>bkjs run</code> - this command runs without other parameters, all additional parameters can be added in the command line, this command
 is a barebone helper to be used with any other custom settings.</li>
<li><code>bkjs shell</code> or <code>bksh</code> - start backendjs shell, no API or Web server is initialized, only the database pools</li>
</ul>
<h1 id="application-structure">Application structure</h1>
<p>The main purpose of the backendjs is to provide API to access the data, the data can be stored in the database or some other way
but the access to that data will be over HTTP and returned back as JSON. This is default functionality but any custom application
may return data in whatever format is required.</p>
<p>Basically the backendjs is a Web server with ability to perform data processing using local or remote jobs which can be scheduled similar to Unix cron.</p>
<p>The principle behind the system is that nowadays the API services just return data which Web apps or mobiles apps can render to
the user without the backend involved. It does not mean this is simple gateway between the database, in many cases it is but if special
processing of the data is needed before sending it to the user, it is possible to do and backendjs provides many convenient helpers and tools for it.</p>
<p>When the API layer is initialized, the api module contains <code>app</code> object which is an Express server.</p>
<p>Special module/namespace <code>app</code> is designated to be used for application development/extension. This module is available in the same way as <code>api</code> and <code>core</code>
which makes it easy to refer and extend with additional methods and structures.</p>
<p>The typical structure of a backendjs application is the following:</p>
<pre><code class="language-javascript">    var bkjs = require(&#39;backendjs&#39;);
    var api = bkjs.api;
    var app = bkjs.app;
    var db = bkjs.db;

    app.listArg = [];

    // Define the module config parameters
    core.describeArgs(&#39;app&#39;, [
        { name: &quot;list-arg&quot;, array: 1, type: &quot;list&quot;, descr: &quot;List of words&quot; },
        { name: &quot;int-arg&quot;, type: &quot;int&quot;, descr: &quot;An integer parameter&quot; },
     ]);

    // Describe the tables or data models, all DB pools will use it, the master or shell
    // process only creates new tables, workers just use the existing tables
    db.describeTables({
         ...
    });

     // Optionally customize the Express environment, setup MVC routes or else, `api.app` is the Express server
    app.configureMiddleware = function(options, callback)
    {
       ...
       callback()
    }

    // Register API endpoints, i.e. url callbacks
    app.configureWeb = function(options, callback)
    {
        api.app.get(&#39;/some/api/endpoint&#39;, function(req, res) {
          // to return an error, the message will be translated with internal i18n module if locales
          // are loaded and the request requires it
          api.sendReply(res, err);
          // or with custom status and message, explicitely translated
          api.sendReply(res, 404, res.__(&quot;not found&quot;));

          // with config check
          if (app.intArg &gt; 5) ...
          if (app.listArg.indexOf(req.query.name) &gt; -1) ...

          // to send data back with optional postprocessing hooks
          api.sendJSON(req, err, data);
          // or simply
          res.json(data);
        });
        ...
        callback();
    }

    // Optionally register post processing of the returned data from the default calls
    api.registerPostProcess(&#39;&#39;, /^\/account\/([a-z\/]+)$/, function(req, res, rows) { ... });
     ...

    // Optionally register access permissions callbacks
    api.registerAccessCheck(&#39;&#39;, /^\/test\/list$/, function(req, status, callback) { ...  });
    api.registerPreProcess(&#39;&#39;, /^\/test\/list$/, function(req, status, callback) { ...  });
     ...
    bkjs.server.start();</code></pre>
<p>Except the <code>app.configureWeb</code> and <code>server.start()</code> all other functions are optional, they are here for the sake of completeness of the example. Also
because running the backend involves more than just running web server many things can be setup using the configuration options like common access permissions,
configuration of the cron jobs so the amount of code to be written to have fully functioning production API server is not that much, basically only
request endpoint callbacks must be provided in the application.</p>
<p>As with any Node.js application, node modules are the way to build and extend the functionality, backendjs does not restrict how
the application is structured.</p>
<h2 id="modules">Modules</h2>
<p>Another way to add functionality to the backend is via external modules specific to the backend, these modules are loaded on startup from the backend
home subdirectory <code>modules/</code> and from the backendjs package directory for core modules. The format is the same as for regular Node.js modules and
only top level .js files are loaded on the backend startup.</p>
<p><em>By default no modules are loaded except <code>bk_accounts|bk_icons</code>, it must be configured by the <code>-allow-modules</code> config parameter.</em></p>
<p>The modules are managed per process role, by default <code>server</code> and <code>master</code> processes do not load any modules at all to keep them
small and because they monitor workers the less code they have the better.</p>
<p>The shell process loads all modules, it is configured with <code>.+</code>.</p>
<p>To enable any module to be loaded in any process it can be configured by using a role in the config parameter:</p>
<pre><code>  // Global modules except server and master
  -allow-modules &#39;.+&#39;

  // Master modules
  -allow-modules-master &#39;bk_accounts|bk_debug&#39;</code></pre><p>Once loaded they have the same access to the backend as the rest of the code, the only difference is that they reside in the backend home and
can be shipped regardless of the npm, node modules and other env setup. These modules are exposed in the <code>core.modules</code> the same way as all other core submodules
methods.</p>
<p>Let&#39;s assume the modules/ contains file facebook.js which implements custom FB logic:</p>
<pre><code class="language-javascript">     var bkjs = require(&quot;backendjs&quot;);
     var fb = {
     }
     module.exports = fb;

     fb.configureWeb = function(options, callback) {
       ...
     }

     fb.makeRequest = function(options, callback) {
       ...
     }</code></pre>
<p>This is the main app code:</p>
<pre><code class="language-javascript">    var bkjs = require(&quot;backendjs&quot;);
    var core = bkjs.core;

    // Using facebook module in the main app
    api.app.get(&quot;some url&quot;, function(req, res) {

       core.modules.facebook.makeRequest({}, function(err, data) {
          ...
       });
    });

    bkj.server.start()</code></pre>
<h2 id="npm-packages-as-modules">NPM packages as modules</h2>
<p>In case different modules is better keep separately for maintenance or development purposes they can be split into
separate NPM packages, the structure is the same, modules must be in the modules/ folder and the package must be loadable
via require as usual. In most cases just empty index.js is enough. Such modules will not be loaded via require though but
by the backendjs <code>core.loadModule</code> machinery, the NPM packages are just keep different module directories separate from each other.</p>
<p>The config parameter <code>allow-packages</code> can be used to specify NPM package names to be loaded separated by comma, as with the default
application structure all subfolders inside each NPM package will be added to the core:</p>
<ul>
<li>modules will be loaded from the modules/ older</li>
<li>locales from the locales/ folder</li>
<li>files in the web/ folder will be added to the static search path</li>
<li>all templates from views/ folder will be used for rendering</li>
</ul>
<p>If there is a config file present as <code>etc/config</code> it will be loaded as well, this way each package can maintain its default config parameters if necessary
without touching other or global configuration. Although such config files will not be reloaded on changes, when NPM installs or updates packages it
moves files around so watching the old config is no point because the updated config file will be different.</p>
<h1 id="database-schema-definition">Database schema definition</h1>
<p>The backend support multiple databases and provides the same db layer for access. Common operations are supported and all other specific usage can be achieved by
using SQL directly or other query language supported by any particular database.
The database operations supported in the unified way provide simple actions like <code>db.get, db.put, db.update, db.del, db.select</code>. The <code>db.query</code> method provides generic
access to the database driver and executes given query directly by the db driver, it can be SQL or other driver specific query request.</p>
<p>Before the tables can be queried the schema must be defined and created, the backend db layer provides simple functions to do it:</p>
<ul>
<li>first the table needs to be described, this is achieved by creating a JavaScript object with properties describing each column, multiple tables can be described
at the same time, for example lets define album table and make sure it exists when we run our application:</li>
</ul>
<pre><code class="language-javascript">        db.describeTables({
           album: {
               id: { primary: 1 },                         // Primary key for an album
               name: { pub: 1 },                           // Album name, public column
               mtime: { type: &quot;now&quot; },                     // Modification timestamp
           },
           photo: {
               album_id: { primary: 1 },                   // Combined primary key
               id: { primary: 1 },                         // consisting of album and photo id
               name: { pub: 1, index: 1 },                 // Photo name or description, public column with the index for faster search
               mtime: { type: &quot;now&quot; }
           }
        });</code></pre>
<ul>
<li>the system will automatically create the album and photos tables, this definition must remain in the app source code
and be called on every app startup. This allows 1) to see the db schema while working with the app and 2) easily maintain it by adding new columns if
necessary, all new columns will be detected and the database tables updated accordingly. And it is all JavaScript, no need to learn one more language or syntax
to maintain database tables.</li>
</ul>
<p>Each database may restrict how the schema is defined and used, the db layer does not provide an artificial layer hiding all specifics, it just provides the same
API and syntax, for example, DynamoDB tables must have only hash primary key or combined hash and range key, so when creating table to be used with DynamoDB, only
one or two columns can be marked with primary property while for SQL databases the composite primary key can consist of more than 2 columns.</p>
<p>The backendjs always creates several tables in the configured database pools by default, these tables are required to support default API functionality and some
are required for backend operations. Refer below for the JavaScript modules documentation that described which tables are created by default. In the custom applications
the <code>db.describeTables</code> method can modify columns in the default table and add more columns if needed.</p>
<p>For example, to make age and some other columns in the accounts table public and visible by other users with additional columns the following can be
done in the <code>api.initApplication</code> method. It will extend the bk_account table and the application can use new columns the same way as the already existing columns.
Using the birthday column we make &#39;age&#39; property automatically calculated and visible in the result, this is done by the internal method <code>api.processAccountRow</code> which
is registered as post process callback for the bk_account table. The computed property <code>age</code> will be returned because it is not present in the table definition
and all properties not defined and configured are passed as is.</p>
<p>The cleanup of the public columns is done by the <code>api.sendJSON</code> which is used by all API routes when ready to send data back to the client. If any post-process
hooks are registered and return data itself then it is the hook responsibility to cleanup non-public columns.</p>
<pre><code class="language-javascript">    db.describeTables({
        bk_account: {
            gender: { pub: 1 },
            birthday: {},
            ssn: {},
            salary: { type: &quot;int&quot; },
            occupation: {},
            home_phone: {},
            work_phone: {},
        });

    app.configureWeb = function(options, callback)
    {
       db.setProcessRow(&quot;post&quot;, &quot;bk_account&quot;, this.processAccountRow);
       ...
       callback();
    }
    app.processAccountRow = function(req, row, options)
    {
       if (row.birthday) row.age = Math.floor((Date.now() - core.toDate(row.birthday))/(86400000*365));
    }</code></pre>
<p>To define tables inside a module just provide a <code>tables</code> property in the module object, it will be picked up by database initialization automatically.</p>
<pre><code class="language-javascript">var mod = {
    name: &quot;billing&quot;,
    tables: {
       invoices: {
          id: { type: &quot;int&quot;, primary: 1 },
          name: {},
          price: { type: &quot;real&quot; },
          mtime: { type: &quot;now&quot; }
       }
    }
}
module.exports = mod;

// Run db setup once all the DB pools are configured, for example produce dynamic icon property
// for each record retrieved
mod.configureModule = function(options, callback)
{
    db.setProcessRows(&quot;post&quot;, &quot;invoices&quot;, function(req, row, opts) {
       if (row.id) row.icon = &quot;/images/&quot; + row.id + &quot;.png&quot;;
    });
    callback();
}</code></pre>
<h1 id="api-requests-handling">API requests handling</h1>
<p>All methods will put input parameters in the <code>req.query</code>, GET or POST.</p>
<p>One way to verify input values is to use <code>lib.toParams</code>, only specified parameters will be returned and converted according to
the type or ignored.</p>
<p>Example:</p>
<pre><code class="language-javascript">   var params = {
      test1: { id: { type: &quot;text&quot; },
               count: { type: &quot;int&quot; },
               email: { regexp: /^[^@]+@[^@]+$/ }
      }
   };

   api.app.all(&quot;/endpoint/test1&quot;, function(req, res) {
      var query = lib.toParams(req.query, params.test1);
      ...
   });</code></pre>
<h1 id="example-of-todo-application">Example of TODO application</h1>
<p>Here is an example how to create simple TODO application using any database supported by the backend. It supports basic
operations like add/update/delete a record, show all records.</p>
<p>Create a file named <code>app.js</code> with the code below.</p>
<pre><code class="language-javascript">    var bkjs = require(&#39;backendjs&#39;);
    var api = bkjs.api;
    var lib = bkjs.lib;
    var app = bkjs.app;
    var db = bkjs.db;

    // Describe the table to store todo records
    db.describeTables({
       todo: {
           id: { type: &quot;uuid&quot;, primary: 1 },  // Store unique task id
           due: {},                           // Due date
           name: {},                          // Short task name
           descr: {},                         // Full description
           mtime: { type: &quot;now&quot; }             // Last update time in ms
       }
    });

    // API routes
    app.configureWeb = function(options, callback)
    {
        api.app.get(/^\/todo\/([a-z]+)$/, function(req, res) {
           var options = api.getOptions(req);
           switch (req.params[0]) {
             case &quot;get&quot;:
                if (!req.query.id) return api.sendReply(res, 400, &quot;id is required&quot;);
                db.get(&quot;todo&quot;, { id: req.query.id }, options, function(err, rows) { api.sendJSON(req, err, rows); });
                break;
             case &quot;select&quot;:
                options.noscan = 0; // Allow empty scan of the whole table if no query is given, disabled by default
                db.select(&quot;todo&quot;, req.query, options, function(err, rows) { api.sendJSON(req, err, rows); });
                break;
            case &quot;add&quot;:
                if (!req.query.name) return api.sendReply(res, 400, &quot;name is required&quot;);
                // By default due date is tomorrow
                if (req.query.due) req.query.due = lib.toDate(req.query.due, Date.now() + 86400000).toISOString();
                db.add(&quot;todo&quot;, req.query, options, function(err, rows) { api.sendJSON(req, err, rows); });
                break;
            case &quot;update&quot;:
                if (!req.query.id) return api.sendReply(res, 400, &quot;id is required&quot;);
                db.update(&quot;todo&quot;, req.query, options, function(err, rows) { api.sendJSON(req, err, rows); });
                break;
            case &quot;del&quot;:
                if (!req.query.id) return api.sendReply(res, 400, &quot;id is required&quot;);
                db.del(&quot;todo&quot;, { id: req.query.id }, options, function(err, rows) { api.sendJSON(req, err, rows); });
                break;
            }
        });
        callback();
     }
     bkjs.server.start();</code></pre>
<p>Now run it with an option to allow API access without an account:</p>
<pre><code>node app.js -log debug -web -api-allow-path /todo -db-create-tables</code></pre><p>To use a different database, for example PostgresSQL(running localy) or DynamoDB(assuming EC2 instance),
all config parametetrs can be stored in the etc/config as well</p>
<pre><code>node app.js -log debug -web -api-allow-path /todo -db-pool dynamodb -db-dynamodb-pool default -db-create-tables
node app.js -log debug -web -api-allow-path /todo -db-pool pgsql -db-pgsql-pool default -db-create-tables</code></pre><p>API commands can be executed in the browser or using <code>curl</code>:</p>
<pre><code>curl &#39;http://localhost:8000/todo?name=TestTask1&amp;descr=Descr1&amp;due=2015-01-01`
curl &#39;http://localhost:8000/todo/select&#39;</code></pre><h1 id="backend-directory-structure">Backend directory structure</h1>
<p>When the backend server starts and no -home argument passed in the command line the backend makes its home environment in the <code>~/.bkjs</code> directory.
It is also possible to set the default home using BKJS_HOME environment variable.</p>
<p>The backend directory structure is the following:</p>
<ul>
<li><p><code>etc</code> - configuration directory, all config files are there</p>
<ul>
<li><p><code>etc/profile</code> - shell script loaded by the bkjs utility to customize env variables</p>
</li>
<li><p><code>etc/config</code> - config parameters, same as specified in the command line but without leading -, each config parameter per line:</p>
<p>  Example:</p>
<pre><code>  debug=1
  db-pool=dynamodb
  db-dynamodb-pool=http://localhost:9000
  db-pgsql-pool=postgresql://postgres@127.0.0.1/backend

  To specify other config file: bkjs shell -config-file file</code></pre></li>
<li><p>etc/config.local - same as the config but for the cases when local environment is different than the production or for dev specific parameters</p>
</li>
<li><p>some config parameters can be configured in DNS as TXT records, the backend on startup will try to resolve such records and use the value if not empty.
All params that  marked with DNS TXT can be configured in the DNS server for the domain where the backend is running, the config parameter name is
concatenated with the domain and queried for the TXT record, for example: <code>cache-host</code> parameter will be queried for cache-host.domain.name for TXT record type.</p>
</li>
<li><p><code>etc/crontab</code> - jobs to be run with intervals, JSON file with a list of cron jobs objects:</p>
<p>  Example:</p>
<ol>
<li><p>Create file in ~/.backend/etc/crontab with the following contents:</p>
<pre><code> [ { &quot;cron&quot;: &quot;0 1 1 * * 1,3&quot;, &quot;job&quot;: { &quot;app.cleanSessions&quot;: { &quot;interval&quot;: 3600000 } } } ]</code></pre></li>
<li><p>Define the function that the cron will call with the options specified, callback must be called at the end, create this app.js file</p>
<pre><code> var bkjs = require(&quot;backendjs&quot;);
 bkjs.app.cleanSessions = function(options, callback) {
      bkjs.db.delAll(&quot;session&quot;, { mtime: options.interval + Date.now() }, { ops: &quot;le&quot; }, callback);
 }
 bkjs.server.start()</code></pre></li>
<li><p>Start the jobs queue and the web server at once</p>
<pre><code> bkjs master -jobs-workers 1 -jobs-cron</code></pre></li>
</ol>
</li>
<li><p>etc/crontab.local - additional local crontab that is read after the main one, for local or dev environment</p>
</li>
</ul>
</li>
<li><p><code>modules</code> - loadable modules with specific functionality</p>
</li>
<li><p><code>images</code> - all images to be served by the API server, every subfolder represent naming space with lots of subfolders for images</p>
</li>
<li><p><code>var</code> - database files created by the server</p>
</li>
<li><p><code>tmp</code> - temporary files</p>
</li>
<li><p><code>web</code> - Web pages served by the static Express middleware</p>
</li>
</ul>
<h1 id="cache-configurations">Cache configurations</h1>
<p>Database layer support caching of the responses using <code>db.getCached</code> call, it retrieves exactly one record from the configured cache, if no record exists it
will pull it from the database and on success will store it in the cache before returning to the client. When dealing with cached records, there is a special option
that must be passed to all put/update/del database methods in order to clear local cache, so next time the record will be retrieved with new changes from the database
and refresh the cache, that is <code>{ cached: true }</code> can be passed in the options parameter for the db methods that may modify records with cached contents. In any case
it is required to clear cache manually there is <code>db.clearCache</code> method for that.</p>
<p>Also there is a configuration option <code>-db-caching</code> to make any table automatically cached for all requests.</p>
<h2 id="local">Local</h2>
<p>If no cache is configured the local driver is used, it keeps the cache on the master process in the LRU pool and any worker or Web process
communicate with it via internal messaging provided by the <code>cluster</code> module. This works only for a single server.</p>
<h2 id="memcached">memcached</h2>
<p>Set <code>ipc-cache=memcache://HOST[:PORT]</code> that points to the host running memcached. To support multiple servers add the option
<code>ipc-cache-options-servers=10.1.1.1,10.2.2.1:5000</code>.</p>
<h2 id="redis">Redis</h2>
<p>Set <code>ipc-cache=redis://HOST[:PORT]</code> that points to the server running Redis server.</p>
<p>To support more than one master Redis server in the client add additional servers in the servers parameter,
<code>ipc-cache-options-servers=10.1.1.1,10.2.2.1:5000</code>, the client will reconnect automatically on every
disconnect. To support quick failover it needs a parameter for the <code>node-redis</code> module (which is used by the driver) <code>max_attempts</code> to be a
number how many attempts to reconnect before switching to another server like <code>ipc-cache-options-max_attempts=3</code>. If there is only one
server then it will keep reconnecting until total reconnect time exceeds the <code>connect_timeout</code> ms.
Any other <code>node-redis</code> module parameter can be passed as well.</p>
<p>Cache configurations also can be passed in the url, the system supports special parameters that start with <code>bk-</code>, it will extract them into options automatically.</p>
<p>For example:</p>
<pre><code>ipc-cache=redis://host1?bk-servers=host2,host3&amp;bk-max_attempts=3
ipc-cache-backup=redis://host2
ipc-cache-backup-options-max_attempts=3</code></pre><h2 id="redis-sentinel">Redis Sentinel</h2>
<p>To enable Redis Sentinel pass in the option <code>-sentinel-servers</code>: <code>ipc-cache=redis://host1?bk-sentinel-servers=host1,host2</code>.</p>
<p>The system will connect to the sentinel, get the master cache server and connect the cache driver to it, also it will listen constantly on
sentinel events and failover to a new master autimatically. Sentinel use the regular redis module and supports all the same
parameters, to pass options to the sentinel driver prefix them with <code>sentinel-</code>:</p>
<pre><code>ipc-cache=redis://host1?bk-servers=host2,host3&amp;bk-max_attempts=3&amp;bk-sentinel-servers=host1,host2,host3
ipc-cache-backup=redis://host2
ipc-cache-backup-options-sentinel-servers=host1,host2
ipc-cache-backup-options-sentinel-max_attempts=5</code></pre><h1 id="pub-sub-or-queue-configurations">PUB/SUB or Queue configurations</h1>
<p>Publish/subscribe functionality allows clients to receive notifications without constantly polling for new events. A client can be anything but
the backend provides some partially implemented subscription notifications for Web clients using the Long Poll.
The Account API call <code>/account/subscribe</code> can use any pub/sub mode.</p>
<p>The flow of the pub/sub operations is the following:</p>
<ul>
<li>a HTTP client makes <code>/account/subscribe</code> API request, the connection is made and is kept open indefinitely or as long as configured using <code>api-subscribe-timeout</code>.</li>
<li>the API backend receives this request, and runs the <code>api.subscribe</code> method with the key being the account id, this will subscribe to the events for the current
account and registers a callback to be called if any events occurred. The HTTP connection is kept open.</li>
<li>some other client makes an API call that triggers an event like makes a connection or sends a message, on such event the backend API handler
always runs <code>ipc.publish</code> after the DB operation succeeds. If the messaging is configured, it publishes the message for the account, the
message being a JSON object with the request API path and mtime, other properties depend on the call made.</li>
<li>the connection that initiated <code>/account/subscribe</code> receives an event</li>
</ul>
<h2 id="redis">Redis</h2>
<p>To configure the backend to use Redis for PUB/SUB messaging set <code>ipc-queue=redis://HOST</code> where HOST is IP address or hostname of the single Redis server.
This will use native PUB/SUB Redis feature.</p>
<h2 id="redis-queue">Redis Queue</h2>
<p>To configure the backend to use Redis for job processing set <code>ipc-queue=redisq://HOST</code> where HOST is IP address or hostname of the single Redis server.
This driver implements reliable Redis queue, with <code>visibilityTimeout</code> config option works similar to AWS SQS.</p>
<p>Once configured, then all calls to <code>jobs.submitJob</code> will push jobs to be executed to the Redis queue, starting somewhere a backend master
process with <code>-jobs-workers 2</code> will launch 2 worker processes which will start pulling jobs from the queue and execute.</p>
<p>An example of how to perform jobs in the API routes:</p>
<pre><code class="language-javascript">   app.processAccounts = function(options, callback) {
       db.select(&quot;bk_account&quot;, { type: options.type || &quot;user&quot; }, function(err, rows) {
          ...
          callback();
       });
   }

   api.all(&quot;/process/accounts&quot;, function(req, res) {
       jobs.submitJob({ job: { &quot;app.processAccounts&quot;: { type: req.query.type } } }, function(err) {
          api.sendReply(res, err);
       });
   });
</code></pre>
<h2 id="rabbitmq">RabbitMQ</h2>
<p>To configure the backend to use RabbitMQ for messaging set <code>ipc-queue=amqp://HOST</code> and optionally <code>amqp-options=JSON</code> with options to the amqp module.
Additional objects from the config JSON are used for specific AMQP functions: { queueParams: {}, subscribeParams: {}, publishParams: {} }. These
will be passed to the corresponding AMQP methods: <code>amqp.queue, amqp.queue.sibcribe, amqp.publish</code>. See AMQP Node.js module for more info.</p>
<h2 id="db">DB</h2>
<p>This is a simple queue implementation using the atomic UPDATE, it polls for new jobs in the table and updates the status, only who succeeds
with the update takes the job and executes it. It is not effective but can be used for simple and not busy systems for more or less long jobs.
The advantage is that it uses the same database and does not require additional servers.</p>
<h2 id="sqs">SQS</h2>
<p>To use AWS SQS for job processing set <code>ipc-queue=https://sqs.amazonaws.com....</code>, this queue system will poll SQS for new messages on a worker
and after successful execution will delete the message. For long running jobs it will automatically extend visibility timeout if it is configured.</p>
<h2 id="local">Local</h2>
<p>The local queue is implemented on the master process as a list, communication is done via local sockets between the master and workers.
This is intended for a single server development purposes only.</p>
<h1 id="security-configurations">Security configurations</h1>
<h2 id="api-only">API only</h2>
<p>This is default setup of the backend when all API requests except must provide valid signature and all HTML, JavaScript, CSS and image files
are available to everyone. This mode assumes that Web development will be based on &#39;single-page&#39; design when only data is requested from the Web server and all
rendering is done using JavaScript. This is how the <code>examples/api/api.html</code> developers console is implemented, using JQuery-UI and Knockout.js.</p>
<p>To see current default config parameters run any of the following commands:</p>
<pre><code>    bkjs bkhelp | grep api-allow

    node -e &#39;require(&quot;backendjs&quot;).core.showHelp()&#39;</code></pre><p>To enable open registration in this mode just add config parameter <code>api-allow-path=^/account/add$</code>.</p>
<h2 id="secure-web-site-client-verification">Secure Web site, client verification</h2>
<p>This is a mode when the whole Web site is secure by default, even access to the HTML files must be authenticated. In this mode the pages must defined &#39;Backend.session = true&#39;
during the initialization on every html page, it will enable Web sessions for the site and then no need to sign every API request.</p>
<p>The typical client JavaScript verification for the html page may look like this, it will redirect to login page if needed,
this assumes the default path &#39;/public&#39; still allowed without the signature:</p>
<pre><code class="language-javascript">   &lt;script src=&quot;/js/jquery.js&quot;&gt;&lt;/script&gt;
   &lt;link href=&quot;/css/bootstrap.css&quot; rel=&quot;stylesheet&quot;&gt;
   &lt;script src=&quot;/js/bootstrap.js&quot;&gt;&lt;/script&gt;
   &lt;script src=&quot;/js/knockout.js&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;
   &lt;script src=&quot;/js/crypto.js&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;
   &lt;script src=&quot;/js/bkjs.js&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;
   &lt;script src=&quot;/js/bkjs-bootstrap.js&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;
   &lt;script src=&quot;/js/bkjs-ko.js&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;
   &lt;script&gt;
    $(function () {
       Bkjs.session = true;
       $(Bkjs).on(&quot;bkjs.nologin&quot;, function() { window.location=&#39;/public/index.html&#39;; });
       Bkjs.koInit();
   });
   &lt;/script&gt;</code></pre>
<h2 id="secure-web-site-backend-verification">Secure Web site, backend verification</h2>
<p>On the backend side in your application app.js it needs more secure settings defined i.e. no html except /public will be accessible and
in case of error will be redirected to the login page by the server. Note, in the login page <code>Bkjs.session</code> must be set to true for all
html pages to work after login without singing every API request.</p>
<ol>
<li>We disable all allowed paths to the html and registration:</li>
</ol>
<pre><code class="language-javascript">   app.configureMiddleware = function(options, callback) {
      this.allow.splice(this.allow.indexOf(&#39;^/$&#39;), 1);
      this.allow.splice(this.allow.indexOf(&#39;\\.html$&#39;), 1);
      callback();
   }</code></pre>
<ol start="2">
<li>We define an auth callback in the app and redirect to login if the request has no valid signature, we check all html pages, all allowed html pages from the /public
will never end up in this callback because it is called after the signature check but allowed pages are served before that:</li>
</ol>
<pre><code class="language-javascript">   api.registerPreProcess(&#39;&#39;, /^\/$|\.html$/, function(req, status, callback) {
      if (status.status != 200) {
          status.status = 302;
          status.url = &#39;/public/index.html&#39;;
      }
      callback(status);
   });</code></pre>
<h1 id="websockets-connections">WebSockets connections</h1>
<p>The simplest way is to configure <code>ws-port</code> to the same value as the HTTP port. This will run WebSockets server along the regular Web server.
All requests must be properly signed with all parameters encoded as for GET requests.</p>
<p>Example:</p>
<pre><code>    wscat --connect ws://localhost:8000
    connected (press CTRL+C to quit)
    &gt; /account/get
    &lt; {
        &quot;status&quot;: 400,
        &quot;message&quot;: &quot;Invalid request: no host provided&quot;
      }
    &gt;</code></pre><h1 id="versioning">Versioning</h1>
<p>There is no ready to use support for different versions of API at the same because there is no just one solution that satisfies all applications. But there are
tools ready to use that will allow to implement such versioning system in the backend. Some examples are provided below:</p>
<ul>
<li><p>Fixed versions
This is similar to AWS version system when versions are fixed and changed not very often. For such cases the backend exposes <code>core.bkVersion</code> which is
supposed to be a core backend version. This version is returned with every backend response in the Server: header. A client also can specify the core version
using <code>bk-version</code> header. When a request is parsed and the version is provided it will be set in the request options object as <code>apiVersion</code>.</p>
<p>All API routes are defined using Express middleware and one of the possible ways of dealing with different versions can look like this, by
appending version to the command it is very simple to call only changed API code.</p>
</li>
</ul>
<pre><code class="language-javascript">          api.all(/\/domain\/(get|put|del)/, function(req, res) {
              var options = api.getOptions(req);
              var cmd = req.params[0];
              if (options.apiVersion) cmd += &quot;/&quot; + options.apiVersion;
              switch (cmd) {
              case &quot;get&quot;:
                  break;

              case &quot;get/2015-01-01&quot;:
                  break;

              case &quot;put&quot;:
                  break;

              case &quot;put/2015-02-01&quot;:
                  break;

              case &quot;del&quot;
                  break;
              }
          });</code></pre>
<ul>
<li><p>Application semver support
For cases when applications support Semver kind of versioning and it may be too many releases the method above still can be used while the number of versions is
small, once too many different versions with different minor/patch numbers, it is easier to support greater/less comparisons.</p>
<p>The application version <code>bk-app</code> can be supplied in the query or as a header or in the user-agent HTTP header which is the easiest case for mobile apps.
In the middlware, the code can look like this:</p>
</li>
</ul>
<pre><code class="language-javascript">        var options = api.getOptions(req);
        var version = lib.toVersion(options.appVersion);
        switch (req.params[0]) {
        case &quot;get&quot;:
            if (version &lt; lib.toVersion(&quot;1.2.5&quot;)) {
                res.json({ id: 1, name: &quot;name&quot;, description: &quot;descr&quot; });
                break;
            }
            if (version &lt; lib.toVersion(&quot;1.1&quot;)) {
                res.json([id, name]);
                break;
            }
            res.json({ id: 1, name: &quot;name&quot;, descr: &quot;descr&quot; });
            break;
        }</code></pre>
<p>The actual implementation can be modularized, split into functions, controllers.... there are no restrictions how to build the working backend code,
the backend just provides all necessary information for the middleware modules.</p>
<h1 id="the-backend-provisioning-utility-bkjs">The backend provisioning utility: bkjs</h1>
<p>The purpose of the <code>bkjs</code> shell script is to act as a helper tool in configuring and managing the backend environment
and as well to be used in operations on production systems. It is not required for the backend operations and provided as a convenience tool
which is used in the backend development and can be useful for others running or testing the backend.</p>
<p>Running without arguments will bring help screen with description of all available commands.</p>
<p>The tool is multi-command utility where the first argument is the command to be executed with optional additional arguments if needed.
On Linux, when started the bkjs tries to load and source the following config files:</p>
<pre><code>    /etc/sysconfig/bkjs
    $BKJS_HOME/etc/profile</code></pre><p>Any of the following config files can redefine any environment variable thus pointing to the correct backend environment directory or
customize the running environment, these should be regular shell scripts using bash syntax.</p>
<p>Most common used commands are:</p>
<ul>
<li><p>bkjs watch - run the backend or the app for development purposes, uses local app.js if exists otherwise runs generic server</p>
</li>
<li><p>bkjs shell - start REPL shell with the backend module loaded and available for use, all submodules are available in the shell as well like core, db, api</p>
</li>
<li><p>bkjs sync [-path path] [-host host] [-user user] - sync sources of the app with the remote site, this is for development version of the backend only</p>
</li>
<li><p>bkjs init-server [-home path] [-user user] [-host name] [-domain name] - initialize Linux instance(Amazon,CentOS) for backend use, optional -home can be specified where the backend
home will be instead of ~/.bkjs, optional -user tells to use existing user instead of the current user and not root.</p>
<p> <strong>This command will create <code>/etc/sysconfig/bkjs</code> file with BKJS_HOME set to the home of the
 backendjs app which was passed in the command line. This makes the bkjs or bksh run globally regardless of the current directory.</strong></p>
</li>
</ul>
<h1 id="web-development-notes">Web development notes</h1>
<p>The server supports simple web bundling using uglify-js utility. To enable it just add to the local config a list of directories to be
watched for changes. For example adding these lines to the local config will enable the watcher and bundle support</p>
<pre><code> watch-web=web/js,web/css,$HOME/src/js,$HOME/src/css
 watch-ignore=.bundle.(js|css)$
 build-web=bkjs web-bundle -dev</code></pre><p>Now instead of incding a bunch of .js or css files in the html pages it only needs /js/bkjs.bundle.js and /css/bkjs.bundle.css. The configuration is in the
package.json file.</p>
<p>The simple script below allows to build the bundle and refresh Chrome tab automatically, saves several clicks:</p>
<pre><code> #!/bin/bash
 bkjs web-bundle -dev -file $2
 [ &quot;$?&quot; != &quot;0&quot; ] &amp;&amp; exit
 osascript -e &quot;tell application \&quot;Google Chrome\&quot; to reload (tabs of window 1 whose URL contains \&quot;$1\&quot;)&quot;
 #osascript -e &#39;tell application &quot;Google Chrome&quot; to tell the active tab of its first window to reload&#39;</code></pre><p>To use it call this script instead in the config.local:</p>
<pre><code> build-web=web-bundle.sh /website</code></pre><h1 id="deployment-use-cases">Deployment use cases</h1>
<h2 id="aws-instance-setup-with-node-and-backendjs">AWS instance setup with node and backendjs</h2>
<p>Here is the example how to setup new custom AWS server, it is not required and completely optional but bkjs provides some helpful commands that may simplify
new image configuration.</p>
<ul>
<li><p>start new AWS instance via AWS console, use Amazon Linux</p>
</li>
<li><p>login as <code>ec2-user</code></p>
</li>
<li><p>install commands</p>
<pre><code>  yum-config-manager --enable epel
  sudo yum install npm
  npm install backendjs
  sudo bkjs init-service
  bkjs restart</code></pre></li>
<li><p>try to access the instance via HTTP port 8000 for the API console or documentation</p>
</li>
<li><p>after reboot the server will be started automatically</p>
</li>
</ul>
<h2 id="aws-instance-as-an-appliance">AWS instance as an appliance</h2>
<p>To make an API appliance by using the backendjs on the AWS instance as user ec2-user with the backend in the user home</p>
<ul>
<li><p>start new AWS instance via AWS console, use Amazon Linux or CentOS 6</p>
</li>
<li><p>login as <code>ec2-user</code></p>
</li>
<li><p>install commands</p>
<pre><code>  git clone https://github.com/vseryakov/backendjs.git
  sudo backendjs/bkjs install-ec2 -tools $(pwd)/backendjs/tools
  bkjs restart</code></pre></li>
<li><p>run <code>ps agx</code>, it should show several backend processes running</p>
</li>
<li><p>try to access the instance via HTTP port for the API console or documentation</p>
</li>
</ul>
<p>NOTE: if running behind a Load balancer and actual IP address is needed set Express option in the command line <code>-api-express-options {&quot;trust%20proxy&quot;:1}</code>. In the config file
replacing spaces with %20 is not required.</p>
<h2 id="aws-beanstalk-deployment">AWS Beanstalk deployment</h2>
<p>As with any Node.js module, the backendjs app can be packaged into zip file according to AWS docs and deployed the same way as any other Node.js app.
Inside the app package etc/config file can be setup for any external connections.</p>
<h2 id="aws-provisioning-examples">AWS Provisioning examples</h2>
<p>Note: on OS X laptop the <code>-aws-sdk-profile uc</code> when AWS credentials are in the ~/.aws/credentials.</p>
<h3 id="make-an-ami">Make an AMI</h3>
<p>On the running machine which will be used for an image:</p>
<pre><code>bksh -aws-create-image -no-reboot</code></pre><p>Use an instance by tag for an image:</p>
<pre><code>bksh -aws-create-image -no-reboot -instance-id `bkjs ec2-show -tag api -fmt id | head -1`</code></pre><h3 id="launch-instances-when-not-using-autoscaling-groups">Launch instances when not using AutoScaling Groups</h3>
<p>When launching from an EC2 instance no need to specify any AWS credentials.</p>
<ul>
<li><p>admin (EC2)</p>
<p> bksh -aws-sdk-profile uc -aws-launch-instances -aws-instance-type t2.small -subnet-name api -name admin -elb-name Admin -alarm-name alarms -public-ip 1 -dry-run</p>
</li>
<li><p>api (EC2)</p>
<p> bksh -aws-sdk-profile uc -aws-launch-instances -aws-instance-type m3.large -subnet-name api -name api -elb-name api -alarm-name alarms -public-ip 1 -dry-run</p>
</li>
<li><p>jobs (EC2)</p>
<p> bksh -aws-sdk-profile uc -aws-launch-instances -aws-instance-type t2.small -subnet-name internal -name sync -alarm-name alarms -dry-run
 bksh -aws-sdk-profile uc -aws-launch-instances -aws-instance-type t2.small -subnet-name internal -name sync -zone 1c -alarm-name alarms -dry-run</p>
</li>
<li><p>Elasticsearch</p>
<p> bksh -aws-sdk-profile uc -aws-launch-instances -aws-instance-type m3.large -subnet-name internal -name elasticsearch -bkjs-cmd stop-service -bkjs-cmd &quot;init-elasticsearch-service -memsize 50&quot; -alarm-name alarms -public-ip 1 -dry-run</p>
</li>
<li><p>Redis</p>
<p> bksh -aws-sdk-profile uc -aws-launch-instances -aws-instance-type m3.large -subnet-name internal -name redis -bkjs-cmd stop-service -bkjs-cmd &quot;init-redis-service -memsize 70&quot; -alarm-name alarms -public-ip 1 -dry-run</p>
</li>
</ul>
<h3 id="launch-configurations">Launch Configurations</h3>
<pre><code>bksh -aws-create-launch-config -config-name elasticsearch -aws-sdk-profile uc -instance-type m3.large -update-groups -bkjs-cmd stop-service -bkjs-cmd init-logwatcher -bkjs-cmd &quot;init-elasticsearch-service -memsize 50&quot; -device /dev/xvda:gp2:16 -dry-run</code></pre><h3 id="copy-autoscaling-launch-configs-after-new-ami-is-created">Copy Autoscaling launch configs after new AMI is created</h3>
<pre><code>bksh -aws-create-launch-config -config-name jobs -aws-sdk-profile uc -update-groups -dry-run
bksh -aws-create-launch-config -config-name api -aws-sdk-profile uc -update-groups -dry-run</code></pre><h3 id="update-route53-with-all-ips-from-running-instances">Update Route53 with all IPs from running instances</h3>
<pre><code>bksh -aws-set-route53 -name elasticsearch.ec-internal -filter elasticsearch</code></pre><h2 id="proxy-mode">Proxy mode</h2>
<p>By default the Web proceses spawned by the server are load balanced using default cluster module which relies on the OS to do scheduling. On Linux with node 0.10
this is proven not to work properly due to the kernel keeping the context switches to a minimum thus resulting in one process to be very busy while the others
idle. Node versions 4 and above perform round-robin by default.</p>
<p>For such case the Backendjs implements the proxy mode by setting <code>proxy-port</code> config parameter to any number above 1000, this will be the initial
port for the web processes to listen for incoming requests, for example if use <code>-proxy-port 3000</code> and launch 2 web processes they will listen on ports
3000 and 3001. The main server process will start internal HTTP proxy and will perform round-robin load balancing the incoming requests between the web processes by forwarding
them to the web processes over TCP and then returning the responses back to the clients.</p>
<h2 id="configure-http-port">Configure HTTP port</h2>
<p>The first thing when deploying the backend into production is to change API HTTP port, by default is is 8000, but we would want port 80 so regardless
how the environment is setup it is ultimately 2 ways to specify the port for HTTP server to use:</p>
<ul>
<li><p>config file</p>
<p>The config file is always located in the etc/ folder in the backend home directory, how the home is specified depends on the system but basically it can be
defined via command line arguments as <code>-home</code> or via environment variables when using bkjs. See bkjs documentation but on AWS instances created with bkjs
<code>init-server</code> command, for non-standard home use <code>/etc/sysconfig/bkjs</code> profile, specify <code>BKJS_HOME=/home/backend</code> there and the rest will be taken care of</p>
</li>
<li><p>command line arguments</p>
<p>When running node scripts which use the backend, just specify <code>-home</code> command line argument with the directory where your backend should be and the backend will use it</p>
<p>Example:</p>
<pre><code>  node app.js -home $HOME -port 80</code></pre></li>
<li><p>config database</p>
<p>If <code>-db-config</code> is specified in the command line or <code>db-config=</code> in the local config file, this will trigger loading additional
config parameters from the specified database pool, it will load all records from the <code>bk_config</code> table on that db pool. Using the database to store
configuration make it easier to maintain dynamic environment for example in case of auto scaling or launching on demand, this way
a new instance will query current config from the database and this eliminates supporting text files and distributing them to all instances.</p>
<p>The config database is refreshed from time to time acording to the <code>db-config-interval</code> parameter, also all records with <code>ttl</code> property in the bk_config
will be pulled every ttl interval and updated in place.</p>
</li>
<li><p>DNS records
Some config options may be kept in the DNS TXT records and every time a instance is started it will query the local DNS for such parameters. Only a small subset of
all config parameters support DNS store. To see which parameters can be stored in the DNS run <code>bkjs show-help</code> and look for &#39;DNS TXT configurable&#39;.</p>
</li>
</ul>
<h1 id="backend-library-development-mac-os-x-developers-">Backend library development (Mac OS X, developers)</h1>
<ul>
<li><p>for DB drivers and ImageMagick to work propely it needs some dependencies to be installed:</p>
<pre><code>  port install libpng jpeg tiff lcms2 mysql56 postgresql93</code></pre></li>
<li><p>make sure there is no openjpeg15 installed, it will conflict with ImageMagick jp2 codec</p>
</li>
<li><p><code>git clone https://github.com/vseryakov/backendjs.git</code> or <code>git clone git@github.com:vseryakov/backendjs.git</code></p>
</li>
<li><p>cd backendjs</p>
</li>
<li><p>if Node.js is already installed skip to the next section</p>
<ul>
<li><p>to install binary release run the command, it will install it into /opt/local on Darwin</p>
<pre><code>   bkjs install-node

   # To install into different path
   bkjs install-node -prefix /usr/local/node</code></pre></li>
<li><p><strong>Important</strong>: Add NODE_PATH=$BKJS_PREFIX/lib/node_modules to your environment in .profile or .bash_profile so
node can find global modules, replace $BKJS_PREFIX with the actual path unless this variable is also set in the .profile</p>
</li>
</ul>
</li>
<li><p>to install all dependencies and make backendjs module and bkjs globally available:</p>
<pre><code>      npm link backendjs</code></pre></li>
<li><p>to run local server on port 8000 run command:</p>
<pre><code>      bkjs web</code></pre></li>
<li><p>to start the backend in command line mode, the backend environment is prepared and initialized including all database pools.
 This command line access allows you to test and run all functions from all modules of the backend without running full server
 similar to Node.js REPL functionality. All modules are accessible from the command line.</p>
<pre><code>      $ ./bkjs shell
      &gt; core.version
      &#39;0.70.0&#39;
      &gt; logger.setLevel(&#39;info&#39;)</code></pre></li>
</ul>
<h1 id="design-considerations">Design considerations</h1>
<p>While creating Backendjs there were many questions and issues to be considered, some I was able to implement, some still not. Below are the thoughts that
might be useful when designing, developing or choosing the API platform:</p>
<ul>
<li>purpose of the API:<ul>
<li>to expose some parts of the existing system to external apps, users...</li>
<li>to make it the only way to access services</li>
<li>to complement another system</li>
</ul>
</li>
<li>scalability considerations:<ul>
<li>unlimited/uncontrolled access like mobile, web, more users the better</li>
<li>enterprise level, controlled growth</li>
<li>not to be horizontally scalable, just vertically</li>
</ul>
</li>
<li>security:<ul>
<li>support authentication, users, accounts, profiles...</li>
<li>just for robots, limited by api key only</li>
<li>signed requests only</li>
<li>support all access, web, mobile, desktop</li>
<li>user access controls, how to distinguish users, grant access to only parts of the API</li>
<li>ability to run custom/specific filters during processing API requests, independently and ability to extend the app without rewriting/rebuilding the whole system</li>
<li>third party authentication, OAUTH, user mapping</li>
</ul>
</li>
<li>platform/framework:<ul>
<li>one for all, same language/SDK/framework to cover all aspects</li>
<li>multiple languages/frameworks for different tasks, then how to integrate, how to communicate, share code</li>
<li>availability of the third party modules, libraries</li>
<li>support, forums, docs, how easy to learn for new developers</li>
<li>modularity, ability to develop by multiple developers, teams</li>
<li>flexibility in extending, how simple/easy to add custom stuff</li>
<li>maintenance, support,how easy to scale, change, replace parts</li>
</ul>
</li>
<li>database layer:<ul>
<li>one central database for everything</li>
<li>multiple database for different parts of the system according to scalability/other requirements</li>
<li>switch databases behind the scene in order to scale, adding to features, easier to maintain</li>
<li>caching, needs to be independent from other parts and easily enabled/disabled for different components preferably via config</li>
<li>to have or not ORM</li>
</ul>
</li>
<li>process management, easy to deploy, monitor</li>
<li>logging, metrics, profiling</li>
<li>agnostic to the frontends or to be included with some kind of MVC/server based tools</li>
<li>ability to support simple Web development for simple web pages without installing/supporting general purpose tools like Apache/PHP/nginx</li>
</ul>
<h1 id="api-endpoints-provided-by-the-backend">API endpoints provided by the backend</h1>
<p>All API endpoints are optional and can be disabled or replaced easily. By default the naming convention is:</p>
<pre><code> /namespace/command[/subname[/subcommand]]</code></pre><p>Any HTTP methods can be used because its the command in the URL that defines the operation. The payload can be url-encoded query
parameters or JSON or any other format supported by any particular endpoint. This makes the backend universal and usable with any
environment, not just a Web browser. Request signature can be passed in the query so it does not require HTTP headers at all.</p>
<h2 id="authentication-and-sessions">Authentication and sessions</h2>
<h3 id="signature">Signature</h3>
<p>All requests to the API server must be signed with account login/secret pair.</p>
<ul>
<li>The algorithm how to sign HTTP requests (Version 1, 2):<ul>
<li>Split url to path and query parameters with &quot;?&quot;</li>
<li>Split query parameters with &quot;&amp;&quot;</li>
<li>&#39;&#39;&#39;ignore parameters with empty names&#39;&#39;&#39;</li>
<li>&#39;&#39;&#39;Sort&#39;&#39;&#39; list of parameters alphabetically</li>
<li>Join sorted list of parameters with &quot;&amp;&quot;<ul>
<li>Make sure all + are encoded as %2B</li>
</ul>
</li>
<li>Form canonical string to be signed as the following:<ul>
<li>Line1: The signature version</li>
<li>Line2: The application tag or other opaque data</li>
<li>Line3: The login name</li>
<li>Line4: The HTTP method(GET), followed by a newline.</li>
<li>Line5: The host name, lowercase, followed by a newline.</li>
<li>Line6: The request URI (/), followed by a newline.</li>
<li>Line7: The sorted and joined query parameters as one string, followed by a newline.</li>
<li>Line8: The expiration value in milliseconds, required, followed by a newline</li>
<li>Line9: The Content-Type HTTP header, lowercase, optional, followed by a newline</li>
<li>Line10: The SHA1 checksum of the body content, optional, for JSON and other forms of requests not supported by query parameters</li>
</ul>
</li>
<li>Computed HMAC-SHA1 digest from the canonical string and encode it as BASE64 string, preserve trailing = if any</li>
<li>Form the signature HTTP header as the following:<ul>
<li>The header string consist of multiple fields separated by pipe |<ul>
<li>Field1: Signature version:<ul>
<li>version 1, obsolete, do not use first 3 lines in the canonical string</li>
<li>version 2,3 to be used in session cookies only</li>
<li>version 4</li>
</ul>
</li>
<li>Field2: Application tag or other app specific data</li>
<li>Field3: account login or whatever it might be in the login column</li>
<li>Field4: HMAC-SHA digest from the canonical string, version 1 uses SHA1, other SHA256</li>
<li>Field5: expiration value in milliseconds, same as in the canonical string</li>
<li>Field6: SHA1 checksum of the body content, optional, for JSON and other forms of requests not supported by query parameters</li>
<li>Field7: empty, reserved for future use</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>The resulting signature is sent as HTTP header <code>bk-signature</code> or in the header specified by the <code>api-signature-name</code> config parameter.</p>
<p>For JSON content type, the method must be POST and no query parameters specified, instead everything should be inside the JSON object
which is placed in the body of the request. For additional safety, SHA1 checksum of the JSON payload can be calculated and passed in the signature,
this is the only way to ensure the body is not modified when not using query parameters.</p>
<p>See <a href="https://github.com/vseryakov/backendjs/blob/master/web/js/bkjs.js">web/js/bkjs.js</a> function <code>Bkjs.createSignature</code> or
<a href="https://github.com/vseryakov/backendjs/blob/master/api.js">api.js</a> function <code>api.createSignature</code> for the JavaScript implementations.</p>
<p>There is also native iOS implementation <a href="https://raw.githubusercontent.com/vseryakov/backendjs-ios/master/BKjs.m">Bkjs.m</a>.</p>
<h3 id="authentication-api">Authentication API</h3>
<ul>
<li><p><code>/auth</code></p>
<p> This API request returns the current user record from the <code>bk_auth</code> table if the request is verified and the signature provided
 is valid. If no signature or it is invalid the result will be an error with the corresponding error code and message.</p>
<p> By default this endpoint is secured, i.e. requires a valid signature.</p>
<p> Parameters:</p>
<ul>
<li><p><code>_session=1</code> - if the call is authenticated a cookie with the session signature is returned, from now on
 all requests with such cookie will be authenticated, the primary use for this is Web apps</p>
</li>
<li><p><code>_accesstoken=1</code> - returns new access token to be used for subsequent requests without a signature for the current account,
 the token is short lived with expiration date returned as well. This access token can be used instead of a signature and
 is passed in the query as <code>bk-access-token=TOKEN</code>.</p>
<p> Example:</p>
<pre><code>     /auth?_accesstoken=1
     &gt; { id: &quot;XXXX...&quot;, name: &quot;Test User&quot;, &quot;bk-access-token&quot;: &quot;XXXXX....&quot;, &quot;bk-access-token-age&quot;: 604800000 }

     /account/get?bk-access-token=XXXXXX...
     &gt; { id: &quot;XXXX...&quot;, name: &quot;Test User&quot;, ... }</code></pre></li>
</ul>
</li>
<li><p><code>/login</code></p>
<p> Same as the /auth but it uses secret for user authentication, this request does not need a signature, just simple
 login and secret query parameters to be sent to the backend. This must be sent over SSL.</p>
<p> The intended usage is for Web sessions which use sessions cookies when sent with <code>_session=1</code> or to be used with access tokens when
 sent with <code>_accesstoken=1</code>.</p>
<p> Parameters:</p>
<ul>
<li><p><code>login</code> - account login</p>
</li>
<li><p><code>zecret</code> - account secret</p>
</li>
<li><p><code>_session=1</code> - same as in /auth request</p>
</li>
<li><p><code>_accesstoken=1</code> - same as in /auth reuest</p>
<p>On successful login, the result contains full account record including the secret, this is the only time when the secret is returned back</p>
<p>Example:</p>
<pre><code>   $.ajax({ url: &quot;/login?login=test123&amp;secret=test123&amp;_session=1&quot;,
            success: function(json, status, xhr) { console.log(json) }
   });

   &gt; { id: &quot;XXXX...&quot;, name: &quot;Test User&quot;, login: &quot;test123&quot;, ...}</code></pre></li>
</ul>
</li>
<li><p><code>/logout</code></p>
<p> Logout the current user, clear session cookies if exist. For pure API access with the signature this will not do anything on the backend side.</p>
</li>
</ul>
<h2 id="accounts">Accounts</h2>
<p>The accounts API manages accounts and authentication, it provides basic user account features with common fields like email, name, address.</p>
<p>This is implemented by the <code>accounts</code> module from the core. To enable accounts functionality specify <code>-allow-modules=bk_accounts</code>.</p>
<ul>
<li><p><code>/account/get</code></p>
<p>Returns information about current account or other accounts, all account columns are returned for the current account and only public columns
returned for other accounts. This ensures that no private fields ever be exposed to other API clients. This call also can used to login into the service or
verifying if the given login and secret are valid, there is no special login API call because each call must be signed and all calls are stateless and independent.</p>
<p>Parameters:</p>
<ul>
<li>no id is given, return only one current account record as JSON</li>
<li>id=id,id,... - return information about given account(s), the id parameter can be a single account id or list of ids separated by comma</li>
<li>_session=1 - after successful login setup a session with cookies so the Web app can perform requests without signing every request anymore</li>
<li>_accesstoken=1 - after successful login, return new access token that ca be used to make requests without signing every request, it can be
 passed in the query or headers with the name <code>bk-access-token</code></li>
</ul>
<p>Note: When retrieving current account, all properties will be present including the location, for other accounts only the properties marked as <code>pub</code> in the
<code>bk_account</code> table will be returned.</p>
<p>Response:</p>
<pre><code>    { &quot;id&quot;: &quot;57d07a4e28fc4f33bdca9f6c8e04d6c3&quot;,
      &quot;name&quot;: &quot;Test User&quot;,
      &quot;name&quot;: &quot;Real Name&quot;,
      &quot;mtime&quot;: 1391824028,
      &quot;latitude&quot;: 34,
      &quot;longitude&quot;: -118,
      &quot;geohash&quot;: &quot;9qh1&quot;,
      &quot;login&quot;: &quot;testuser&quot;,
    }</code></pre></li>
</ul>
<ul>
<li><p><code>/account/add</code></p>
<p>Add new account, all parameters are the columns from the <code>bk_account</code> table, required columns are: <strong>name, secret, login</strong>.</p>
<p>To enable open registration add <code>api-allow-path=^/account/add$</code> to the config file or specify in the command line.
More complex ways to perform registration will require adding pre and.or post callbacks to handle account registration
for example with invitation codes....</p>
<p>In the table <code>bk_auth</code>, the column <code>type</code> is used to distinguish between account roles, by default only account with type <code>admin</code> can
add other accounts with this type specified, this column can also be used in account permissions implementations. Because it is in the bk_auth table,
all columns of this table are available as <code>req.account</code> object after the successful authentication where <code>req</code> is an Express request object used in the middleware
parameters.</p>
<p>Secret and login can be anything, the backend does not require any specific formats and does not process the contents of the login/secret fields.</p>
<p>There are several ways to create authenticated account:</p>
<ul>
<li>API only access with signed signature, supply a login and secret which will be stored in the database as is, when making requests use the same login and secret to produce the signature.
<em>In the Web client <code>web/js/bkjs.js</code>, if <code>Bkjs.scramble</code> is set to 1 then the secret is replaced by the BASE64_HMAC_SHA256(secret, login) automatically,
no actual secret is ever saved or sent, only used in the login form. This is intended for Web apps not to store the actual secret anywhere in the memory or localStorage,
for the backend this is still just a secret.</em></li>
</ul>
<p>Example:</p>
<pre><code>      /account/add?name=test&amp;login=test@test.com&amp;secret=fc4f33bd07a4e6c8e&amp;gender=f&amp;phone=1234567</code></pre><p>How to make an account as admin</p>
<pre><code>      # Run backend shell
      bkjs shell

      # Update record by login
      &gt; db.update(&quot;bk_auth&quot;, { login: &#39;login@name&#39;, type: &#39;admin&#39; });</code></pre></li>
<li><p><code>/account/select</code></p>
<p>Return list of accounts by the given condition, calls <code>db.select</code> for bk_account table. Parameters are the column values to be matched and
all parameters starting with underscore are control parameters that goes into options of the <code>db.select</code> call with underscore removed. This will work for SQL
databases only because DynamoDB or Cassandra will not search by non primary keys. In the DynamoDB case this will run ScanTable action which will be very expensive for
large tables. Supports special query parameters <code>_select,_ops</code>, see docs about <code>db.select</code> for more info.</p>
<p>Example:</p>
<pre><code>      /account/search?email=test&amp;_ops=email,begins_with
      /account/search?name=test</code></pre><p>Response:</p>
<pre><code>      {  &quot;data&quot;: [{
                    &quot;id&quot;: &quot;57d07a4e28fc4f33bdca9f6c8e04d6c3&quot;,
                    &quot;name&quot;: &quot;Test User1&quot;,
                    &quot;mtime&quot;: 1391824028,
                    &quot;login&quot;: &quot;test1&quot;,
                  },
                  {
                    &quot;id&quot;: &quot;57d07a4e2824fc43bd669f6c8e04d6c3&quot;,
                    &quot;name&quot;: &quot;Test User2&quot;,
                    &quot;mtime&quot;: 1391824028,
                    &quot;login&quot;: &quot;test2&quot;,
                  }],
          &quot;next_token&quot;: &quot;&quot;
      }</code></pre></li>
<li><p><code>/account/del</code></p>
<p>Delete current account, after this call no more requests will be authenticated with the current credentials</p>
</li>
<li><p><code>/account/update</code></p>
<p>Update current account with new values, the parameters are columns of the table <code>bk_account</code>, only columns with non empty values will be updated.</p>
<p>Example:</p>
<pre><code>      /account/update?name=New%2BName&amp;gender=m</code></pre></li>
<li><p><code>/account/put/secret</code></p>
<p>Change account secret for the current account, no columns except the secret will be updated and expected.</p>
<p>Parameters:</p>
<ul>
<li>secret - new secret for the account</li>
<li>token_secret - set to 1 to reset access token secret to a new value thus revoking access from all existing access tokens</li>
</ul>
<p>Example:</p>
<pre><code>      /account/put/secret?secret=blahblahblah</code></pre></li>
</ul>
<ul>
<li><p><code>/account/subcribe</code></p>
<p>Subscribe to account events delivered via HTTP Long Poll, a client makes the connection and waits for events to come, whenever
somebody updates the account&#39;s counter or send a message or creates a connection to this account the event about it will be sent to this HTTP
connection and delivered as JSON object. This is not a persistent queue so if not listening, all events will just be ignored, only events published
since the connect will be delivered. To specify what kind of events needs to be delivered, <code>match</code> query parameter can be specified which is a
RegExp of the whole event body string.</p>
<p><em>Note: On the server side there is a config parameter <code>api-subscribe-interval</code> which defines how often to deliver notifications, by default it is 5 seconds which means
only every 5 seconds new events will be delivered to the Web client, if more than one event happened, they all accumulate and will be sent as a JSON list.</em></p>
<p>Example:</p>
<pre><code>  /account/subscribe
  /account/subscribe?match=connection/add.*type:*like

  // To run in the browser:
  (function poll() {
      Bkjs.send({ url: &quot;/account/subscribe&quot;, complete: poll }, function(data) {
          console.log(&quot;received event:&quot;, data);
       });
   })();</code></pre><p>Response:</p>
<pre><code>  [ { &quot;path&quot;: &quot;/message/add&quot;, &quot;mtime:&quot; 1234566566, &quot;type&quot;: &quot;1&quot; },
    { &quot;path&quot;: &quot;/counter/incr&quot;, &quot;mtime:&quot; 1234566566, &quot;type&quot;: &quot;like,invite&quot; } },
    { &quot;path&quot;: &quot;/connection/add&quot;, &quot;mtime&quot;: 1223345545, &quot;type&quot;: &quot;like&quot; } ]</code></pre></li>
<li><p><code>/account/select/icon</code></p>
<p>Return a list of available account icons, icons that have been uploaded previously with /account/put/icon calls. The <code>url</code> property is an URL to retrieve this particular icon.</p>
<p>Parameters:</p>
<ul>
<li>id - if specified then icons for the given account will be returned</li>
</ul>
<p>Example:</p>
<pre><code>  /account/select/icon?id=12345</code></pre><p>Response:</p>
<pre><code>  [ { id: &#39;12345&#39;, type: &#39;1&#39;, url: &#39;/account/get/icon?id=12345&amp;type=1&#39; },
    { id: &#39;12345&#39;, type: &#39;2&#39;, url: &#39;/account/get/icon?id=12345&amp;type=2&#39; } ]</code></pre></li>
<li><p><code>/account/get/icon</code></p>
<p>Return an account icon, <em>the icon is returned in the body as binary BLOB</em>, if no icon with specified type exists, i.e. never been uploaded then 404 is returned.</p>
<p>Parameters:</p>
<ul>
<li>type - a number from 0 to 9 or any single letter a..z which defines which icon to return, if not specified 0 is used</li>
</ul>
<p>Example:</p>
<pre><code>  /account/get/icon?type=2</code></pre></li>
</ul>
<ul>
<li><p><code>/account/put/icon</code></p>
<p>Upload an account icon, once uploaded, the next <code>/account/get</code> call will return properties in the format <code>iconN</code> where N is any of the
type query parameters specified here, for example if we uploaded an icon with type 5, then /account/get will return property icon5 with the URL
to retrieve this icon.
<em>By default all icons uploaded only accessible for the account which uploaded them.</em></p>
<p>Parameters:</p>
<ul>
<li>type - icon type, a number between 0 and 9 or any single letter a..z, if not specified 0 is used</li>
<li>icon - can be passed as base64 encoded image in the query,<ul>
<li>can be passed as base64 encoded string in the body as JSON, like: { type: 0, icon: &#39;iVBORw0KGgoA...&#39; },
for JSON the Content-Type HTTP headers must be set to <code>application/json</code> and data should be sent with POST request</li>
<li>can be uploaded from the browser using regular multi-part form</li>
</ul>
</li>
<li>acl_allow - icon access permissions:<ul>
<li>&quot;&quot; (empty) - only own account can access</li>
<li>all - public, everybody can see this icon</li>
<li>auth - only authenticated users can see this icon</li>
<li>id,id.. - list of account ids that can see this account</li>
</ul>
</li>
<li>_width - desired width of the stored icon, if negative this means do not upscale, if the image width is less than given keep it as is</li>
<li>_height - height of the icon, same rules apply as for the width above</li>
<li>_ext - image file format, default is jpg, supports: gif, png, jpg, jp2</li>
</ul>
<p>Example:</p>
<pre><code>  /account/put/icon?type=1&amp;icon=iVBORw0KGgoAAAANSUhEUgAAAAcAAAAJCAYAAAD+WDajAAAABGdBTUEAALGPC/xhBQAAAAlwSFlzAAAOwgAADs....</code></pre></li>
<li><p><code>/account/del/icon</code></p>
<p>Delete account icon</p>
<p>Parameters:</p>
<ul>
<li>type - what icon to delete, if not specified 0 is used</li>
</ul>
<p>Example:</p>
<pre><code>  /account/icon/del?type=1</code></pre></li>
</ul>
<h3 id="health-enquiry">Health enquiry</h3>
<p>When running with AWS load balancer there should be a url that a load balancer polls all the time and this must be very quick and lightweight request. For this
purpose there is an API endpoint <code>/ping</code> that just responds with status 200. It is not open by default, the <code>allow-path</code> or other way to allow non-authenticated access
needs to be configured. This is to be able to control how pinging can be perform in the apps in case it is not simple open access.</p>
<h2 id="public-images-endpoint">Public Images endpoint</h2>
<p>This endpoint can server any icon uploaded to the server for any account, it is supposed to be a non-secure method, i.e. no authentication will be performed and no signature
will be needed once it is configured which prefix can be public using <code>api-allow</code> or <code>api-allow-path</code> config parameters.</p>
<p>The format of the endpoint is:</p>
<ul>
<li><p><code>/image/prefix/id/type[.png|.jpg]</code></p>
<p>  Example:</p>
<pre><code>  # Configure accounts icons to be public in the etc/config
  api-allow-path=/image/account/

  # Or pass in the command line
  ./app.sh -api-allow-path /image/account/

  # Make requests
  /image/account/12345/0
  /image/account/12345/1
  /image/account/12345/1.jpg

  #Return icons for account 12345 for types 0 and 1</code></pre></li>
</ul>
<h2 id="icons">Icons</h2>
<p>The icons API provides ability for an account to store icons of different types. Each account keeps its own icons separate form other
accounts, within the account icons can be separated by <code>prefix</code> which is just a namespace assigned to the icons set, for example to keep messages
icons separate from albums, or use prefix for each separate album. Within the prefix icons can be assigned with unique type which can be any string.</p>
<p>Prefix and type can consist from alphabetical characters and numbers, dots, underscores and dashes: [a-z0-9._-]. This means, they are identifiers, not real titles or names,
a special mapping between prefix/type and album titles for example needs to be created separately.</p>
<p>The supposed usage for type is to concatenate common identifiers first with more specific to form unique icon type which later can be queried
by prefix or exactly by icon type. For example album id can be prefixed first, then sequential con number like album1:icon1, album1:icon2....
then retrieving all icons for an album would be only query with album1: prefix.</p>
<p>The is implemented by the <code>icons</code> module from the core. To enable this functionality specify <code>-allow-modules=bk_icons</code>.</p>
<ul>
<li><p><code>/icon/get</code></p>
<p> Return icon for the current account in the given prefix, icons are kept on the local disk in the directory
 configured by <code>-api-images-dir</code> parameter(default is images/ in the backend directory). Current account id is used to keep icons
 separate from other accounts. Icon presence is checked in the bk_icon table before returning it and if any permissions are set in
 the <code>acl_allow</code> column it will be checked if this icon can be returned.</p>
<p>The following parameters can be used:</p>
<ul>
<li><code>prefix</code> - must be specified, this defines the icons namespace</li>
<li><code>type</code> is used to specify unique icon created with such type which can be any string.</li>
<li><code>_ext</code> - image extension, like png or jpg if it was saved with it previously</li>
</ul>
</li>
<li><p><code>/icon/put</code></p>
<p>Upload new icon for the given account in the folder prefix, if type is specified it creates an icon for this type to separate
multiple icons for the same prefix. <code>type</code> can be any string consisting from alpha and digits characters. It creates a record in the bk_icon
table with all the parameters passed.</p>
<p>The following parameters can be used:</p>
<ul>
<li>prefix - prefix for the icons, required</li>
<li>descr - optional description of the icon</li>
<li>latitude, longitude - optional coordinates for the icon</li>
<li>acl_allow - allow access permissions, see <code>/account/put/icon</code> for the format and usage</li>
<li>_width - desired width of the stored icon, if negative this means do not upscale, if the image width is less than given then keep it as is</li>
<li>_height - height of the icon, same rules apply as for the width above</li>
<li>_ext - image file format, default is jpg, supports: gif, png, jpg</li>
</ul>
</li>
<li><p><code>/icon/upload</code></p>
<p> Upload a new image and store on the server, no record is created in bk_icon table, just simple image upload,
 but all the same query parameters as for /icon/put are accepted. Returns an JSON object with url property being the full path
 to the uploaded image.</p>
</li>
<li><p><code>/icon/del</code></p>
<p> Delete the default icon for the current account in the folder prefix or by type</p>
</li>
<li><p><code>/icon/select</code></p>
<p>Return list of available icons for the given prefix and type, all icons starting with prefix/type will be returned,
the <code>url</code> property will provide full URL to retrieve the icon contents</p>
<p>Example:</p>
<pre><code>  /icon/select?prefix=album&amp;type=me
  /icon/select?prefix=album&amp;type=12345</code></pre><p>Responses:</p>
<pre><code>  [ { id: &#39;b3dcfd1e63394e769658973f0deaa81a&#39;, type: &#39;me-1&#39;, icon: &#39;/icon/get?prefix=album&amp;type=me1&#39; },
    { id: &#39;b3dcfd1e63394e769658973f0deaa81a&#39;, type: &#39;me-2&#39;, icon: &#39;/icon/get?prefix=album&amp;type=me2&#39; } ]

  [ { id: &#39;b3dcfd1e63394e769658973f0deaa81a&#39;, type: &#39;12345-f0deaa81a&#39;, icon: &#39;/icon/get?prefix=album&amp;type=12345-f0deaa81a&#39; } ]</code></pre></li>
</ul>
<h2 id="file-api">File API</h2>
<p>The file API provides ability to store and retrieve files. The operations are similar to the Icon API.</p>
<p>This is implemented by the <code>files</code> module from the core. To enable this functionality specify <code>-allow-modules=bk_files</code>.</p>
<ul>
<li><p><code>/file/get</code></p>
<p>  Return a file with given prefix and name, the contents are returned in the response body.</p>
<p>  The following parameters can be used:</p>
<ul>
<li><code>prefix</code> - must be provided, defines the namespace where the file is stored</li>
<li><code>name</code> - name of the file, required</li>
</ul>
</li>
<li><p><code>/file/put</code></p>
<p>  Store a file on the backend, the file can be sent using form multipart upload or as JSON</p>
<p>  The following parameters can be used:</p>
<ul>
<li><code>prefix</code> - must be provided, defines the namespace where the file is stored</li>
<li><code>name</code> - name of the file, required</li>
<li><code>_name</code> - name of the property that contains the file contents, for use with JSON or defines the name of the file attribute for multipart upload</li>
<li><code>_tm</code> - append the current timestamp to the file name</li>
<li><code>_ext</code> - extension to be assign to the file, otherwise the actual extension from the file name is used</li>
</ul>
</li>
<li><p><code>/file/del</code></p>
<p>  Delete file, prefix and name must be given</p>
</li>
</ul>
<h2 id="connections">Connections</h2>
<p>The connections API maintains two tables <code>bk_connection</code> and <code>bk_reference</code> for links between accounts of any type. bk_connection table maintains my
links, i.e. when i make explicit connection to other account, and bk_reference table is automatically updated with reference for that other account that I made
a connection with it. No direct operations on bk_reference is allowed.</p>
<p>This is implemented by the <code>connections</code> module from the core. To enable this functionality specify <code>-allow-modules=bk_connections</code>.</p>
<ul>
<li><p><code>/connection/add</code></p>
</li>
<li><p><code>/connection/put</code>
Create or replace a connection between two accounts, required parameters are:</p>
<ul>
<li><code>peer</code> - id of account to connect to</li>
<li><code>type</code> - type of connection, like,dislike,....</li>
<li>_connected - the reply will contain a connection record if the other side of our connection is connected to us as well</li>
</ul>
<p>This call automatically creates a record in the bk_reference table which is reversed connection for easy access to information like
&#39;&#39;who is connected to me&#39;&#39;.</p>
<p>Example:</p>
<pre><code>  /connection/add?peer=12345&amp;type=invite&amp;state=sent</code></pre></li>
<li><p><code>/connection/update</code></p>
</li>
<li><p><code>/connection/incr</code>
Update other properties of the existing connection, for connections that may take more than i step or if a connection has other data associated with it beside
the type of the connection.</p>
<p>Example:</p>
<pre><code>  /connection/update?peer=12345&amp;type=invite&amp;state=accepted</code></pre></li>
<li><p><code>/connection/del</code>
Delete existing connection(s), <code>id</code> and/or <code>type</code> may be be specified, if not all existing connections will be deleted.</p>
<p>Example:</p>
<pre><code>  /connection/del?type=invite&amp;peer=12345</code></pre></li>
<li><p><code>/connection/get</code>
Return a single connection for given id</p>
<p>Parameters:</p>
<ul>
<li>peer - account id of the connection, required</li>
<li>type - connection type, required</li>
</ul>
<p>Example:</p>
<pre><code>  /connection/get?peer=12345&amp;type=like</code></pre><p>Response:</p>
<pre><code>  { &quot;id&quot;: &quot;1111&quot;,
    &quot;type: &quot;like&quot;,
    &quot;peer&quot;: &quot;12345&quot;,
    &quot;mtime&quot;: &quot;2434343543543&quot; }</code></pre></li>
<li><p><code>/reference/get</code>
Return a single reference record for given account id, works the same way as <code>/connection/get</code></p>
</li>
</ul>
<ul>
<li><p><code>/connection/select</code>
Receive all my connections of the given type, i.e. connection(s) i made, if <code>id</code> is given only one record for the specified connection will be returned. Supports special
query parameters <code>_select,_ops,_desc</code>, see docs about <code>db.select</code> for more info.</p>
<p>Example:</p>
<pre><code>  # Return all accounts who i invited
  /connection/select?type=invite
  # Return connection for specific type and account id
  /connection/select?type=invite&amp;peer=12345
  # Return accounts who i invited me after specified mtime
  /connection/select?type=invite&amp;_ops=mtime,gt&amp;mtime=12334312543
  # Return accounts who i invited before specified mtime
  /connection/select?type=invite&amp;_ops=mtime,le&amp;_desc=1&amp;mtime=12334312543</code></pre><p>Response:</p>
<pre><code>  { &quot;data&quot;: [ { &quot;id&quot;: &quot;111&quot;,
                &quot;type&quot;: &quot;invite&quot;,
                &quot;peer&quot;: &quot;12345&quot;,
                &quot;status&quot;: &quot;&quot;,
                &quot;mtime&quot;: &quot;12334312543&quot;
            }],
    &quot;next_token&quot;: &quot;&quot;
  }</code></pre></li>
<li><p><code>/reference/select</code>
Receive all references that connected with my account, i.e. connections made by somebody else with me, works the same way as for connection query call</p>
<p>Example:</p>
<pre><code>  # Return all accounts who invited me
  /reference/select?type=invite
  # Return accounts who invited me after specified mtime
  /reference/select?type=invite&amp;_ops=mtime,gt&amp;mtime=12334312543</code></pre><p>Response:</p>
<pre><code>  { &quot;data&quot;: [ { &quot;id&quot;: &quot;111&quot;,
                &quot;type&quot;: &quot;invite&quot;,
                &quot;peer&quot;: &quot;12345&quot;,
                &quot;status&quot;: &quot;&quot;,
                &quot;mtime&quot;: &quot;12334312543&quot;
            }],
    &quot;next_token&quot;: &quot;&quot;
  }</code></pre></li>
</ul>
<h2 id="locations">Locations</h2>
<p>The location API maintains a table <code>bk_location</code> with geolocation coordinates for accounts and allows searching it by distance. The configuration parameter
<code>min-distance</code> defines the radius for the smallest bounding box in km containing single location, radius searches will combine neighboring boxes of
this size to cover the whole area with the given distance request, also this affects the length of geohash keys stored in the bk_location table. By default min-distance is 5 km
which means all geohashes in bk_location table will have geohash of size 4. Once min-distance is set it cannot be changed without rebuilding the bk_location table with new geohash size.</p>
<p>The location search is implemented by using geohash as a primary key in the bk_location table with the account id as the second part of the primary key, for DynamoDB this is the range key.
When request comes for all matches for the location for example 37.7, -122.4, the search that is executed looks like this:</p>
<ul>
<li>geohash for latitude 37.7 and longitude -122.4 and radius 10 km will be <code>9q8y</code></li>
<li>all neighboring areas around this point within 10 km radius will be &#39;9q8z&#39;, &#39;9q8v&#39;, &#39;9q8w&#39;, &#39;9q8x&#39;, &#39;9q8t&#39;, &#39;9q9n&#39;, &#39;9q9p&#39;, &#39;9q9j&#39;</li>
<li>we start the search on the bk_location table by the primary key geohash with the value 9q8y</li>
<li>filter out all records beyond our radius by calculating the difference between our point and the candidate record</li>
<li>if total number of results expected is still less than required, continue to the next neighbor area</li>
<li>continue until we visit all neighbors or received required number of matched records</li>
<li>on return the next_token opaque value will be provided if we want to continue the search for more matched for the same location</li>
</ul>
<p>This is implemented by the <code>locations</code> module from the core. To enable this functionality specify <code>allow-modules=bk_locations</code>.</p>
<ul>
<li><p><code>/location/put</code>
Store currenct location for current account, latitude and longitude parameters must be given, this call will update the bk_account table as well with
these coordinates</p>
<p>Example:</p>
<pre><code>  /location/put?latitude=-188.23232&amp;longitude=23.4545454</code></pre></li>
<li><p><code>/location/get</code>
Return matched accounts within the distance(radius) specified by <code>distance=</code> parameter in kilometers and current position specified by latitude/longitude parameters. This
call returns results in chunks and requires navigation through all pages to receive all matched records. Records returned will start with the closest to the current
point. If there are more matched records than specified by the <code>_count</code>, the <code>next_token</code> property is set with the token to be used in the subsequent call,
it must be passed as is as <code>_token=</code> parameter with all original query parameters.</p>
<p>Note: The current account will not be present in the results  even if it is within the range, to know my own location use <code>/account/get</code> call.</p>
<p>Example:</p>
<pre><code>      /location/get?distance=10&amp;latitude=-118.23434&amp;longitude=23.45665656&amp;_count=25
      /location/get?distance=10&amp;latitude=-118.23434&amp;longitude=23.45665656&amp;_count=25&amp;_token=FGTHTRHRTHRTHTTR.....</code></pre><p>Response:</p>
<pre><code>     { &quot;data&quot;: [ { &quot;id&quot;: &quot;12345&quot;,
                   &quot;distance&quot;: 5,
                   &quot;latitude&quot;: -118.123,
                   &quot;longitude&quot;: 23.45
                   &quot;mtime&quot;: &quot;12334312543&quot;
                 },
                 { &quot;id&quot;: &quot;45678&quot;,
                   &quot;distance&quot;: 5,
                   &quot;latitude&quot;: -118.133,
                   &quot;longitude&quot;: 23.5
                   &quot;mtime&quot;: &quot;12334312543&quot;
                 }],
       &quot;next_token&quot;: &quot;&quot;
     }</code></pre></li>
</ul>
<h2 id="messages">Messages</h2>
<p>The messaging API allows sending and receiving messages between accounts, it supports text and images. All new messages arrive into the bk_messsage table, the inbox. The client
may keep messages there as new, delete or archive them. Archiving means transferring messages into the bk_archive table. All sent messages are kept in the bk_sent table.</p>
<p>This is implemented by the <code>messages</code> module from the core. To enable this functionality specify <code>-allow-modules=bk_messages</code>.</p>
<ul>
<li><p><code>/message/get/unread</code>
 Return how many unread messages in the inbox, this is just a flag to signal about new messages, the actual number may not be up to date,
 it is cleared on messages read.</p>
<p> Example:</p>
<pre><code> /message/get/unread</code></pre><p> Response:</p>
<pre><code>{ count: 1 }</code></pre></li>
<li><p><code>/message/get</code>
Read all messages from the inbox.</p>
<p>Parameters:</p>
<ul>
<li><code>_archive</code> - if set to 1, all returned messages will be archived automatically, so no individual /message/read call needed</li>
<li><code>_trash</code> - if set to 1, all returned messages will be deleted, not archived</li>
<li><code>_total</code> - if set to 1 then return how many messages in the inbox
the unread flag with the actual number of unread messages.</li>
</ul>
<p>Example:</p>
<pre><code>  # Get all new messages
  /message/get

  # Get all new messages and archive them
  /message/get?_archive=1

  # Get all new messages from the specific sender
  /message/get?sender=12345

  # How many new messages
  /message/get?_total=1</code></pre></li>
<li><p><code>/message/get/archive</code>
Receive archived messages. The images are not returned, only link to the image in <code>icon</code> property of reach record,
the actual image data must be retrieved separately.</p>
<p>Parameters:</p>
<ul>
<li><code>mtime</code> - if specified then only messages received since that time will be returned, it must be in milliseconds since midnight GMT on January 1, 1970, this is what
Date.now() return in JavaScript.<ul>
<li><code>sender</code> - if specified then all messages from the given sender will be returned.</li>
</ul>
</li>
</ul>
<p>NOTE: The <code>mtime</code> is when the backend server received the message, if client and the server clocks are off this may return wrong data or not return anything at all,
also because the arrival order of the messages cannot be guaranteed, sending fast multiple messages may be received in different order by the backend and this will
result in mtimes that do not correspond to actual times when the message has been sent.</p>
<p>Example:</p>
<pre><code>  # Get all messages
  /message/get/archive

  # Get all messages received after given mtime
  /message/get/archive?mtime=123475658690

  # Get all messages received before given mtime
  /message/get/archive?mtime=123475658690&amp;_ops=mtime,lt

  # Get all messages with custom filter: if msg text contains Hi
  /message/get/archive?_ops=msg,iregexp&amp;msg=Hi

  # Get all messages from the specific sender
  /message/get/archive?sender=12345</code></pre><p>Response:</p>
<pre><code>  { &quot;data&quot;: [ { &quot;sender&quot;: &quot;12345&quot;,
                &quot;msg&quot;: &quot;Hi, how r u?&quot;,
                &quot;mtime&quot;: &quot;12334312543&quot;
              },
              { &quot;sender&quot;: &quot;45678&quot;,
                &quot;msg&quot;: &quot;check this out!&quot;,
                &quot;icon&quot;: &quot;/message/image?sender=45678&amp;mtime=12334312543&quot;,
                &quot;mtime&quot;: &quot;12334312543&quot;
              }],
       &quot;next_token&quot;: &quot;&quot;
     }</code></pre></li>
<li><p><code>/message/get/sent</code>
 Return all messages i sent out. All the same query rules apply as for the archived messages API call.</p>
<p>Parameters:</p>
<ul>
<li><code>recipient</code> - id of the recipient where i have sent messages</li>
<li><code>mtime</code> - time before or after messages sent, defined by _ops parametrs</li>
</ul>
<p>Example:</p>
<pre><code>  /message/get/sent?recipient=123
  /message/get/sent?recipient=123&amp;mtime=123475658690&amp;_ops=mtime,le</code></pre></li>
<li><p><code>/message/add</code>
Send a message to an account, the following parameters must be specified:</p>
<ul>
<li><code>id</code> - recipient account id</li>
<li><code>msg</code> - text of the message, can be empty if <code>icon</code> property exists</li>
<li><code>icon</code> - icon of the message, it can be base64 encoded image in the query or JSON string if the whole message is posted as JSON or
can be a multipart file upload if submitted via browser, can be omitted if <code>msg/connection/get?type=invite&amp;id=12345</code> property exists.</li>
<li>_nosent - do not save this message in my sent messages</li>
</ul>
<p>Example:</p>
<pre><code>  /message/add?id=12345&amp;msg=Hello
  /message/add?id=12345&amp;msg=this%2Bis%2Bthe%2Bpic&amp;icon=KHFHTDDKH7676758JFGHFDRDEDET....TGJNK%2D</code></pre></li>
<li><p><code>/message/read</code>
Mark a message as read</p>
<p>Example:</p>
<pre><code>  /message/read?sender=12345&amp;mtime=124345656567676</code></pre></li>
<li><p><code>/message/archive</code>
Move a new message to the archive. The required query parameters are <code>sender</code> and <code>mtime</code>.</p>
<p>Example:</p>
<pre><code>  /message/read?sender=12345&amp;mtime=12366676434</code></pre></li>
<li><p><code>/message/update</code>
Update a message, can be used to keep track of read/unread status, etc...</p>
<p>Example:</p>
<pre><code>  /message/update?sender=12345&amp;mtime=124345656567676&amp;read=1</code></pre></li>
<li><p><code>/message/update/archive</code>
Update a message in the archive.</p>
</li>
</ul>
<ul>
<li><p><code>/message/del</code>
Delete new message(s) by <code>sender</code> and/or <code>mtime</code> which must be passed as query parameters. If no mtime is given, all messages from the given sender will be deleted.</p>
<p>Example:</p>
<pre><code>  /message/del?sender=12345&amp;mtime=124345656567676</code></pre></li>
<li><p><code>/message/del/archive</code>
Delete archived message(s) by <code>sender</code> and/or <code>mtime</code> which must be passed as query parameters. If no mtime is given, all messages from the given sender will be deleted.</p>
<p>Example:</p>
<pre><code>  /message/del/archive?sender=12345&amp;mtime=124345656567676</code></pre></li>
<li><p><code>/message/del/sent</code>
Delete the message(s) by <code>recipient</code> and/or <code>mtime</code> which must be passed as query parameters. If no mtime is given, all messages to the given recipient will be deleted.</p>
<p>Example:</p>
<pre><code>  /message/del/sent?recipient=12345&amp;mtime=124345656567676</code></pre></li>
<li><p><code>/message/image</code>
Return the image data for the given message, the required parameters are:</p>
<ul>
<li>sender - id of the sender returned in the by <code>/message/get</code> reply results for every message</li>
<li>mtime - exact timestamp of the message</li>
</ul>
</li>
</ul>
<h2 id="counters">Counters</h2>
<p>The counters API maintains realtime counters for every account records, the counters record may contain many different counter columns for different purposes and
is always cached with whatever cache service is used, by default it is cached by the Web server process on every machine. Web worker processes ask the master Web server
process for the cached records thus only one copy of the cache per machine even in the case of multiple CPU cores.</p>
<p>This is implemented by the <code>counters</code> module from the core. To enable this functionality specify <code>-allow-modules=bk_counters|bk_accounts</code>.</p>
<ul>
<li><p><code>/counter/get</code>
Return counter record for current account with all available columns of if <code>id</code> is given return public columns for given account, it works with <code>bk_counter</code> table
which by default defines some common columns:</p>
<ul>
<li>ping - a counter for general use, can be used to send a notification event to any account by increasing this counter for an account</li>
<li>like0 - how many i liked, how many time i liked someone, i.e. made a new record in bk_connection table with type &#39;like&#39;</li>
<li>like1 - how many liked me, reverse counter, who connected to me with type &#39;like&#39;
More columns can be added to the bk_counter table.</li>
</ul>
<p>NOTE: The columns with suffixes 0 and 1 are special columns that support the Connections API, every time a new connection is created, the type of new connection
is checked against any columns in the bk_counter table, if a property type0 exists and marked in the table description as <code>autoincr</code> then the corresponding
counter property is increased, this is how every time new connection like/dislike/invite/follow is added, the counters in the bk_counter table are increased.</p>
</li>
<li><p><code>/counter/put</code>
Replace my counters record, all values if not specified will be set to 0</p>
</li>
<li><p><code>/counter/incr</code>
Increase one or more counter fields, each column can provide a numeric value and it will be added to the existing value, negative values will be substracted.
if <code>id</code> parameter is specified, only public columns will be increased for other account.</p>
<p>Example:</p>
<pre><code>  /counter/incr?msg_read=5&amp;
  /counter/incr?id=12345&amp;ping=1</code></pre></li>
</ul>
<h2 id="data">Data</h2>
<p>The data API is a generic way to access any table in the database with common operations, as oppose to the any specific APIs above this API only deals with
one table and one record without maintaining any other features like auto counters, cache...</p>
<p><em>Because it exposes the whole database to anybody who has a login it is a good idea to disable this endpoint in the production or provide access callback that verifies
who can access it.</em></p>
<ul>
<li><p>To disable this endpoint completely in the config: <code>deny-modules=data</code></p>
</li>
<li><p>To allow admins to access it only in the config: <code>api-allow-admin=^/data</code></p>
</li>
<li><p>To allow admins to access it only:</p>
<pre><code>api.registerPreProcess(&#39;GET&#39;, &#39;/data&#39;, function(req, status, cb) { if (req.account.type != &quot;admin&quot;) return cb({ status: 401, message: &#39;access denied&#39; }; cb(status)); });</code></pre></li>
</ul>
<p>This is implemented by the <code>data</code> module from the core.</p>
<ul>
<li><p><code>/data/columns</code></p>
</li>
<li><p><code>/data/columns/TABLE</code>
Return columns for all tables or the specific TABLE</p>
</li>
<li><p><code>/data/keys/TABLE</code>
Return primary keys for the given TABLE</p>
</li>
<li><p><code>/data/(select|search|list|get|add|put|update|del|incr|replace)/TABLE</code>
Perform database operation on the given TABLE, all options for the <code>db</code> functiobns are passed as query parametrrs prepended with underscore,
regular parameters are the table columns.</p>
<p>By default the API does not allow table scans without a condition to avoid expensive and long queries, to enable a scan pass <code>_noscan=0</code>.
For this to work the Data API must be configured as unsecure in the config file using the parameter <code>api-unsecure=data</code>.</p>
<p>Some tables like messages and connections perform data convertion before returning the results, mostly splitting combined columns like type into
separate fields. To return raw data pass the parameter <code>_noprocessrows=1</code>.</p>
<p>Example:</p>
<pre><code>  /data/get/bk_account?id=12345
  /data/put/bk_counter?id=12345&amp;like0=1
  /data/select/bk_account?name=john&amp;_ops=name,gt&amp;_select=name,email
  /data/select/bk_connection?_noscan=0&amp;_noprocessrows=1</code></pre></li>
</ul>
<h2 id="pages">Pages</h2>
<p>The pages API provides a simple Wiki like system with Markdown formatting. It keeps all pages in the database table <code>bk_pages</code> and
exposes an API to manage and render pages.</p>
<p>The pages support public mode, all pages with <code>pub</code> set to true will be returning without an account, this must be enabled with <code>api-allow-path=^/pages/(get|select|show)</code>
to work.</p>
<p>All .md files will be rendered into html automatically if there is not _raw=1 query parameter and pages view exists (api-pages-view=pages.html by default).</p>
<p>This is implemented by the <code>pages</code> module from the core. To enable this functionality specify <code>-allow-modules=bk_accounts</code>.</p>
<ul>
<li><p><code>/pages/get/ID</code>
Return a page with given id or the main page if id is empty. If the query parameter <code>_render=1</code> is given, the content will be rendered into html from markdown, otherwise
returns all data as is.</p>
</li>
<li><p><code>/pages/select</code>
Return all pages or only ones which match the query criteria. This potentially scans the whole table to return all pages and
is used to show pages index.</p>
</li>
<li><p><code>/pages/put</code>
Replace or add a new page.</p>
</li>
<li><p><code>/pages/del</code>
Delete a page from the database</p>
</li>
<li><p><code>/pages/show/ID</code>
Render a page with given id, markdown is converted into html using <code>marked</code>. A view must be configured in order to render to work, by default pages.html view
is provided to simply wrap the markdown in the page layout.</p>
</li>
</ul>
<h2 id="system-api">System API</h2>
<p>The system API returns information about the backend statistics, allows provisioning and configuration commands and other internal maintenance functions. By
default is is open for access to all users but same security considerations apply here as for the Data API.</p>
<p>This is implemented by the <code>system</code> module from the core. To enable this functionality specify <code>-allow-modules=accounts</code>.</p>
<ul>
<li><p><code>/system/restart</code>
  Perform restart of the Web processes, this will be done gracefully, only one Web worker process will be restarting while the other processes will keep
  serving requests. The intention is to allow code updates on live systems without service interruption.</p>
</li>
<li><p><code>/system/cache/(init|stats|keys|get|set|put|incr|del|clear)</code>
  Access to the caching functions</p>
</li>
<li><p><code>/system/config/(init)</code>
  Access to the config functions</p>
</li>
<li><p><code>/system/msg/(init|send)</code>
  Access to the messaging functions</p>
</li>
<li><p><code>/system/jobs/(send)</code>
  Access to the jobs functions</p>
</li>
<li><p><code>/system/queue/(init|publish)</code>
  Access to the queue functions</p>
</li>
<li><p><code>/system/params/get</code>
  Return all config parameters applied from the config file(s) or remote database.</p>
</li>
<li><p><code>/system/stats/get</code>
Database pool statistics and other diagnostics</p>
<ul>
<li>latency - how long a pending request waits in queue at this moment</li>
<li>busy - how many busy error responses have been returned so far</li>
<li>pool - database metrics<ul>
<li>response - stats about how long it takes between issuing the db request and till the final moment all records are ready to be sent to the client</li>
<li>queue - stats about db requests at any given moment queued for the execution</li>
<li>cache - db cache response time and metrics</li>
</ul>
</li>
<li>api - Web requests metrics, same structure as for the db pool metrics</li>
<li>url - metrics per url endpoints</li>
</ul>
<p>Individual sub-objects:</p>
<ul>
<li>meter - Things that are measured as events / interval.<ul>
<li>rmean: The average rate since the meter was started.</li>
<li>rcnt: The total of all values added to the meter.</li>
<li>rate: The rate of the meter since the last toJSON() call.</li>
<li>r1m: The rate of the meter biased towards the last 1 minute.</li>
<li>r5m: The rate of the meter biased towards the last 5 minutes.</li>
<li>r15m: The rate of the meter biased towards the last 15 minutes.</li>
</ul>
</li>
<li>queue or histogram - Keeps a reservoir of statistically relevant values biased towards the last 5 minutes to explore their distribution<ul>
<li>hmin: The lowest observed value.</li>
<li>mmax: The highest observed value.</li>
<li>hsum: The sum of all observed values.</li>
<li>hvar: The variance of all observed values.</li>
<li>hmean: The average of all observed values.</li>
<li>hdev: The standard deviation of all observed values.</li>
<li>hcnt: The number of observed values.</li>
<li>hmed: median, 50% of all values in the reservoir are at or below this value.</li>
<li>hp75: See median, 75% percentile.</li>
<li>hp95: See median, 95% percentile.</li>
<li>hp99: See median, 99% percentile.</li>
<li>hp999: See median, 99.9% percentile.</li>
</ul>
</li>
</ul>
<p>Response:</p>
<pre><code>       {
            &quot;id&quot;: &quot;172.31.31.85-25170&quot;,
            &quot;ip&quot;: &quot;172.31.31.85&quot;,
            &quot;mtime&quot;: 1417500027321,
            &quot;ctime&quot;: 1416941754760,
            &quot;type&quot;: &quot;&quot;,
            &quot;host&quot;: &quot;&quot;,
            &quot;pid&quot;: 25170,
            &quot;instance&quot;: &quot;i-d4c89eff&quot;,
            &quot;worker&quot;: 27,
            &quot;latency&quot;: 0,
            &quot;cpus&quot;: 4,
            &quot;mem&quot;: 15774367744,
            &quot;rss_hmin&quot;: 66879488,
            &quot;rss_hmax&quot;: 151891968,
            &quot;rss_hsum&quot;: 2451506479104,
            &quot;rss_hvar&quot;: 254812067010902.66,
            &quot;rss_hmean&quot;: 118895507.98312236,
            &quot;rss_hdev&quot;: 15962833.92793719,
            &quot;rss_hcnt&quot;: 20619,
            &quot;rss_hmed&quot;: 147644416,
            &quot;rss_h75p&quot;: 149262336,
            &quot;rss_h95p&quot;: 150834585.6,
            &quot;rss_h99p&quot;: 151550033.92000002,
            &quot;rss_h999p&quot;: 151886266.368,
            &quot;heap_hmin&quot;: 25790920,
            &quot;heap_hmax&quot;: 72316184,
            &quot;heap_hsum&quot;: 1029889929504,
            &quot;heap_hvar&quot;: 54374337037311.65,
            &quot;heap_hmean&quot;: 49948587.68630874,
            &quot;heap_hdev&quot;: 7373895.648658967,
            &quot;heap_hcnt&quot;: 20619,
            &quot;heap_hmed&quot;: 57480704,
            &quot;heap_h75p&quot;: 61934254,
            &quot;heap_h95p&quot;: 67752391.2,
            &quot;heap_h99p&quot;: 70544797.92,
            &quot;heap_h999p&quot;: 72315029.104,
            &quot;avg_hmin&quot;: 0.04541015625,
            &quot;avg_hmax&quot;: 0.06005859375,
            &quot;avg_hsum&quot;: 938.234375,
            &quot;avg_hvar&quot;: 4.491222722966496e-7,
            &quot;avg_hmean&quot;: 0.04550338886463941,
            &quot;avg_hdev&quot;: 0.0006701658543201448,
            &quot;avg_hcnt&quot;: 20619,
            &quot;avg_hmed&quot;: 0.04541015625,
            &quot;avg_h75p&quot;: 0.04541015625,
            &quot;avg_h95p&quot;: 0.04541015625,
            &quot;avg_h99p&quot;: 0.05078125,
            &quot;avg_h999p&quot;: 0.05997363281250001,
            &quot;free_hmin&quot;: 12879872000,
            &quot;free_hmax&quot;: 13228994560,
            &quot;free_hsum&quot;: 268429937405952,
            &quot;free_hvar&quot;: 5839592954606286,
            &quot;free_hmean&quot;: 13018572064.889277,
            &quot;free_hdev&quot;: 76417229.43555522,
            &quot;free_hcnt&quot;: 20619,
            &quot;free_hmed&quot;: 12908707840,
            &quot;free_h75p&quot;: 12915716096,
            &quot;free_h95p&quot;: 12919331430.4,
            &quot;free_h99p&quot;: 12922073088,
            &quot;free_h999p&quot;: 12922164563.968,
            &quot;util_hmin&quot;: 0.05905642141342145,
            &quot;util_hmax&quot;: 0.0607655708794173,
            &quot;util_hsum&quot;: 1230.6298386264643,
            &quot;util_hvar&quot;: 2.1530671850148948e-7,
            &quot;util_hmean&quot;: 0.059684263961708346,
            &quot;util_hdev&quot;: 0.0004640115499656118,
            &quot;util_hcnt&quot;: 20619,
            &quot;util_hmed&quot;: 0.05920415878947068,
            &quot;util_h75p&quot;: 0.059217278415661254,
            &quot;util_h95p&quot;: 0.05934395790869296,
            &quot;util_h99p&quot;: 0.059361851867105964,
            &quot;util_h999p&quot;: 0.0593659827984017,
            &quot;pool_name&quot;: &quot;dynamodb&quot;,
            &quot;pool_que_rate&quot;: 0,
            &quot;pool_que_rcnt&quot;: 1989,
            &quot;pool_que_rmean&quot;: 0.0035627883554577716,
            &quot;pool_que_r1m&quot;: 0,
            &quot;pool_que_r5m&quot;: 0,
            &quot;pool_que_r15m&quot;: 0,
            &quot;pool_que_hmin&quot;: 0,
            &quot;pool_que_hmax&quot;: 230,
            &quot;pool_que_hsum&quot;: 45843,
            &quot;pool_que_hvar&quot;: 366.86587852909315,
            &quot;pool_que_hmean&quot;: 23.048265460030166,
            &quot;pool_que_hdev&quot;: 19.15374319889178,
            &quot;pool_que_hcnt&quot;: 1989,
            &quot;pool_que_hmed&quot;: 21,
            &quot;pool_que_h75p&quot;: 23,
            &quot;pool_que_h95p&quot;: 33,
            &quot;pool_que_h99p&quot;: 126.42000000000007,
            &quot;pool_que_h999p&quot;: 225.971,
            &quot;pool_req_hmin&quot;: 1,
            &quot;pool_req_hmax&quot;: 2,
            &quot;pool_req_hsum&quot;: 1991,
            &quot;pool_req_hvar&quot;: 0.001005024617286425,
            &quot;pool_req_hmean&quot;: 1.0010055304172951,
            &quot;pool_req_hdev&quot;: 0.03170212322994195,
            &quot;pool_req_hcnt&quot;: 1989,
            &quot;pool_req_hmed&quot;: 1,
            &quot;pool_req_h75p&quot;: 1,
            &quot;pool_req_h95p&quot;: 1,
            &quot;pool_req_h99p&quot;: 1,
            &quot;pool_req_h999p&quot;: 1.9710000000000036,
            &quot;pool_count&quot;: 0,
            &quot;pool_req_0&quot;: 2,
            &quot;pool_cache_rate&quot;: 0.1303780964797914,
            &quot;pool_cache_rcnt&quot;: 284,
            &quot;pool_cache_rmean&quot;: 0.0005087436344326025,
            &quot;pool_cache_r1m&quot;: 0,
            &quot;pool_cache_r5m&quot;: 0,
            &quot;pool_cache_r15m&quot;: 0,
            &quot;pool_cache_hmin&quot;: 0,
            &quot;pool_cache_hmax&quot;: 2,
            &quot;pool_cache_hsum&quot;: 70,
            &quot;pool_cache_hvar&quot;: 0.19345045538247163,
            &quot;pool_cache_hmean&quot;: 0.24647887323943662,
            &quot;pool_cache_hdev&quot;: 0.4398300301053483,
            &quot;pool_cache_hcnt&quot;: 284,
            &quot;pool_cache_hmed&quot;: 0,
            &quot;pool_cache_h75p&quot;: 0,
            &quot;pool_cache_h95p&quot;: 1,
            &quot;pool_cache_h99p&quot;: 1,
            &quot;pool_cache_h999p&quot;: 2,
            &quot;pool_hits&quot;: 239,
            &quot;pool_misses&quot;: 45,
            &quot;cache_inserted&quot;: 484,
            &quot;cache_deleted&quot;: 310,
            &quot;cache_cleanups&quot;: 0,
            &quot;cache_hits&quot;: 7642,
            &quot;cache_misses&quot;: 1411,
            &quot;cache_max&quot;: 1000000,
            &quot;cache_size&quot;: 61586,
            &quot;cache_count&quot;: 174,
            &quot;api_que_hmin&quot;: 1,
            &quot;api_que_hmax&quot;: 6,
            &quot;api_que_hsum&quot;: 13237,
            &quot;api_que_hvar&quot;: 0.005674280465987009,
            &quot;api_que_hmean&quot;: 1.0024992426537414,
            &quot;api_que_hdev&quot;: 0.07532782000022972,
            &quot;api_que_hcnt&quot;: 13204,
            &quot;api_que_hmed&quot;: 1,
            &quot;api_que_h75p&quot;: 1,
            &quot;api_que_h95p&quot;: 1,
            &quot;api_que_h99p&quot;: 1,
            &quot;api_que_h999p&quot;: 2,
            &quot;api_nreq&quot;: 1,
            &quot;api_req_rate&quot;: 0,
            &quot;api_req_rcnt&quot;: 13203,
            &quot;api_req_rmean&quot;: 0.02365120609256502,
            &quot;api_req_r1m&quot;: 0,
            &quot;api_req_r5m&quot;: 0,
            &quot;api_req_r15m&quot;: 0,
            &quot;api_req_hmin&quot;: 0,
            &quot;api_req_hmax&quot;: 536,
            &quot;api_req_hsum&quot;: 20115,
            &quot;api_req_hvar&quot;: 89.12554520926801,
            &quot;api_req_hmean&quot;: 1.5235173824130879,
            &quot;api_req_hdev&quot;: 9.440632669968046,
            &quot;api_req_hcnt&quot;: 13203,
            &quot;api_req_hmed&quot;: 1,
            &quot;api_req_h75p&quot;: 1,
            &quot;api_req_h95p&quot;: 1,
            &quot;api_req_h99p&quot;: 33.13000000000011,
            &quot;api_req_h999p&quot;: 99.36200000000008,
            &quot;url_message_get_rate&quot;: 0,
            &quot;url_message_get_rcnt&quot;: 24,
            &quot;url_message_get_rmean&quot;: 0.00004299242196761214,
            &quot;url_message_get_r1m&quot;: 0,
            &quot;url_message_get_r5m&quot;: 0,
            &quot;url_message_get_r15m&quot;: 0,
            &quot;url_message_get_hmin&quot;: 16,
            &quot;url_message_get_hmax&quot;: 71,
            &quot;url_message_get_hsum&quot;: 792,
            &quot;url_message_get_hvar&quot;: 208.34782608695653,
            &quot;url_message_get_hmean&quot;: 33,
            &quot;url_message_get_hdev&quot;: 14.434258764722092,
            &quot;url_message_get_hcnt&quot;: 24,
            &quot;url_message_get_hmed&quot;: 30.5,
            &quot;url_message_get_h75p&quot;: 40.75,
            &quot;url_message_get_h95p&quot;: 68,
            &quot;url_message_get_h99p&quot;: 71,
            &quot;url_message_get_h999p&quot;: 71,
            &quot;url_message_get_0&quot;: 0,
            &quot;api_req_0&quot;: 20,
            &quot;url_ping_rate&quot;: 0,
            &quot;url_ping_rcnt&quot;: 12407,
            &quot;url_ping_rmean&quot;: 0.022226981327796796,
            &quot;url_ping_r1m&quot;: 0,
            &quot;url_ping_r5m&quot;: 0,
            &quot;url_ping_r15m&quot;: 0,
            &quot;url_ping_hmin&quot;: 0,
            &quot;url_ping_hmax&quot;: 4,
            &quot;url_ping_hsum&quot;: 6915,
            &quot;url_ping_hvar&quot;: 0.25785489698686204,
            &quot;url_ping_hmean&quot;: 0.5573466591440316,
            &quot;url_ping_hdev&quot;: 0.5077941482400737,
            &quot;url_ping_hcnt&quot;: 12407,
            &quot;url_ping_hmed&quot;: 1,
            &quot;url_ping_h75p&quot;: 1,
            &quot;url_ping_h95p&quot;: 1,
            &quot;url_ping_h99p&quot;: 1,
            &quot;url_ping_h999p&quot;: 2,
            &quot;url_ping_0&quot;: 5,
            &quot;url_image_account_rate&quot;: 0,
            &quot;url_image_account_rcnt&quot;: 95,
            &quot;url_image_account_rmean&quot;: 0.00017084907295404685,
            &quot;url_image_account_r1m&quot;: 0,
            &quot;url_image_account_r5m&quot;: 0,
            &quot;url_image_account_r15m&quot;: 0,
            &quot;url_image_account_hmin&quot;: 17,
            &quot;url_image_account_hmax&quot;: 121,
            &quot;url_image_account_hsum&quot;: 4295,
            &quot;url_image_account_hvar&quot;: 372.42329227323637,
            &quot;url_image_account_hmean&quot;: 45.21052631578947,
            &quot;url_image_account_hdev&quot;: 19.29827174317007,
            &quot;url_image_account_hcnt&quot;: 95,
            &quot;url_image_account_hmed&quot;: 42,
            &quot;url_image_account_h75p&quot;: 51,
            &quot;url_image_account_h95p&quot;: 89.59999999999991,
            &quot;url_image_account_h99p&quot;: 121,
            &quot;url_image_account_h999p&quot;: 121,
            &quot;url_image_account_0&quot;: 0,
            &quot;incr_follow_0&quot;: 0,
            &quot;api_bad_0&quot;: 3,
            &quot;url_account_update_rate&quot;: 0,
            &quot;url_account_update_rcnt&quot;: 6,
            &quot;url_account_update_rmean&quot;: 0.000010813705805470248,
            &quot;url_account_update_r1m&quot;: 0,
            &quot;url_account_update_r5m&quot;: 0,
            &quot;url_account_update_r15m&quot;: 0,
            &quot;url_account_update_hmin&quot;: 53,
            &quot;url_account_update_hmax&quot;: 182,
            &quot;url_account_update_hsum&quot;: 573,
            &quot;url_account_update_hvar&quot;: 2041.5,
            &quot;url_account_update_hmean&quot;: 95.5,
            &quot;url_account_update_hdev&quot;: 45.18296139032943,
            &quot;url_account_update_hcnt&quot;: 6,
            &quot;url_account_update_hmed&quot;: 82,
            &quot;url_account_update_h75p&quot;: 120.5,
            &quot;url_account_update_h95p&quot;: 182,
            &quot;url_account_update_h99p&quot;: 182,
            &quot;url_account_update_h999p&quot;: 182,
            &quot;url_account_update_0&quot;: 0,
            &quot;auth_add_0&quot;: 0,
            &quot;url_account_get_rate&quot;: 0,
            &quot;url_account_get_rcnt&quot;: 9,
            &quot;url_account_get_rmean&quot;: 0.0001993511695335063,
            &quot;url_account_get_r1m&quot;: 0,
            &quot;url_account_get_r5m&quot;: 0,
            &quot;url_account_get_r15m&quot;: 0,
            &quot;url_account_get_hmin&quot;: 2,
            &quot;url_account_get_hmax&quot;: 100,
            &quot;url_account_get_hsum&quot;: 435,
            &quot;url_account_get_hvar&quot;: 844.0000000000001,
            &quot;url_account_get_hmean&quot;: 48.333333333333336,
            &quot;url_account_get_hdev&quot;: 29.051678092667903,
            &quot;url_account_get_hcnt&quot;: 9,
            &quot;url_account_get_hmed&quot;: 46,
            &quot;url_account_get_h75p&quot;: 67,
            &quot;url_account_get_h95p&quot;: 100,
            &quot;url_account_get_h99p&quot;: 100,
            &quot;url_account_get_h999p&quot;: 100,
            &quot;url_account_get_0&quot;: 1,
            &quot;url_system_stats_rate&quot;: 0,
            &quot;url_system_stats_rcnt&quot;: 1,
            &quot;url_system_stats_rmean&quot;: 0.04501665616278023,
            &quot;url_system_stats_r1m&quot;: 0,
            &quot;url_system_stats_r5m&quot;: 0,
            &quot;url_system_stats_r15m&quot;: 0,
            &quot;url_system_stats_hmin&quot;: 3,
            &quot;url_system_stats_hmax&quot;: 3,
            &quot;url_system_stats_hsum&quot;: 3,
            &quot;url_system_stats_hmean&quot;: 3,
            &quot;url_system_stats_hdev&quot;: 0,
            &quot;url_system_stats_hcnt&quot;: 1,
            &quot;url_system_stats_hmed&quot;: 3,
            &quot;url_system_stats_h75p&quot;: 3,
            &quot;url_system_stats_h95p&quot;: 3,
            &quot;url_system_stats_h99p&quot;: 3,
            &quot;url_system_stats_h999p&quot;: 3,
            &quot;url_system_stats_0&quot;: 2
        }</code></pre></li>
</ul>
<h1 id="author">Author</h1>
<p>  Vlad Seryakov</p>
<p>Check out the <a href="http://bkjs.io">Documentation</a> for more details.</p>
<h2 id="configuration-parameters">Configuration parameters</h2>
<ul>
<li><code>help</code> - Print help and exit</li>
<li><code>log</code> - Set debugging level to any of test,dev,debug,info,notice,log,warn,error,none</li>
<li><code>log-filter</code> - Enable debug filters, format is: label,... to enable, and !label,... to disable. Only first argument is used for label in logger.debug</li>
<li><code>log-file</code> - Log to a file, if not specified used default logfile, disables syslog</li>
<li><code>log-ignore</code> - Regexp with property names which must not be exposed in the log when using custom logger inspector</li>
<li><code>log-inspect</code> - Install custom secure logger inspection instead of util.inspect</li>
<li><code>syslog</code> - Write all logging messages to syslog, connect to the local syslog server over Unix domain socket</li>
<li><code>console</code> - All logging goes to the console resetting all previous log related settings, this is used in the development mode mostly</li>
<li><code>home</code> - Specify home directory for the server, the server will try to chdir there or exit if it is not possible, the directory must exist</li>
<li><code>conf-file</code> - Name of the config file to be loaded instead of the default etc/config, can be relative or absolute path</li>
<li><code>err-file</code> - Path to the error log file where daemon will put app errors and crash stacks</li>
<li><code>etc-dir</code> - Path where to keep config files</li>
<li><code>tmp-dir</code> - Path where to keep temp files</li>
<li><code>spool-dir</code> - Path where to keep modifiable files</li>
<li><code>log-dir</code> - Path where to keep other log files, log-file and err-file are not affected by this</li>
<li><code>files-dir</code> - Path where to keep uploaded files</li>
<li><code>images-dir</code> - Path where to keep images</li>
<li><code>web-path</code> - Path where to keep web pages and other static files to be served by the web servers</li>
<li><code>views-path</code> - Path where to keep virtual hosts web pages, every subdirectory name is a host name to match with Host: header, www. is always stripped before matching vhost directory</li>
<li><code>modules-path</code> - Directory from where to load modules, these are the backendjs modules but in the same format and same conventions as regular node.js modules, the format of the files is NAME_{web,worker,shell}.js. The modules can load any other files or directories, this is just an entry point</li>
<li><code>locales-path</code> - Path where to keep locale translations</li>
<li><code>allow-packages</code> - NPM packages to load on startup, the modules, locales, viewes, web subfolders from the package will be added automatically to the system paths, modules will be loaded if present, the config file in etc subfolder will be parsed if present</li>
<li><code>role</code> - Override servers roles, this may have very strange side effects and should only be used for testing purposes</li>
<li><code>uid</code> - User id or name to switch after startup if running as root, used by Web servers and job workers</li>
<li><code>gid</code> - Group id or name to switch after startup if running to root</li>
<li><code>force-uid</code> - Drop privileges if running as root by all processes as early as possibly, this reqiures uid being set to non-root user. A convenient switch to start the backend without using any other tools like su or sudo.</li>
<li><code>umask</code> - Permissions mask for new files, calls system umask on startup, if not specified the current umask is used</li>
<li><code>port</code> - port to listen for the HTTP server, this is global default</li>
<li><code>bind</code> - Bind to this address only, if not specified listen on all interfaces</li>
<li><code>backlog</code> - The maximum length of the queue of pending connections, used by HTTP server in listen.</li>
<li><code>ws-port</code> - port to listen for WebSocket server, it can be the same as HTTP/S ports to co-exist on existing web servers</li>
<li><code>ws-bind</code> - Bind to this address only for WebSocket, if not specified listen on all interfaces, only when the port is different from existing web ports</li>
<li><code>ssl-port</code> - port to listen for HTTPS server, this is global default, be advised that proxy-port takes precedence</li>
<li><code>ssl-bind</code> - Bind to this address only for HTTPS server, if not specified listen on all interfaces</li>
<li><code>ssl-key</code> - Path to SSL prvate key</li>
<li><code>ssl-cert</code> - Path to SSL certificate</li>
<li><code>ssl-pfx</code> - A string or Buffer containing the private key, certificate and CA certs of the server in PFX or PKCS12 format. (Mutually exclusive with the key, cert and ca options.)</li>
<li><code>ssl-ca</code> - An array of strings or Buffers of trusted certificates in PEM format. If this is omitted several well known root CAs will be used, like VeriSign. These are used to authorize connections.</li>
<li><code>ssl-passphrase</code> - A string of passphrase for the private key or pfx</li>
<li><code>ssl-crl</code> - Either a string or list of strings of PEM encoded CRLs (Certificate Revocation List)</li>
<li><code>ssl-ciphers</code> - A string describing the ciphers to use or exclude. Consult <a href="http://www.openssl.org/docs/apps/ciphers.html#CIPHER_LIST_FORMAT">http://www.openssl.org/docs/apps/ciphers.html#CIPHER_LIST_FORMAT</a> for details on the format</li>
<li><code>ssl-request-cert</code> - If true the server will request a certificate from clients that connect and attempt to verify that certificate. </li>
<li><code>concurrency</code> - How many simultaneous tasks to run at the same time inside one process, this is used by async module only to perform several tasks at once, this is not multithreading but and only makes sense for I/O related tasks</li>
<li><code>timeout</code> - HTTP request idle timeout for servers in ms, how long to keep the connection socket open, this does not affect Long Poll requests</li>
<li><code>daemon</code> - Daemonize the process, go to the background, can be specified only in the command line</li>
<li><code>shell</code> - Run command line shell, load the backend into the memory and prompt for the commands, can be specified only in the command line</li>
<li><code>monitor</code> - For production use, monitors the master and Web server processes and restarts if crashed or exited, can be specified only in the command line</li>
<li><code>master</code> - Start the master server, can be specified only in the command line, this process handles job schedules and starts Web server, keeps track of failed processes and restarts them</li>
<li><code>web</code> - Start Web server processes, spawn workers that listen on the same port, for use without master process which starts Web servers automatically</li>
<li><code>proxy-port</code> - Start the HTTP reverse proxy server, all Web workers will listen on different ports and will be load-balanced by the proxy, the proxy server will listen on global HTTP port and all workers will listen on ports starting with the proxy-port</li>
<li><code>proxy-ssl</code> - Start HTTPS reverse proxy to accept incoming SSL requests, ssl-key/cert must be defined</li>
<li><code>salt</code> - Set random or specific salt value to be used for consistent suuid generation</li>
<li><code>app-name</code> - Set appName and version explicitely an skip reading it from package.json, it can be just a name or name-version</li>
<li><code>app-package</code> - NPM package containing the application package.json, it will be added to the list of package.json files for app name and version discovery. The package must be included in the -allow-packages list.</li>
<li><code>instance-tag</code> - Set instance tag explicitely, skip all meta data checks for it</li>
<li><code>instance-region</code> - Set instance region explicitely, skip all meta data checks for it</li>
<li><code>instance-zone</code> - Set instance zone explicitely, skip all meta data checks for it</li>
<li><code>instance-job</code> - Enables remote job mode, it means the backendjs is running in the cloud to execute a job or other task and can be terminated during the idle timeout</li>
<li><code>run-mode</code> - Running mode for the app, used to separate different running environment and configurations. DNS TXT configurable.</li>
<li><code>no-monitor</code> - Disable monitor process, for cases when the master will be monitored by other tool like monit...</li>
<li><code>no-master</code> - Do not start the master process</li>
<li><code>no-watch</code> - Disable source code watcher</li>
<li><code>no-web</code> - Disable Web server processes, without this flag Web servers start by default</li>
<li><code>no-db</code> - Do not initialize DB drivers</li>
<li><code>no-dns</code> - Do not use DNS configuration during the initialization</li>
<li><code>no-configure</code> - Do not run configure hooks during the initialization</li>
<li><code>repl-port-([a-z]+)$</code> - Base REPL port for process role (server, master, web, worker), if specified it initializes REPL in the processes, for workers the port is computed by adding a worker id to the base port, for example if specified <code>-repl-port-web 2090</code> then a web worker will use any available 2091,2092...</li>
<li><code>repl-bind</code> - Listen only on specified address for REPL server in the master process</li>
<li><code>repl-file</code> - User specified file for REPL history</li>
<li><code>worker</code> - Set this process as a worker even it is actually a master, this skips some initializations</li>
<li><code>allow-modules-?(.+)?</code> - A regexp with modules name to be loaded on startup, only matched modules will be loaded, basename of the file is matched only, no path or extension, it is per role or global if no role is provided</li>
<li><code>deny-modules-?(.+)?</code> - A regexp with modules names that will never be loaded even if allowed, this is for blacklisted modules, can be per role or global for all processes</li>
<li><code>logwatcher-from</code> - Email address to send logwatcher notifications from, for cases with strict mail servers accepting only from known addresses</li>
<li><code>logwatcher-interval</code> - How often to check for errors in the log files in minutes</li>
<li><code>logwatcher-any-range</code> - Number of lines for matched channel <code>any</code> to be attached to the previous matched channel, if more than this number use the channel <code>any</code> on its own</li>
<li><code>logwatcher-match-[a-z]+</code> - Regexp patterns that match conditions for logwatcher notifications, this is in addition to default backend logger patterns, suffix defines the log channel to use, like error, warning.... Special channel <code>any</code> is reserved to send matched lines to the previously matched channel if within configured range. Example: <code>-logwatcher-match-error=^failed:</code> <code>-logwatcher-match-any=line:[0-9]+</code></li>
<li><code>logwatcher-send-[a-z]+</code> - Email address or other supported transport for the logwatcher notifications, the monitor process scans system and backend log files for errors and sends them to this email address, if not specified no log watching will happen, each channel must define a transport separately, one of error, warning, info, all. Supported transports: table://TABLE, <a href="http://URL">http://URL</a>, sns://ARN, ses://EMAIL, email@addr. Example: <code>-logwatcher-send-error=help@error.com</code></li>
<li><code>logwatcher-ignore-[a-z]+</code> - Regexp with patterns that need to be ignored by the logwatcher process, it is added to the list of existing patterns for each specified channel separately</li>
<li><code>logwatcher-once-[a-z0-9]+</code> - Regexp with patterns that need to be included only once by the logwatcher process, it is added to the list of existng patterns by tag to keep track each pattern separately, example: -logwatcher-once-restart &#39;restarting.+&#39; -logwatcher-once-recon &#39;reconnecting:.+&#39;</li>
<li><code>logwatcher-file(-[a-z]+)?</code> - Add a file to be watched by the logwatcher, it will use all configured match patterns</li>
<li><code>user-agent</code> - Add HTTP user-agent header to be used in HTTP requests, for scrapers or other HTTP requests that need to be pretended coming from Web browsers</li>
<li><code>backend-host</code> - Host of the master backend, can be used for backend nodes communications using core.sendRequest function calls with relative URLs, also used in tests.</li>
<li><code>backend-login</code> - Credentials login for the master backend access when using core.sendRequest</li>
<li><code>backend-secret</code> - Credentials secret for the master backend access when using core.sendRequest</li>
<li><code>host-name</code> - Hostname/domain to use for communications, default is current domain of the host machine</li>
<li><code>config-domain</code> - Domain to query for configuration TXT records, must be specified to enable DNS configuration</li>
<li><code>watch</code> - Watch sources directory for file changes to restart the server, for development only, the backend module files will be added to the watch list automatically, so only app specific directores should be added. In the production -monitor must be used.</li>
<li><code>watch-ignore</code> - Files to be ignored by the watcher</li>
<li><code>watch-match</code> - Files to be watched, .js and .css is the default</li>
<li><code>watch-web</code> - List of directories to be watched for file modifications and execute a <code>buildWeb</code> command to produce bundles, apps, etc... Relative paths will be applied to all packages, example: web/js,web/css</li>
<li><code>build-web</code> - Command to run on web files modifications, to be used with tools like minify/uglify</li>
<li><code>locales</code> - A list of locales to load from the locales/ directory, only language name must be specified, example: en,es. It enables internal support for <code>res.__</code> and <code>req.__</code> methods that can be used for translations, for each request the internal language header will be honored forst, then HTTP Accept-Language</li>
<li><code>no-locales</code> - Do not load locales on start</li>
<li><code>email-from</code> - Email address to be used when sending emails from the backend</li>
<li><code>email-transport</code> - Send emails via supported transports: ses:, sendgrid://?key=SG, if not set default SMTP settings are used</li>
<li><code>smtp-(.+)</code> - SMTP server parameters, user, password, host, ssl, tls...see emailjs for details</li>
<li><code>tmp-watcher-(.+)</code> - How long to keep files per subdirectory in seconds</li>
<li><code>stop-on-error</code> - Exit the process on any error when loading modules, for dev purposes</li>
<li><code>aws-key</code> - AWS access key</li>
<li><code>aws-secret</code> - AWS access secret</li>
<li><code>aws-token</code> - AWS secuiry token</li>
<li><code>aws-region</code> - AWS region</li>
<li><code>aws-zone</code> - AWS availability zone</li>
<li><code>aws-sdk-profile</code> - AWS SDK profile to use when reading credentials file</li>
<li><code>aws-sns-app-arn</code> - SNS Platform application ARN to be used for push notifications</li>
<li><code>aws-key-name</code> - AWS instance keypair name for remote job instances or other AWS commands</li>
<li><code>aws-elb-name</code> - AWS ELB name to be registered with on start up or other AWS commands</li>
<li><code>aws-target-group</code> - AWS ELB target group to be registered with on start up or other AWS commands</li>
<li><code>aws-elastic-ip</code> - AWS Elastic IP to be associated on start</li>
<li><code>aws-host-name</code> - List of hosts to update in Route54 zone with the current private IP address, hosts must be in FQDN format, supports @..@ core.instance placeholders</li>
<li><code>aws-iam-profile</code> - IAM instance profile name for instances or commands</li>
<li><code>aws-image-id</code> - AWS image id to be used for instances or commands</li>
<li><code>aws-subnet-id</code> - AWS subnet id to be used for instances or commands</li>
<li><code>aws-vpc-id</code> - AWS VPC id to be used for instances or commands</li>
<li><code>aws-group-id</code> - AWS security group(s) to be used for instances or commands</li>
<li><code>aws-instance-type</code> - AWS instance type to launch on demand</li>
<li><code>aws-account-id</code> - AWS account id if not running on an instance</li>
<li><code>aws-eni-id</code> - AWS Elastic Network Interfaces to attach on start, format is: eni[:index],eni...</li>
<li><code>aws-config-parameters</code> - Prefix for AWS Config Parameters Store to load and parse as config before initializing the database pools, example: /bkjs/config/</li>
<li><code>aws-set-parameters</code> - AWS Config Parameters Store to set on start, supports @..@ core.instance placeholders: format is: path:value,....</li>
<li><code>aws-ddb-read-capacity</code> - Default DynamoDB read capacity for all tables</li>
<li><code>aws-ddb-write-capacity</code> - Default DynamoDB write capacity for all tables</li>
<li><code>aws-ddb-shard-concurrency</code> - DynamoDB Streams number of shards to process in parallel</li>
<li><code>aws-ddb-shard-records-count</code> - DynamoDB Streams records count in a batch</li>
<li><code>aws-ddb-shard-retry-count</code> - DynamoDB Streams number of times to read records from an open shard with no records</li>
<li><code>aws-ddb-shard-retry-closed-count</code> - DynamoDB Streams number of times to read records from a closed shard</li>
<li><code>aws-ddb-shard-retry-timeout</code> - DynamoDB Streams min timeout in ms between retries on empty records, exponential backoff is used</li>
<li><code>aws-ddb-shard-retry-max-timeout</code> - DynamoDB Streams max timeout in ms on empty records, once reached the timeout is reset back to the min and exponential backoff starts again</li>
<li><code>ipc-ping-interval</code> - Interval for a worker keep-alive pings, if not received within this period it will be killed</li>
<li><code>ipc-system-queue</code> - System queue name to subscribe for control messages, this is a PUB/SUB queue to process system messages like restart, re-init config,...</li>
<li><code>ipc-lru-max</code> - Max number of items in the LRU cache, this cache is managed by the master Web server process and available to all Web processes maintaining only one copy per machine, Web proceses communicate with LRU cache via IPC mechanism between node processes</li>
<li><code>ipc-lru-max-(.+)</code> - Max number of items in the local LRU cache by process role, works the same way as the global one but only is set for the specified process role</li>
<li><code>ipc-cache-?([a-z0-9]+)?</code> - An URL that points to the cache server in the format <code>PROTO://HOST[:PORT]?PARAMS</code>, to use for caching in the API/DB requests, default is local LRU cache, multiple caches can be defined with unique names, all params starting with <code>bk-</code> will be copied into the options without the prefix and removed from the url, the rest of params will be left in the url</li>
<li><code>ipc-queue-?([a-z0-9]+)?</code> - An URL that points to the queue server in the format <code>PROTO://HOST[:PORT]?PARAMS</code>, to use for PUB/SUB or job queues, default is no local queue, multiple queues can be defined with unique names, params are handled the same way as for cache</li>
<li><code>ipc-cache(-([a-z0-9]+)?-?options)-(.+)$</code> - Additional parameters for cache clients, specific to each implementation, Example: <code>-ipc-cache-options-ttl 3000</code></li>
<li><code>ipc-queue(-([a-z0-9]+)?-?options)-(.+)$</code> - Additional parameters for queue clients, specific to each implementation, Example: <code>-ipc-queue-redis-options-max_attempts 3</code></li>
<li><code>db-pool</code> - Default pool to be used for db access without explicit pool specified. DNS TXT configurable.</li>
<li><code>db-name</code> - Default database name to be used for default connections in cases when no db is specified in the connection url</li>
<li><code>db-create-tables</code> - Create tables in the database or perform table upgrades for new columns in all pools, only master processes can perform this operation, never workers</li>
<li><code>db-cache-tables</code> - List of tables that can be cached: bk_auth, bk_counter. This list defines which DB calls will cache data with currently configured cache. This is global for all db pools.</li>
<li><code>db-cache-pools</code> - List of pools which trigger cache flushes on update.</li>
<li><code>db-cache-sync</code> - List of tables that perform synchronized cache updates before returning from a DB call, by default cache updates are done in the background</li>
<li><code>db-cache-keys-([a-z0-9_]+)-(.+)</code> - List of columns to be used for the table cache, all update operations will flush the cache if the cache key can be created from the record columns. This is for ad-hoc and caches to be used for custom selects which specified the cache key.</li>
<li><code>db-describe-tables</code> - A JSON object with table descriptions to be merged with the existing definitions</li>
<li><code>db-cache-ttl</code> - Default global TTL for cached tables</li>
<li><code>db-cache-ttl-(.+)</code> - TTL in milliseconds for each individual table being cached</li>
<li><code>db-cache-name-(.+)</code> - Cache client name to use for each table instead of the default in order to split cache usage for different tables, it can be just a table name or <code>pool.table</code></li>
<li><code>db-cache2-(.+)</code> - Tables with TTL for level2 cache, i.e. in the local process LRU memory. It works before the primary cache and keeps records in the local LRU cache for the given amount of time, the TTL is in ms and must be greater than zero for level 2 cache to work. Make sure <code>ipc-lru-max-</code> is properly configured for each process role</li>
<li><code>db-jscache2-(.+)</code> - Same as cache2 but keeps references to the actual objects in the heap</li>
<li><code>db-jscache2-max</code> - Max number of items to keep in the Javascript Level 2 cache</li>
<li><code>db-custom-column-([a-z0-9_]+)-(.+)</code> - A column that is allowed to be used in any table, the name is a regexp with the value to be a type</li>
<li><code>db-local</code> - Local database pool for properties, cookies and other local instance only specific stuff</li>
<li><code>db-config</code> - Configuration database pool to be used to retrieve config parameters from the database, must be defined to use remote db for config parameters, set to <code>default</code> to use current default pool</li>
<li><code>db-config-interval</code> - Interval between loading configuration from the database configured with -db-config, in minutes, 0 disables refreshing config from the db</li>
<li><code>db-config-roles</code> - Roles to assume when pulling config parameters from the config table</li>
<li><code>db-cache-columns-interval</code> - How often in minutes to refresh tables columns from the database, it calls cacheColumns for each pool which supports it</li>
<li><code>db-match-tables-(.+)</code> - Table columns to be returned by matching the regexp, for ad-hoc tables</li>
<li><code>db-skip-drop</code> - A pattern of table names which will skipped in db.drop operations to prevent accidental table deletion</li>
<li><code>db-([a-z0-9]+)-pool$</code> - A database pool name, depending on the driver it can be an URL, name or pathname, examples of db pools: <code>-db-pgsql-pool, -db-dynamodb-pool</code>, examples of urls: <code>postgresql://[user:password@]hostname[:port]/db, mysql://[user:password@]hostname/db, mongodb://hostname[:port]/dbname, cql://[user:password@]hostname[:port]/dbname</code></li>
<li><code>db-([a-z0-9]+)-pool-(disabled)$</code> - Disable the specified pool but keep the configuration</li>
<li><code>db-([a-z0-9]+)-pool-(max)$</code> - Max number of open connections for a pool, default is Infinity</li>
<li><code>db-([a-z0-9]+)-pool-(min)$</code> - Min number of open connections for a pool</li>
<li><code>db-([a-z0-9]+)-pool-(idle)$</code> - Number of ms for a db pool connection to be idle before being destroyed</li>
<li><code>db-([a-z0-9]+)-pool-tables$</code> - A DB pool tables, pattern of tables that belong to this pool only</li>
<li><code>db-([a-z0-9]+)-pool-(connect)$</code> - Connect options for a DB pool driver for new connection, driver specific</li>
<li><code>db-([a-z0-9]+)-pool-options-([a-zA-Z0-9_.-]+)$</code> - General options for a DB pool</li>
<li><code>db-([a-z0-9]+)-pool-(cache-columns)$</code> - Enable caching table columns for this pool if it supports it</li>
<li><code>db-([a-z0-9]+)-pool-(create-tables)$</code> - Create tables for this pool on startup</li>
<li><code>db-([a-z0-9]+)-pool-cache2-(.+)</code> - Level 2 cache TTL for the specified pool and table, data is JSON strings in the LRU cache</li>
<li><code>db-([a-z0-9]+)-pool-jscache2-(.+)</code> - Level 2 cache TTL for the specified pool and table, data is Js objects in the heap</li>
<li><code>api-images-url</code> - URL where images are stored, for cases of central image server(s), must be full URL with optional path</li>
<li><code>api-images-s3</code> - S3 bucket name where to store and retrieve images</li>
<li><code>api-images-raw</code> - Return raw urls for the images, requires images-url to be configured. The path will reflect the actual 2 level structure and account id in the image name</li>
<li><code>api-images-s3-options</code> - S3 options to sign images urls, may have expires:, key:, secret: properties</li>
<li><code>api-images-ext</code> - Default image extension to use when saving images</li>
<li><code>api-files-raw</code> - Return raw urls for the files, requires files-url to be configured. The path will reflect the actual 2 level structure and account id in the file name</li>
<li><code>api-files-url</code> - URL where files are stored, for cases of central file server(s), must be full URL with optional path</li>
<li><code>api-files-s3</code> - S3 bucket name where to store files uploaded with the File API</li>
<li><code>api-max-latency</code> - Max time in ms for a request to wait in the queue, if exceeds this value server returns too busy error</li>
<li><code>api-max-cpu-util</code> - Max CPU utilization allowed, if exceeds this value server returns too busy error</li>
<li><code>api-max-memory-heap</code> - Max number of bytes of V8 heap allowed, if exceeds this value server returns too busy error</li>
<li><code>api-max-memory-rss</code> - Max number of bytes in RSS memory allowed, if exceeds this value server returns too busy error</li>
<li><code>api-max-request-queue</code> - Max number of requests in the processing queue, if exceeds this value server returns too busy error</li>
<li><code>api-no-access-log</code> - Disable access logging in both file or syslog</li>
<li><code>api-access-log-file</code> - File for access logging</li>
<li><code>api-access-log-fields</code> - Additional fields from the request or account to put in the access log, prefix defines where the field is lcoated: q: - query, h: - headers, a: - account otherwise from the request, Example: -api-log-fields h:Referer,a:name,q:action</li>
<li><code>api-salt</code> - Salt to be used for scrambling credentials or other hashing activities</li>
<li><code>api-no-static</code> - Disable static files from /web folder, no .js or .html files will be served by the server</li>
<li><code>api-static-options-(.+)</code> - Options to pass to serve-static module: maxAge, dotfiles, etag, redirect, fallthrough, extensions, index, lastModified</li>
<li><code>api-vhost-path-([^/]+)</code> - Define a virtual host regexp to be matched against the hostname header to serve static content from a different root, a vhost path must be inside the web directory, if the regexp starts with !, that means negative match, example: api-vhost-path-test_dir=test.com$</li>
<li><code>api-no-vhost-path</code> - Add to the list of URL paths that should be served for all virtual hosts</li>
<li><code>api-no-templating</code> - Disable templating engine completely</li>
<li><code>api-templating</code> - Templating engne to use, see consolidate.js for supported engines</li>
<li><code>api-no-session</code> - Disable cookie session support, all requests must be signed for Web clients</li>
<li><code>api-session-age</code> - Session age in milliseconds, for cookie based authentication</li>
<li><code>api-session-secret</code> - Secret for session cookies, session support enabled only if it is not empty</li>
<li><code>api-query-token-secret</code> - Name of the property to be used for encrypting tokens for pagination or other sensitive data, any property from bk_auth can be used, if empty no secret is used, if not a valid property then it is used as the secret</li>
<li><code>api-app-header-name</code> - Name for the app name/version query parameter or header, it is can be used to tell the server about the application version</li>
<li><code>api-version-header-name</code> - Name for the access version query parameter or header, this is the core protocol version that can be sent to specify which core functionality a client expects</li>
<li><code>api-no-signature</code> - Disable signature verification for requests</li>
<li><code>api-no-cache-files</code> - Set cache-control=no-cache header for matching static files</li>
<li><code>api-tz-header-name</code> - Name for the timezone offset header a client can send for time sensitive requests, the backend decides how to treat this offset</li>
<li><code>api-signature-header-name</code> - Name for the access signature query parameter, header and session cookie</li>
<li><code>api-lang-header-name</code> - Name for the language query parameter, header and session cookie, primary language for a client</li>
<li><code>api-signature-age</code> - Max age for request signature in milliseconds, how old the API signature can be to be considered valid, the &#39;expires&#39; field in the signature must be less than current time plus this age, this is to support time drifts</li>
<li><code>api-no-access-token</code> - Disable access tokens support</li>
<li><code>api-access-time-interval</code> - Intervals to refresh last access time for accounts, only updates the cache if <code>bk_auth</code> is configured to be cached</li>
<li><code>api-access-token-name</code> - Name for the access token query parameter or header</li>
<li><code>api-access-token-secret</code> - A secret to be used for access token signatures, additional enryption on top of the signature to use for API access without signing requests, it is required for access tokens to be used</li>
<li><code>api-access-token-age</code> - Access tokens age in milliseconds, for API requests with access tokens only</li>
<li><code>api-disable-session</code> - Disable access to API endpoints for Web sessions, must be signed properly</li>
<li><code>api-disable-session-acl</code> - Combine regexps from the specified acls for the check explained by <code>-api-disable-session</code> parameter</li>
<li><code>api-allow-authenticated</code> - Add URLs which can be accessed by any authenticated user account, can be partial urls or Regexp, it is checked before any other account types, if matched then no account specific paths will be checked anymore(any of the allow-account-...)</li>
<li><code>api-allow-acl-authenticated</code> - Combine regexps from the specified acls for the check explained by <code>-api-allow-authenticated</code> parameter</li>
<li><code>api-allow-admin</code> - Add URLs which can be accessed by admin accounts only, can be partial urls or Regexp</li>
<li><code>api-allow-acl-admin</code> - Combine regexps from the specified acls for the check explained by <code>-api-allow-admin</code> parameter</li>
<li><code>api-allow-account-([a-z0-9_]+)</code> - Add URLs which can be accessed by specific account type, can be partial urls or Regexp</li>
<li><code>api-allow-acl-([a-z0-9_]+)</code> - Combine regexps from the specified acls for allow checks for the specified account type</li>
<li><code>api-only-account-([a-z0-9_]+)</code> - Add URLs which can be accessed by specific account type only, can be partial urls or Regexp</li>
<li><code>api-only-acl-([a-z0-9_]+)</code> - Combine regexps from the specified acls allowed for the specified account type only</li>
<li><code>api-deny-authenticated</code> - Add URLs which CAN NOT be accessed by any authenticated user account, can be partial urls or Regexp, it is checked before any other account types, if matched then no account specific paths will be checked anymore(any of the deny-account-...)</li>
<li><code>api-deny-acl-authenticated</code> - Combine regexps from the specified acls for the check explained by <code>-api-deny-authenticated</code> parameter</li>
<li><code>api-deny-account-([a-z0-9_]+)</code> - Add URLs which CAN NOT be accessed by specific account type, can be partial urls or Regexp, this is checked before any allow parameters</li>
<li><code>api-deny-acl-([a-z0-9_]+)</code> - Combine regexps from the specified acls for deny checks for the specified account type</li>
<li><code>api-acl-([a-z0-9_]+)</code> - Add URLs to the named ACL which can be used in allow/deny rules per account</li>
<li><code>api-allow</code> - Regexp for URLs that dont need credentials, replaces the whole access list</li>
<li><code>api-allow-path</code> - Add to the list of allowed URL paths without authentication, adds to the <code>-api-allow</code> parameter</li>
<li><code>api-allow-acl</code> - Combine regexps from the specified acls for the check explained by <code>-api-allow</code> parameter</li>
<li><code>api-deny</code> - Regexp for URLs that will be denied access, replaces the whole access list</li>
<li><code>api-deny-path</code> - Add to the list of URL paths to be denied without authentication, adds to the <code>-api-deny</code> parameter</li>
<li><code>api-deny-acl</code> - Combine regexps from the specified acls for the check explained by <code>-api-deny</code> parameter</li>
<li><code>api-allow-anonymous</code> - Add to the list of allowed URL paths that can be served with or without valid account, the difference with <code>-api-allow-path</code> is that it will check for signature and an account but will continue if no login is provided, return error in case of wrong account or not account found</li>
<li><code>api-allow-acl-anonymous</code> - Combine regexps from the specified acls for the check explained by <code>-allow-anonymous</code> parameter</li>
<li><code>api-allow-empty</code> - Regexp for URLs that should return empty responses if not found, for example return nothing for non-existent javascript files or css files</li>
<li><code>api-allow-ssl</code> - Add to the list of allowed URL paths using HTTPs only, plain HTTP requests to these urls will be refused</li>
<li><code>api-ignore-ssl</code> - Allow plain HTTP from matched IP addresss or paths</li>
<li><code>api-ignore-allow</code> - Regexp for URLs that should be ignored by the allow rules, the processing will continue</li>
<li><code>api-ignore-allow-path</code> - Add to the list of URL paths which should be ignored by the allow rules, in order to keep allow/deny rules simple, for example to keep some js files from open to all: -allow-path .js -ignore-allow-path /secure/</li>
<li><code>api-ignore-allow-acl</code> - Combine regexps from the specified acls for the check explained by <code>-ignore-allow-path</code> parameter</li>
<li><code>api-allow-ip</code> - Set regexp for IPs that dont need credentials, replaces the whole access list. It is checked before endpoint access list</li>
<li><code>api-deny-ip</code> - Set regexp for IPs that will be denied access, replaces the whole access list. It is checked before endpoint access list.</li>
<li><code>api-redirect-ssl</code> - Add to the list of the URL paths to be redirected to the same path but using HTTPS protocol, for proxy mode the proxy server will perform redirects</li>
<li><code>api-redirect-url</code> - Add to the list a JSON object with property name defining a host/path regexp to be matched early against in order to redirect using the value of the property, if the regexp starts with !, that means negative match, 2 variables can be used for substitution: @HOST@, @PATH@, @URL@, @BASE@, @DIR@, @QUERY@, example: { &#39;^[^/]+/path/$&#39;: &#39;/path2/index.html&#39;, &#39;.+/$&#39;: &#39;@PATH@/index.html&#39; } </li>
<li><code>api-subscribe-timeout</code> - Timeout for Long POLL subscribe listener, how long to wait for events before closing the connection, milliseconds</li>
<li><code>api-subscribe-interval</code> - Interval between delivering events to subscribed clients, milliseconds</li>
<li><code>api-express-options</code> - Set Express config options during initialization,example: <code>-api-express-options { &quot;trust proxy&quot;: 1, &quot;strict routing&quot;: true }</code></li>
<li><code>api-mime-body</code> - Collect full request body in the req.body property for the given MIME type in addition to json and form posts, this is for custom body processing</li>
<li><code>api-mime-ignore</code> - Ignore the body for the following MIME content types, request body will not be parsed at all</li>
<li><code>api-mime-map-(.+)</code> - File extension to MIME content type mapping, this is used by static-serve, example: -api-mime-map-mobileconfig application/x-apple-aspen-config</li>
<li><code>api-collect-host</code> - The backend URL where all collected statistics should be sent over, if set to <code>pool</code> then each web worker will save metrics directly into the statistics database pool</li>
<li><code>api-collect-pool</code> - Database pool where to save collected statistics, make sure to call -api-collect-create before using this</li>
<li><code>api-collect-create-tables</code> - Create statistics tables, it is not created by default with -db-create-tables</li>
<li><code>api-collect-interval</code> - How often to collect statistics and metrics in seconds</li>
<li><code>api-collect-send-interval</code> - How often to send collected statistics to the master server in seconds</li>
<li><code>api-platform-match</code> - An JSON object with list of regexps to match user-agent header for platform detection, example: { &#39;ios|iphone|ipad&#39;: &#39;ios&#39;, &#39;android&#39;: &#39;android&#39; }</li>
<li><code>api-cors-origin</code> - Origin header for CORS requests</li>
<li><code>api-no-cors</code> - Disable CORS requests</li>
<li><code>api-server-header</code> - Custom Server: header to return for all requests</li>
<li><code>api-error-message</code> - Default error message to return in case of exceptions</li>
<li><code>api-allow-error-code</code> - Error codes in exceptions to return in the response to the user, if not matched the error-message will be returned</li>
<li><code>api-url-metrics-ignore</code> - Ignore matched urls from the metrics collection</li>
<li><code>api-url-metrics-max</code> - Max number of path related metrics objects to keep, the most recent or the most popular urls will be kept only</li>
<li><code>api-url-metrics-path-(.+)</code> - Defines the length of an API request path to be stored in the statistics by matching the beginning of the request url, the smallest path wins, example: -api-url-metrics-/img 2 -api-url-metrics-/account/get 3</li>
<li><code>api-rlimits-max-(.+)</code> - Set max/burst rate limit by the given property, it is used by the request rate limiter using Token Bucket algorithm. Predefined types: ip, path, id, login</li>
<li><code>api-rlimits-rate-(.+)</code> - Set fill/normal rate limit by the given property, it is used by the request rate limiter using Token Bucket algorithm. Predefined types: ip, path, id, login</li>
<li><code>api-rlimits-interval-(.+)</code> - Set rate interval in ms by the given property, it is used by the request rate limiter using Token Bucket algorithm. Predefined types: ip, path, id, login</li>
<li><code>api-rlimits-total</code> - Total number of servers used in the rate limiter behind a load balancer, rates will be divided by this number so each server handles only a portion of the total rate limit</li>
<li><code>api-rlimits-interval</code> - Interval in ms for all rate limiters, defines the time unit, default is 1000 ms</li>
<li><code>api-exit-on-error</code> - Exit on uncaught exception in the route handler</li>
<li><code>api-upload-limit</code> - Max size for uploads, bytes</li>
<li><code>api-limiter-queue</code> - Name of an ipc queue for API rate limiting</li>
<li><code>api-errlog-limiter-max</code> - How many error messages to put in the log before throttling kicks in</li>
<li><code>api-errlog-limiter-interval</code> - Interval for error log limiter, max errors per this interval</li>
<li><code>api-errlog-limiter-ignore</code> - Do not show errors that match the regexp</li>
<li><code>api-proxy-reverse</code> - A Web server where to proxy requests not macthed by the url patterns or host header, in the form: <a href="http://host%5B:port%5D">http://host[:port]</a></li>
<li><code>api-proxy-url-(.+)</code> - URL regexp to be passed to other web server running behind, each parameter defines an url regexp and the destination in the value in the form <a href="http://host%5B:port%5D">http://host[:port]</a>, example: -api-proxy-url-^/api <a href="http://127.0.0.1:8080">http://127.0.0.1:8080</a></li>
<li><code>api-proxy-host-(.+)</code> - Virtual host mapping, to match any Host: header, each parameter defines a host name and the destination in the value in the form <a href="http://host%5B:port%5D">http://host[:port]</a>, example: -api-proxy-host-<a href="http://www.myhost.com">www.myhost.com</a> <a href="http://127.0.0.1:8080">http://127.0.0.1:8080</a></li>
<li><code>api-routing-(.+)</code> - URL path to be re-routed to other path, this is done inside the server at the beginning, only the path is replaced, same format and placeholders as in redirect-url, example: -api-routing-^/account/get /acount/read</li>
<li><code>api-auth-routing-(.+)</code> - URL path to be re-routed to other path after the authentication is successful, this is done inside the server, only the path is replaced, same format and placeholders as in redirect-url, example: -api-routing-auth-^/account/get /acount/read</li>
<li><code>api-login-redirect-(.+)</code> - Define a location where to redirect if no login is provided, same format and placeholders as in redirect-url, example: api-login-redirect-^/admin/=/login.html</li>
<li><code>api-auth-status</code> - Default authenticated status, if no auth rules matched but valid signature this is the status returned</li>
<li><code>api-auth-message:</code> - Default authenticated message to be returned the default auth status</li>
<li><code>api-reset-acl</code> - Reset all ACL related rules and permissions</li>
<li><code>msg-([^-]+)-(cert)(@.+)?</code> - A certificate for APN or similar services in pfx format, can be a file name with .p12 extension or a string with certificate contents encoded with base64, if the suffix is specified in the config parameter name will be used as the app name, otherwise it is global</li>
<li><code>msg-([^-]+)-authkey-([^-]+)-(.+)</code> - A auth key for APN in p8 format, can be a file name with .p8 extension or a string with the key contents encoded with base64</li>
<li><code>msg-([^-]+)-(key)(@.+)?</code> - API key for GCM or similar services, if the suffix is specified in the config parameter will be used as the app name, without the suffix it is global</li>
<li><code>msg-([^-]+)-(sandbox)(@.+)?</code> - Enable sandbox for a service, default is production mode</li>
<li><code>msg-([^-]+)-options-([^@]+)(@.+)?</code> - A config property to the specified agent, driver specific</li>
<li><code>msg-shutdown-timeout</code> - How long to wait for messages draining out in ms on shutdown before exiting</li>
<li><code>msg-app-default</code> - Default app id(app bundle id) to be used when no app_id is specified</li>
<li><code>msg-app-dependency@(.+)</code> - List of other apps that are considered in the same app family, sending to the primary app will also send to all dependent apps</li>
<li><code>msg-app-team-(.+)</code> - Regexp that identifies all app bundles for a team</li>
<li><code>msg-app-team</code> - Default team to use for apps that do not match any specific teams</li>
<li><code>jobs-workers</code> - How many worker processes to launch to process the job queue, -1 disables jobs, 0 means launch as many as the CPUs available</li>
<li><code>jobs-worker-cpu-factor</code> - A number to multiply the number of CPUs available to make the total number of workers to launch, only used if <code>workers</code> is 0</li>
<li><code>jobs-worker-args</code> - Node arguments for workers, for passing v8 jobspec, see <code>process</code></li>
<li><code>jobs-worker-env</code> - Environment to be passed to the worker via fork, see <code>cluster.fork</code></li>
<li><code>jobs-worker-delay</code> - Delay in milliseconds for a worker before it will start accepting jobs, for cases when other dependencies may take some time to start</li>
<li><code>jobs-max-runtime</code> - Max number of seconds a job can run before being killed</li>
<li><code>jobs-max-lifetime</code> - Max number of seconds a worker can live, after that amount of time it will exit once all the jobs are finished, 0 means indefinitely</li>
<li><code>jobs-shutdown-timeout</code> - Max number of milliseconds to wait for the graceful shutdown sequence to finish, after this timeout the process just exits</li>
<li><code>jobs-worker-queue</code> - Queue(s) to subscribe for workers, multiple queues can be processes at the same time, i.e. more than one job can run from different queues</li>
<li><code>jobs-cron-queue</code> - Default queue to use for cron jobs</li>
<li><code>jobs-global-queue</code> - Default queue for all jobs, the queueName is ignored</li>
<li><code>jobs-unique-queue</code> - Default queue name to use for keeping track of unique jobs</li>
<li><code>jobs-cron</code> - Allow cron jobs to be executed from the local etc/crontab file or via config parameter</li>
<li><code>jobs-schedule</code> - Cron jobs to be scheduled, the JSON must be in the same format as crontab file</li>
<li><code>server-max-processes</code> - Max number of processes to launch for Web servers, 0 means <code>NumberOfCPUs-1</code>, &lt; 0 means <code>NumberOfCPUs*abs(N)</code></li>
<li><code>server-crash-delay</code> - Delay between respawing the crashed process</li>
<li><code>server-restart-delay</code> - Delay between respawning the server after changes</li>
<li><code>server-no-restart</code> - Do not restart any processes terminated, for debugging crashes only</li>
<li><code>server-log-errors</code> - If true, log crash errors from child processes by the logger, otherwise write to the daemon err-file. The reason for this is that the logger puts everything into one line thus breaking formatting for stack traces.</li>
<li><code>server-process-name</code> - Path to the command to spawn by the monitor instead of node, for external processes guarded by this monitor</li>
<li><code>server-process-args</code> - Arguments for spawned processes, for passing v8 options or other flags in case of external processes</li>
<li><code>server-worker-args</code> - Node arguments for workers, job and web processes, for passing v8 options</li>
</ul>
<h2 id="module-api">Module: API</h2>
<ul>
<li><p><code>Database tables</code></p>
<pre><code>      // Authentication by login, only keeps id and secret to check the siganture
      bk_auth: {
          login: { primary: 1 },                              // Account login
          id: { type: &quot;uuid&quot;, prefix: &quot;u_&quot; },                 // Auto generated UUID to be linked with other records
          name: { type: &quot;text&quot; },                             // Account name
          status: { type: &quot;text&quot; },                           // Status of the account
          type: { type: &quot;text&quot;, list: 1, admin: 1 },          // Account roles: admin, ....
          flags: { type: &quot;list&quot;, list: 1, admin: 1 },         // Admin assigned flags about the account
          secret: { secure: 1 },                              // Signature secret or scrambled password
          auth_secret: { admin: 1, secure: 1 },               // Code for 2-factor authentication
          session_secret: { admin: 1, secure: 1 },            // Secret for session signatures
          acl_deny: { admin: 1, secure: 1 },                  // Deny access to matched url, a regexp
          acl_allow: { admin: 1, secure: 1 },                 // Only grant access if path matches this regexp
          query_deny: { admin: 1, secure: 1 },                // Ignore these query params, a regexp
          rlimits_max: { type: &quot;int&quot; },                       // Burst/max reqs/sec rate allowed for this account, 0 to disable
          rlimits_rate: { type: &quot;int&quot; },                      // Fill/normal reqs/sec rate for this account, 0 to disable
          expires: { type: &quot;bigint&quot;, admin: 1, secure: 1 },   // Deny access to the account if this value is before current date, milliseconds
          ctime: { type: &quot;now&quot;, readonly: 1 },                // Create time
          mtime: { type: &quot;now&quot; }
      },</code></pre></li>
</ul>

<ul>
<li><p><code>api.init(options, callback)</code></p>
<p> Initialize API layer, this must be called before the <code>api</code> module can be used but it is called by the server module automatically so <code>api.init</code> is
rearely need to called directly, only for new server implementation or if using in the shell for testing.</p>
<p>During the init sequence, this function calls <code>api.initMiddleware</code> and <code>api.initApplication</code> methods which by default are empty but can be redefined in the user aplications.</p>
<p>The bkjs.js uses its own request parser that places query parameters into <code>req.query</code> or <code>req.body</code> depending on the method.</p>
<p>For GET method, <code>req.query</code> contains all url-encoded parameters, for POST method <code>req.body</code> contains url-encoded parameters or parsed JSON payload or multipart payload.</p>
<p>The reason not to do this by default is that this may not be the alwayse wanted case and distinguishing data coming in the request or in the body may be desirable,
also, this will needed only for Express handlers <code>.all</code>, when registering handler by method like <code>.get</code> or <code>.post</code> then the handler needs to deal with only either source of the request data.</p>
</li>
</ul>

<ul>
<li><p><code>api.shutdown(callback)</code></p>
<p> Gracefully close all connections, call the callback after that</p>
</li>
</ul>

<ul>
<li><p><code>api.shutdownWeb(options, callback)</code></p>
<p> Gracefully close all database pools when the shutdown is initiated by a Web process</p>
</li>
</ul>

<ul>
<li><p><code>api.configureAccessLog()</code></p>
<p> Setup access log stream</p>
</li>
</ul>

<ul>
<li><p><code>api.handleServerRequest(req, res)</code></p>
<p> Start Express middleware processing wrapped in the node domain</p>
</li>
</ul>

<ul>
<li><p><code>api.setupSocketConnection(socket)</code></p>
<p> Called on new socket connection, supports all type of sockets</p>
</li>
</ul>

<ul>
<li><p><code>api.cleanupSocketConnection(socket)</code></p>
<p> Called when a socket connections is closed to cleanup all additional resources associated with it</p>
</li>
</ul>

<ul>
<li><p><code>api.checkWebSocketRequest(data, callback)</code></p>
<p> Called before allowing the WebSocket connection to be authorized</p>
</li>
</ul>

<ul>
<li><p><code>api.handleWebSocketConnect(socket)</code></p>
<p> Wrap external WeSocket connection into the Express routing, respond on backend command</p>
</li>
</ul>

<ul>
<li><p><code>api.createWebSocketRequest(socket, url, reply)</code></p>
<p> Wrap WebSocket into HTTP request to be proceses by the Express routes</p>
</li>
</ul>

<ul>
<li><p><code>api.closeWebSocketRequest(socket)</code></p>
<p> Close all pending requests, this is called on socket close or disconnect</p>
</li>
</ul>

<ul>
<li><p><code>api.prepareRequest(req)</code></p>
<p> Prepare request options that the API routes will merge with, can be used by pre process hooks, initialize
required properties for subsequent use</p>
</li>
</ul>

<ul>
<li><p><code>api.prepareOptions(req)</code></p>
<p> Parse or re-parse special headers about app version, language and timezone, it is called early to parse headers first and then
right after the query parameters are available, query values have higher priority than headers.</p>
</li>
</ul>

<ul>
<li><p><code>api.startMetrics(req, res, next)</code></p>
<p> This is supposed to be called at the beginning of request processing to start metrics and install the handler which
will be called at the end to finalize the metrics and call the cleanup handlers</p>
</li>
</ul>

<ul>
<li><p><code>api.handleMetrics(req, elapsed)</code></p>
<p> Finish metrics collection about the current rquest</p>
</li>
</ul>

<ul>
<li><p><code>api.handleCleanup(req)</code></p>
<p> Call registered cleanup hooks and clear the request explicitly</p>
</li>
</ul>

<ul>
<li><p><code>api.checkQuery(req, res, next)</code></p>
<p> Parse incoming query parameters</p>
</li>
</ul>

<ul>
<li><p><code>api.checkBody(req, res, next)</code></p>
<p> Parse multipart forms for uploaded files</p>
</li>
</ul>

<ul>
<li><p><code>api.checkRouting(req, name)</code></p>
<p> Check if the current request must be re-routed to another endpoint</p>
</li>
</ul>

<ul>
<li><p><code>api.checkRedirectPlaceholders(req, pathname)</code></p>
<p> Replace redirect placeholders</p>
</li>
</ul>

<ul>
<li><p><code>api.checkRedirect(req, options)</code></p>
<p> Check a request for possible redirection condition based on the configuration, this can be SSL checks or
defined redirect rules. This is used by API servers and proxy servers for early redirections. It returns null
if no redirects or errors happend, otherwise an object with status that is expected by the <code>api.sendStatus</code> method.
The options is expected to contain the following cached request properties:</p>
<ul>
<li>path - from req.path or the request pathname only</li>
<li>host - from req.host or the hostname part only</li>
<li>port - port from the host: header if specified</li>
<li>secure - if the protocol is https</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>api.checkRedirectRules(req, options, name)</code></p>
<p> Redirect rules, supports regexpobj and regexpmap parameters</p>
</li>
</ul>

<ul>
<li><p><code>api.checkAccountType(account, type)</code></p>
<p> Return true if the current user belong to the specified type, account type may contain more than one type.
NOTE: after this call the <code>type</code> property is converted into an array</p>
</li>
</ul>

<ul>
<li><p><code>api.checkRateLimits(req, options, callback)</code></p>
<p> Perform rate limiting by specified property, if not given no limiting is done.</p>
<p>The following options properties can be used:</p>
<ul>
<li><p>type - predefined: <code>ip,  path, login, id</code>, determines by which property to perform rate limiting, when using account properties
 the rate limiter should be called after the request signature has been parsed. Any other value is treated as
 custom type and used as is.
 <strong>This property is required.</strong></p>
<p> The predefined types:</p>
<ul>
<li>ip - limit number of requests per configured interval for an IP address</li>
<li>path - limit number of requests per configured interval for an API path and IP address, must be configured like: <code>-api-rlimits-/api/path-rate=2</code></li>
<li>id - limit number of requests per configured interval for an account id</li>
<li>login - limit number of requests per configured interval for a login from the signature, this is called
  before the account record is pulled from the DB</li>
</ul>
</li>
<li><p>ip - to use the specified IP address for type=ip</p>
</li>
<li><p>max - max capacity to be used by default</p>
</li>
<li><p>rate - fill rate to be used by default</p>
</li>
<li><p>interval - interval in ms within which the rate is measured, default 1000 ms</p>
</li>
<li><p>message - more descriptive text to be used in the error message for the type, if not specified a generic error message is used</p>
</li>
<li><p>total - apply this factor to the rate, it is used in case of multiple servers behind a loadbalancer, so for
 total 3 servers in the cluster the factor will be 3, i.e. each individual server checks for a third of the total request rate</p>
</li>
</ul>
<p>The metrics are kept in the LRU cache in the master process.</p>
<p>When used for accounts, it is possible to override rate limits for each account in the <code>bk_auth</code> table by setting <code>rlimit_max</code> and <code>rlimit_rate</code>
columns. To enable account rate limits the global defaults still must be set with the config paramaters <code>-api-rlimit-login-max</code> and <code>-api-rlimit-login-rate</code>
for example.</p>
<p>Example:</p>
<pre><code> api.checkLimit(req, { type: &quot;ip&quot;, rate: 100, interval: 60000 }, function(err) {
    if (err) return api.sendReply(err);
    ...
 });</code></pre></li>
</ul>

<ul>
<li><p><code>api.registerRateLimits(name, rate, max, interval)</code></p>
<p> Register access rate limit for a given name, all other rate limit properties will be applied as described in the <code>checkRateLimits</code></p>
</li>
</ul>

<ul>
<li><p><code>api.registerControlParams(options)</code></p>
<p> Add special control parameters that will be recognized in the query and placed in the <code>req.options</code> for every request.</p>
<p>Control params start with underscore and will be converted into the configured type according to the spec.
The <code>options</code> is an object in the format that is used by <code>lib.toParams</code>, no default type is allowed, even for string
it needs to be defined as { type: &quot;string&quot; }.</p>
<p>No existing control parameters will be overridden, also care must be taken when defining new control parameters so they do not
conflict with the existing ones.</p>
<p>These are default common parameters that can be used by any module:</p>
<ul>
<li><code>_count, _page, _tm, _sort, _select, _ext, _start, _token, _session, _format, _total, _encoding, _ops</code></li>
</ul>
<p>These are the reserved names that cannot be used for parameters, they are defined by the engine for every request:</p>
<ul>
<li><code>path, apath, ip, host, mtime, cleanup, secure, noscan, appName, appVersion, appPlatform, appLocale, appTimezone, apiVersion</code></li>
</ul>
<p>NOTE: <code>noscan</code> is set to 1 in every request to prevent accidental full scans, this means it cannot be enabled via the API but any module
can do it in the code if needed.</p>
<p>Example:</p>
<pre><code>mod.configureMiddleware = function(options, callback) {
    api.registerControlParams({ notify: { type: &quot;bool&quot; }, level: { type: &quot;int&quot;, min: 1, max: 10 } });
    callback();
}

Then if a request arrives for example as `_notify=true&amp;_level=5`, it will be parsed and placed in the `req.options`:

mod.configureWeb = function(options, callback) {

   api.app.all(&quot;/send&quot;, function(req, res) {
       if (req.options.notify) { ... }
       if (req.options.level &gt; 5) { ... }
   });
   callback()
}</code></pre></li>
</ul>

<ul>
<li><p><code>api.getOptions(req, controls)</code></p>
<p> Convert query options into internal options, such options are prepended with the underscore to
distinguish control parameters from the query parameters.</p>
<p>For security purposes this is the only place that translates special control query parameters into the options properties,
all the supported options are defined in the <code>api.controls</code> and can be used by the apps freely but with caution. See <code>registerControlParams</code>.</p>
<p>if <code>controls</code> is an object it will be used to define additional control parameters or override existing ones for this request only. Same rules as for
<code>registerControlParams</code> apply.</p>
<pre><code>   api.getOptions(req, { count: { min: 5, max: 100 } })</code></pre></li>
</ul>

<ul>
<li><p><code>api.getQuery(req, params, controls)</code></p>
<p> Parse query parameters according to the <code>params</code>, optionally process control parameters if <code>controls</code> si specified, this call combines
<code>lib.toParams()</code> with <code>api.getOptions</code>. Returns a query object or an error message, on success all controls will be set in the <code>req.options</code></p>
<pre><code>  var query = api.getQuery(req, { q: { required: 1 } }, { _count: { type: &quot;int&quot;, min: 10, max: 25 } });</code></pre></li>
</ul>

<ul>
<li><p><code>api.getTokenSecret(req)</code></p>
<p> Return a secret to be used for enrypting tokens, it uses the account property if configured or the global API token
to be used to encrypt data and pass it to the clients. <code>-api-query-token-secret</code> can be configured and if a column in the <code>bk_auth</code>
with such name exists it is used as a secret, otherwise the value of this property is used as a secret.</p>
</li>
</ul>

<ul>
<li><p><code>api.getResultPage(req, options, rows, info)</code></p>
<p> Return an object to be returned to the client as a page of result data with possibly next token
if present in the info. This result object can be used for pagination responses.</p>
</li>
</ul>

<ul>
<li><p><code>api.getPublicColumns(table, options)</code></p>
<p> Columns that are allowed to be visible, used in select to limit number of columns to be returned by a query</p>
<ul>
<li>pub property means public column</li>
<li>admins property means visible to admins and owners only</li>
</ul>
<p>options may be used to define the following properties:</p>
<ul>
<li><p>skip - a regexp with names to be excluded as well</p>
</li>
<li><p>allow - a list of properties which can be checked along with the <code>pub</code> property for a column to be considered public</p>
</li>
<li><p>disallow - a list of properties which if set will prevent a column to be returned, it is checked before the &#39;allow&#39; rule</p>
<p>api.getPublicColumns(&quot;bk_account&quot;, { allow: [&quot;admins&quot;], skip: /device_id|0$/ });</p>
</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>api.checkResultColumns(table, data, options)</code></p>
<p> Process records and keep only public properties as defined in the table columns. This method is supposed to be used in the post process
callbacks after all records have been processes and are ready to be returned to the client, the last step would be to cleanup
all non public columns if necessary.</p>
<p><code>table</code> can be a single table name or a list of table names which combined public columns need to be kept in the rows. List of request tables
is kept in the <code>req.options.cleanup</code> which by default is a table name of the API endpoint, for example for /account/get it will contain bk_account, for
/connection/get - bk_connection.</p>
<p>In the <code>options</code> account object can be present to detect account own records which will not be cleaned and all properties will be returned, by default <code>id</code>
property is used to detect current account but can be specified by the <code>options.account_key</code> property.</p>
<p>By default primary keys are not kept and must be marked with <code>pub</code> property in the table definition to be returned.</p>
<p>If any column is marked with <code>secure</code> property this means never return that column in the result even for the owner of the record</p>
<p>If any column is marked with <code>admin</code> or <code>admins</code> property and the current account is an admin this property will be returned as well. The <code>options.admin</code>
can be used to make it an artificial admin.</p>
<p>The <code>options.cleanup_strict</code> will enforce that all columns not present in the table definition will be skipped as well, by default all
new columns or columns created on the fly are returned to the client.</p>
<p>The <code>options.cleanup_rules</code> can be an object with property names and the values -1, 0, or 1 which correspond to:
-1 - never return, 0 return only to the owner, 1 always return.</p>
<p>The <code>options.pool</code> property must match the actual rowset to be applied properly, in case the records have been retrieved for the different
database pool.</p>
</li>
</ul>

<ul>
<li><p><code>api.clearQuery(table, query, options)</code></p>
<p> Clear request query properties specified in the table definition or in custom schema.</p>
<p>The <code>table</code> argument can be a table name or an object with properties as columns.</p>
<p>If <code>options.filter</code> is not specified the <code>query</code> will only keep existing columns for the given table.</p>
<p>If <code>options.filter</code> is a list then the <code>query</code> will delete properties for columns that contain any specified
property from the filter list. This is used for the <code>bk_auth</code> table to remove properties that supposed to be updated by admins only.
The filter will keep non-existent columns in the <code>query</code>. To remove such columns when using the filter specify <code>options.force</code>.</p>
<p>If a name in the filter is prefixed with ! then the logic is reversed, keep all except this property</p>
<p>If <code>options.keep</code> is a regexp it will be used to keep matched properties by name in the <code>query</code> regardless of any condition.</p>
<p>If <code>options.clear</code> is a regexp it will be used to remove matched properties by name in the <code>query</code>.</p>
<p>Example:</p>
<pre><code>  api.clearQuery(&quot;bk_account&quot;, req.query)
  api.clearQuery(&quot;bk_auth&quot;, req.query, { filter: [&quot;admin&quot;] })
  api.clearQuery(&quot;bk_auth&quot;, req.query, { filter: [&quot;!pub&quot;] })
  api.clearQuery(&quot;bk_account&quot;, req.query, { filter: [&quot;admin&quot;,&quot;secure&quot;] })
  api.clearQuery(&quot;bk_account&quot;, req.query, { filter: [&quot;admin&quot;,&quot;!secure&quot;], keep: /^__/ })
  api.clearQuery({ name: {}, id: { admin: 1 } }, req.query, { filter: [&quot;admin&quot;] })</code></pre></li>
</ul>

<ul>
<li><p><code>api.findHook(type, method, path)</code></p>
<p> Find registered hooks for given type and path</p>
</li>
</ul>

<ul>
<li><p><code>api.addHook(type, method, path, callback)</code></p>
<p> Register a hook callback for the type and method and request url, if already exists does nothing.</p>
</li>
</ul>

<ul>
<li><p><code>api.registerAccessCheck(method, path, callback)</code></p>
<p> Register a handler to check access for any given endpoint, it works the same way as the global accessCheck function and is called before
validating the signature or session cookies. No account information is available at this point yet.</p>
<ul>
<li>method can be &#39;&#39; in such case all methods will be matched</li>
<li>path is a string or regexp of the request URL similar to registering Express routes</li>
<li>callback is a function with the following parameters: function(req, cb) {}, to indicate an error condition pass an object
with the callback with status: and message: properties, status != 200 means error,
status == 0 means continue processing, ignore this match</li>
</ul>
<p>Example:</p>
<pre><code>    api.registerAccessCheck(&#39;&#39;, &#39;account&#39;, function(req, cb) { cb({ status: 500, message: &quot;access disabled&quot;}) }))

    api.registerAccessCheck(&#39;POST&#39;, &#39;/account/add&#39;, function(req, cb) {
       if (!req.query.invitecode) return cb({ status: 400, message: &quot;invitation code is required&quot; });
       cb();
    });</code></pre></li>
</ul>

<ul>
<li><p><code>api.registerPreProcess(method, path, callback)</code></p>
<p> Similar to <code>registerAccessCheck</code> but this callback will be called after the signature or session is verified but before
the API route method is called. The <code>req.account</code> object will always exist at this point but may not contain the user in case of an error.</p>
<p>The purpose of this hook is to perform some preparations or check permissions of a valid user to resources or in case of error perform any other action
like redirection or returning something explaining what to do in case of failure. The callback for this call is different then in <code>checkAccess</code> hooks.</p>
<ul>
<li>method can be &#39;&#39; in such case all methods will be matched</li>
<li>path is a string or regexp of the request URL similr to registering Express routes</li>
<li>callback is a function(req, status, cb) where status is an object { status:..., message: ..} passed from the checkRequestSignature call, if status != 200 it means
an error condition, the callback must pass the same or modified status object in its own <code>cb</code> callback</li>
</ul>
<p>Example:</p>
<pre><code>     api.registerPreProcess(&#39;GET&#39;, &#39;/account/get&#39;, function(req, status, cb) {
          if (status.status != 200) status = { status: 302, url: &#39;/error.html&#39; };
          cb(status)
     });</code></pre><p>Example with admin access only:</p>
<pre><code>    api.registerPreProcess(&#39;POST&#39;, &#39;/data/&#39;, function(req, status, cb) {
        if (req.account.type != &quot;admin&quot;) return cb({ status: 401, message: &quot;access denied, admins only&quot; });
        cb();
    });</code></pre></li>
</ul>

<ul>
<li><p><code>api.registerPostProcess(method, path, callback)</code></p>
<p> Register a callback to be called after successfull API action, status 200 only. To trigger this callback the primary response handler must return
results using <code>api.sendJSON</code> or <code>api.sendFormatted</code> methods.</p>
<p>The purpose is to perform some additional actions after the standard API completed or to customize the result</p>
<ul>
<li>method can be &#39;&#39; in such case all methods will be matched</li>
<li>path is a string or regexp of the request URL similar to registering Express routes</li>
<li>callback is a function with the following parameters: function(req, res, rows) where rows is the result returned by the API handler,
the callback may not return data back to the client, in this case next post-process hook will be called and eventually the result will be sent back to the client.
<strong>To indicate that this hook will send the result eventually it must return true, otherwise the rows will be sent afer all hooks are called</strong></li>
</ul>
<p>Note: the <code>req.account,req.options,req.query</code> objects may become empty if any callback decided to do some async action, they are explicitly emptied at the end of the request,
in such cases make a copy of the needed objects if it will needed</p>
<p>Example, just update the rows, it will be sent at the end of processing all post hooks</p>
<pre><code>    api.registerPostProcess(&#39;&#39;, &#39;/data/&#39;, function(req, res, rows) {
        rows.forEach(function(row) { ...});
    });</code></pre><p>Example, add data to the rows and return result after it</p>
<pre><code>    api.registerPostProcess(&#39;&#39;, &#39;/data/&#39;, function(req, res, row) {
        db.get(&quot;bk_account&quot;, { id: row.id }, function(err, rec) {
            row.name = rec.name;
            res.json(row);
        });
        return true;
    });</code></pre></li>
</ul>

<ul>
<li><p><code>api.registerCleanup(method, path, callback)</code></p>
<p> Register a cleanup callback that will be called at the end of a request, all registered cleanup callbacks will be called in the order
of registration. At this time the result has been sent so connection is not valid anymore but the request and account objects are still available.</p>
<p>Example, do custom logging of all requests</p>
<pre><code>    api.registerCleanup(&#39;&#39;, &#39;/data/&#39;, function(req, next) {
        db.add(&quot;log&quot;, req.query, next);
    });</code></pre></li>
</ul>

<ul>
<li><p><code>api.registerSendStatus(method, path, callback)</code></p>
<p> Register a status callback that will be called when <code>api.sendReply</code> or <code>api.sendStatus</code> is called,
all registered callbacks will be called in the order of registration. At this time the result has NOT been sent yet so connection is
still valid and can be changed.</p>
<p>Example, do custom logging of all requests</p>
<pre><code>    api.registerSendStatus(&#39;&#39;, &#39;/data/&#39;, function(req, data) {
        logger.info(&quot;response&quot;, req.path, data);
    });</code></pre></li>
</ul>

<ul>
<li><p><code>api.registerSignature(method, path, callback)</code></p>
<p> The purpose of this hook is to manage custom signatures.</p>
<ul>
<li>method can be &#39;&#39; in such case all methods will be matched</li>
<li>path is a string or regexp of the request URL similr to registering Express routes</li>
<li>callback is a function(req, account, sig, cb) where<ul>
<li>if sig is null it means to generate a new signature for the given account and return in the callback, if multiple hooks are registered the processing
stops on first signature returned</li>
<li>if sig is provided that means to verify the signature against given account and return it if valid or return null if it is invalid or
cannot be verified by current hook, multiple hooks can be supported and it stops on first signature returned in the callback</li>
</ul>
</li>
</ul>
<p>Example:</p>
<pre><code>     api.registerSignature(&#39;&#39;, &#39;/&#39;, function(req, account, sig, cb) {
          if (sig) {
              if (invalid) sig = null;
          } else {
              sig = api.createSignature(.....);
          }
          cb(sig)
     });</code></pre></li>
</ul>

<ul>
<li><p><code>api.registerSecret(login, callback)</code></p>
<p> Register a secret generation method.</p>
<ul>
<li>login is a regexp for logins to have a special secret encryption method</li>
<li>callback is a function(account, options, cb)</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>api.registerOAuthStrategy(strategy, options, callback)</code></p>
<p> Given passport strategy setup OAuth callbacks and handle the login process by creating a mapping account for each
OAUTH authenticated account.
The callback if specified will be called as function(req, options, info) with <code>req.user</code> signifies the successful
login and hold the account properties. If given it is up to the callback to perform any redirects reqauired for
completion of the login process.</p>
<p>The following options properties are accepted:</p>
<ul>
<li>cliendID,</li>
<li>clientSecret,</li>
<li>callbackURL - passport OAUTH properties</li>
<li>session - setup cookie session on success</li>
<li>successUrl - redirect url on success if no callback is specified</li>
<li>failureUrl - redirect url on failure if no callback is specified</li>
<li>fetchAccount - a new function to be used instead of api.fetchAccount for new account creation or mapping
 for the given authenticated profile. This is for processing or customizing new account properties and doing
 some post processing work after the account has been created.
 For any function, <code>query._profile</code>, <code>query._accessToken</code>, <code>query._refreshToken</code> will be set for the authenticated profile object from the provider.</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>api.sendJSON(req, err, rows)</code></p>
<p> Send result back with possibly executing post-process callback, this is used by all API handlers to allow custom post processing in the apps.
If err is not null the error message is returned immediately.</p>
</li>
</ul>

<ul>
<li><p><code>api.sendFormatted(req, err, data, options)</code></p>
<p> Send result back formatting according to the options properties:</p>
<ul>
<li>format - json, csv, xml, JSON is default</li>
<li>separator - a separator to use for CSV and other formats</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>api.sendStatus(res, options)</code></p>
<p> Return reply to the client using the options object, it contains the following properties:</p>
<ul>
<li>status - defines the respone status code</li>
<li>message  - property to be sent as status line and in the body</li>
<li>type - defines Content-Type header, the message will be sent in the body</li>
<li>url - for redirects when status is 301 or 302</li>
</ul>
<p><strong>i18n Note:</strong></p>
<p>The API server attaches fake i18n functions <code>req.__</code> and <code>res.__</code> which are used automatically for the <code>message</code> property
before sending the response.</p>
<p>With real i18n module these can/will be replaced performing actual translation without
using <code>i18n.__</code> method for messages explicitely in the application code for <code>sendStatus</code> or <code>sendReply</code> methods.</p>
</li>
</ul>

<ul>
<li><p><code>api.sendReply(res, status, text)</code></p>
<p> Send formatted JSON reply to an API client, if status is an instance of Error then error message with status 500 is sent back.</p>
<p>If the status is an object it is sent as is.</p>
<p>All Error objects will return a generic error message without exposing the real error message, it will log all error exceptions in the logger
subject to log throttling configuration.</p>
</li>
</ul>

<ul>
<li><p><code>api.sendFile(req, file, redirect)</code></p>
<p> Send file back to the client, res is Express response object</p>
</li>
</ul>

<ul>
<li><p><code>api.subscribe(req, options)</code></p>
<p> Subscribe for events, this is used by <code>/acount/subscribe</code> API call but can be used in generic way, if no options
provided by default it will listen on req.account.id, the default API implementation for Connection, Counter, Messages publish
events using account id as a key.</p>
<ul>
<li>req is always an Express request object</li>
<li>optons may contain the following propertis:<ul>
<li>key - alternative key to subscribe for</li>
<li>timeout - how long to wait before dropping the connection, default 15 mins</li>
<li>interval - how often send notifications to the client, this allows buffering several events and notify about them at once instead triggering
 event condition every time, useful in case of very frequent events</li>
<li>match - a regexp that matched the message text, if not matched these events will be dropped</li>
</ul>
</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>api.unsubscribe(req, options)</code></p>
<p> Disconnect from subscription service. This forces disconnect even for persistent connections like websockets.</p>
</li>
</ul>

<ul>
<li><p><code>api.publish(key, event, options)</code></p>
<p> Publish an event for an account, key is account id or other key used for subscription, event is a string or an object</p>
</li>
</ul>

<ul>
<li><p><code>api.sendEvent(req, key, data, next)</code></p>
<p> Process a message received from subscription server or other event notifier, it is used by <code>api.subscribe</code> method for delivery events to the clients</p>
</li>
</ul>

<h2 id="module-api_accounts">Module: API_ACCOUNTS</h2>
<ul>
<li><p><code>api.configureDefaultAPI()</code></p>
<p> Default API calls that endpoints for health check, authentication and public image access</p>
</li>
</ul>

<ul>
<li><p><code>api.prepareAccountSecret(account, options, callback)</code></p>
<p> If specified in the options, prepare credentials to be stored in the db, if no error occured return null, otherwise an error object</p>
<ul>
<li>scramble is used to encrypt the secret with login as HMAC_SHA256 so the db never stores cleartext credentials</li>
<li>bcrypt - use bcrypt module to generate password hash with specified number of salt rounds, recommended value is 10</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>api.fetchAccount(query, options, callback)</code></p>
<p> Given a profile data from some other system, check if there is an account or create a new account for the given
profile, returns account record in the callback. req.query contains profile fields converted to bk_auth names
so the whole req.query can be saved as it is. <code>query.login</code> must exist.</p>
<p>This method is supposed to be called after the user is authenticated and verified, it does not
check secrets but only existence of a user by login. On success existing or new account is returned by the callback.</p>
<p>If new account is created, the generated secret will be returned and must be saved by the client for subsequent
API calls unless cookie session is established.</p>
<p>if <code>query.icon&#39; is set with the url of the profile image, it will be downloaded and saved as account icon type</code>0<code>.</code>options.width`
if specified will be used to resize the image.</p>
<p>In case when a new account was created the account record will have a property <code>_added</code> set to true.
This is to explicitely distinguish existing and new accounts.</p>
</li>
</ul>

<ul>
<li><p><code>api.addAccount(query, options, callback)</code></p>
<p> Register new account, return new account record in the callback, when options.admin is true then allow to set all properties
otherwise admin properties will not be updated</p>
</li>
</ul>

<ul>
<li><p><code>api.updateAccount(query, options, callback)</code></p>
<p> Update existing account, if <code>options.admin</code> is true then allow to update all properties</p>
</li>
</ul>

<ul>
<li><p><code>api.setAccountSecret(query, options, callback)</code></p>
<p> Change account secret</p>
</li>
</ul>

<h2 id="module-api_auth">Module: API_AUTH</h2>
<ul>
<li><p><code>api.handleSignature(req, res, next)</code></p>
<p> Perform authorization of the incoming request for access and permissions</p>
</li>
</ul>

<ul>
<li><p><code>api.handleSessionSignature(req, callback)</code></p>
<p> Setup session cookies or access token for automatic authentication without signing, req must be complete with all required
properties after successful authorization.</p>
</li>
</ul>

<ul>
<li><p><code>api.newSignature(req)</code></p>
<p> Returns a new signature object with all required properties filled form the request object</p>
</li>
</ul>

<ul>
<li><p><code>api.getSignature(req)</code></p>
<p> Parse incoming request for signature and return all pieces wrapped in an object, this object will be used by <code>verifySignature</code> function.</p>
<p>If the signature successfully recognized it is saved in the request as <code>req.signature</code>,
it always returns a signature object, a new one or existing</p>
</li>
</ul>

<ul>
<li><p><code>api.verifySignature(req, sig, account, callback)</code></p>
<p> Returns true if the signature <code>sig</code> matches given account secret. <code>account</code> object must be a <code>bk_auth</code> record.</p>
</li>
</ul>

<ul>
<li><p><code>api.createSignature(login, secret, method, host, uri, options)</code></p>
<p> Create secure signature for an HTTP request. Returns an object with HTTP headers to be sent in the response.</p>
<p>The options may contains the following:</p>
<ul>
<li>expires is absolute time in milliseconds when this request will expire, default is 30 seconds from now</li>
<li>version a version number defining how the signature will be signed</li>
<li>type - content-type header, may be omitted</li>
<li>tag - a custom tag, vendor specific, opaque to the bkjs, can be used for passing additional account or session inforamtion</li>
<li>checksum - SHA1 digest of the whole content body, may be omitted</li>
<li>query - on object with query parameters to use instead of parameters in the uri</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>api.checkRequestSignature(req, callback)</code></p>
<p> Verify request signature from the request object, uses properties: .host, .method, .url or .originalUrl, .headers</p>
</li>
</ul>

<ul>
<li><p><code>api.checkAccountSignature(req, sig, account, callback)</code></p>
<p> Check account record against a incoming request signature</p>
</li>
</ul>

<ul>
<li><p><code>api.setCurrentAccount(req, account)</code></p>
<p> Assign or clear the current account record for the given request, if account is null the account is cleared</p>
</li>
</ul>

<ul>
<li><p><code>api.checkAccess(req, callback)</code></p>
<p> Perform URL based access checks, this is called before the signature verification, very early in the request processing step.</p>
<p>Checks access permissions, calls the callback with the following argument:</p>
<ul>
<li>nothing if checkRequestSignature needs to be called</li>
<li>an object with status: 200 to skip authorization and proceed with other routes</li>
<li>an object with status: 0 means response has been sent, just stop</li>
<li>an object with status other than 0 or 200 to return the status and stop request processing,
for statuses 301,302 there should be url property in the object returned</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>api.checkAuthorization(req, status, callback)</code></p>
<p> Perform authorization checks after the account been checked for valid signature, this is called even if the signature verification failed,
in case of a custom authentication middlware this must be called at the end and use the status object returned in the callback to
return an error or proceed with the request. In any case the result of this function is final.</p>
<p>If a user has valid login by default access to all API endpoints is granted, to restrict access to specific APIs use any combinations of
<code>api-allow</code> or <code>api-deny</code> config parameters.</p>
<ul>
<li>req is Express request object</li>
<li>status contains the signature verification status, an object with status: and message: properties, can not be null.
The status property is passed to each hook in the chain, the result status will be returned to the client.</li>
<li>callback is a function(status) to be called with the resulted status where status must be an object with status and message properties as well</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>api.checkLogin(req, callback)</code></p>
<p> Check login and secret from a client</p>
</li>
</ul>

<h2 id="module-api_files">Module: API_FILES</h2>
<ul>
<li><p><code>api.fileUrl(file, options)</code></p>
<p> Returns absolute file url if it is configured with any prefix or S3 bucket, otherwise returns empty string</p>
</li>
</ul>

<ul>
<li><p><code>api.getFile(req, file, options)</code></p>
<p> Send a file to the client</p>
</li>
</ul>

<ul>
<li><p><code>api.putFile(req, name, options, callback)</code></p>
<p> Upload file and store in the filesystem or S3, try to find the file in multipart form, in the body or query by the given name</p>
<ul>
<li>name is the name property to look for in the multipart body or in the request body or query</li>
<li>callback will be called with err and actual filename saved</li>
</ul>
<p>Output file name is built according to the following options properties:</p>
<ul>
<li>name - defines the output name for the file, if not given same name as property will be used</li>
<li>prefix - the folder prefix where the file will be uploaded, all leading folders will be created automatically</li>
<li>ext - what file extention to use, appended to the name, if no ext is given the extension from the uploaded file will be used or no extention if could not determine one.</li>
<li>extkeep - keep actual extention from the uploaded file, ignore the ext parameter</li>
<li>namekeep - keep the name of the uploaded file if present in the multipart form</li>
<li>encoding - encoding of the body, default is base64</li>
<li>allow - a Regexp with allowed MIME types, this will use detectFile method to discover file type by the contents</li>
</ul>
<p>On return the options may have the following properties set:</p>
<ul>
<li>filesize - size of the file in bytes if available</li>
<li>mimetype - file mime type if detected</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>api.storeFile(tmpfile, outfile, options, callback)</code></p>
<p> Place the uploaded tmpfile to the destination pointed by outfile</p>
</li>
</ul>

<ul>
<li><p><code>api.delFile(file, options, callback)</code></p>
<p> Delete file by name from the local filesystem or S3 drive if filesS3 is defined in api or options objects</p>
</li>
</ul>

<ul>
<li><p><code>api.detectFile(file, callback)</code></p>
<p> Returns detected mime type and ext for a file</p>
</li>
</ul>

<h2 id="module-api_icons">Module: API_ICONS</h2>
<ul>
<li><p><code>api.scaleIcon(infile, options, callback)</code></p>
<p> Scale image using ImageMagick, return err if failed</p>
<ul>
<li>infile can be a string with file name or a Buffer with actual image data</li>
<li>options can specify image properties:<ul>
<li>outfile - if not empty is a file name where to store scaled image or if empty the new image contents will be returned in the callback as a buffer</li>
<li>width, height - new image dimensions<ul>
<li>if width or height is negative this means do not perform upscale, keep the original size if smaller than given positive value,</li>
<li>if any is 0 that means keep the original size</li>
</ul>
</li>
<li>filter - ImageMagick image filters, default is lanczos</li>
<li>quality - 0-99 percent, image scaling quality</li>
<li>ext - image format: png, gif, jpg, svg</li>
<li>flip - flip horizontally</li>
<li>flop - flip vertically</li>
<li>blue_radius, blur_sigma - perform adaptive blur on the image</li>
<li>crop_x, crop_y, crop_width, crop_height - perform crop using given dimensions</li>
<li>sharpen_radius, sharpen_sigma - perform sharpening of the image</li>
<li>brightness - use thing to change brightness of the image</li>
<li>contrast - set new contrast of the image</li>
<li>rotate - rotation angle, if 1 strip=1 is assigned automatically</li>
<li>bgcolor - color for the background, used in rotation</li>
<li>quantized - set number of colors for quantize</li>
<li>treedepth - set tree depth for quantixe process</li>
<li>dither - set 0 or 1 for quantixe and posterize processes</li>
<li>posterize - set number of color levels</li>
<li>normalize - normalize image</li>
<li>opacity - set image opacity</li>
<li>strip - remove EXIF meta data, if not specified defaults to 1</li>
</ul>
</li>
</ul>
<p>The callback takes 4 arguments: function(err, data, info)</p>
<p>where <code>data</code> will contain a new image data and <code>info</code> is an object with the info about the new or unmodified image: ext, width, height.
For animated GIF files it will not convert but return it as is unless it is specified in the options to do so, frame and no_animation
must be used if no animation is required, see bk_wand for more info. This means that the resulting image type must be consulted
from the info.format because it may be not the desired format.</p>
</li>
</ul>

<ul>
<li><p><code>api.iconPath(id, options)</code></p>
<p> Full path to the icon, perform necessary hashing and sharding, id can be a number or any string.</p>
<p><code>options.type</code> may contain special placeholders:</p>
<ul>
<li>@uuid@ - will be replaced with a unique UUID and placed back to the options.type</li>
<li>@now@ - will be replaced with the current timestamp</li>
<li>@filename@ - will be replaced with the basename of the uploaded file from the filename property if present</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>api.iconUrl(row, options)</code></p>
<p> Returns constructed icon url from the icon record</p>
</li>
</ul>

<ul>
<li><p><code>api.checkIcon(req, row)</code></p>
<p> Verify icon permissions and format for the result, used in setProcessRow for the bk_icon table</p>
</li>
</ul>

<ul>
<li><p><code>api.sendIcon(req, id, options)</code></p>
<p> Send an icon to the client, only handles files</p>
</li>
</ul>

<ul>
<li><p><code>api.putIcon(req, name, id, options, callback)</code></p>
<p> Store an icon for account, the options are the same as for the <code>iconPath</code> method</p>
<ul>
<li>name is the name property to look for in the multipart body or in the request body or query</li>
<li>id is used in <code>iconPath</code> along with the options to build the icon absolute path</li>
<li>autodel - if true, auto delete the base64 icon property from the query or the body after it is decoded, this is to
mark it for deallocation while the icon is being processed, the worker queue is limited so with large number of requests
all these query objects will remain in the query wasting memory</li>
<li>verify - check the given image of file header for known image types</li>
<li>extkeep - a regexp with image types to preserve, not to convert into the specified image type</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>api.saveIcon(file, id, options, callback)</code></p>
<p> Save the icon data to the destination, if api.imagesS3 or options.imagesS3 specified then plave the image on the S3 drive.
Store in the proper location according to the types for given id, this function is used after downloading new image or
when moving images from other places. On success the callback will be called with the second argument set to the output
file name where the image has been saved.
Valid properties in the options:</p>
<ul>
<li>type - icon type, this will be prepended to the name of the icon, there are several special types:<ul>
<li>@uuid@ - auto generate an UUID</li>
<li>@now@ - use current timestamp</li>
<li>@filename@ - if filename is present the basename without extension will be used</li>
</ul>
</li>
<li>prefix - top level subdirectory under images/</li>
<li>width, height, filter, ext, quality for <code>resizeImage</code> function</li>
<li>filesize - file size if available</li>
<li>filename - name of a file uploaded if available</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>api.delIcon(id, options, callback)</code></p>
<p> Delete an icon for account, .type defines icon prefix</p>
</li>
</ul>

<ul>
<li><p><code>api.isIconUrl(url)</code></p>
<p> Return true if the given file or url point ot an image</p>
</li>
</ul>

<ul>
<li><p><code>api.isIcon(buf)</code></p>
<p> Returns detected image type if the given buffer contains an image, it checks the header only</p>
</li>
</ul>

<h2 id="module-api_proxy">Module: API_PROXY</h2>
<ul>
<li><p><code>api.handleProxyRequest(req, res, callback)</code></p>
<p> Process incoming proxy request, can be overriden for custom logic with frontend proxy server. If any
response is sent or an error returned in the calback
then the request will be aborted and will not be forwarded to the web processes</p>
</li>
</ul>

<ul>
<li><p><code>api.createProxyServer()</code></p>
<p> Create a proxy server to handle incoming requests and distribute them to the workers</p>
</li>
</ul>

<ul>
<li><p><code>api.createProxyWorker()</code></p>
<p> Create/fork a worker to handle API requests, register a new port for load balancing between web workers</p>
</li>
</ul>

<ul>
<li><p><code>api.getProxyPort()</code></p>
<p> Return a target port for proxy requests, rotates between all web workers</p>
</li>
</ul>

<ul>
<li><p><code>api.getProxyTarget(req)</code></p>
<p> Return a target for proxy requests</p>
</li>
</ul>

<ul>
<li><p><code>api.runProxyRequest(req, res, ssl)</code></p>
<p> Process a proxy request, perform all filtering or redirects</p>
</li>
</ul>

<h2 id="module-api_statistics">Module: API_STATISTICS</h2>
<ul>
<li><p><code>api.createStatisticsTables()</code></p>
<p> Collected metrics per worker process, basic columns are defined in the table to be collected like
api and db request rates(.rmean), response times(.hmean) and total number of requests(<em>0).
Counters ending with `</em>0<code>are snapshots, i.e. they must be summed up for any given interval.
All other counters are averages. Only subset of all available API endpoints is defined here
for example purposes, for SQL databases all columns must be defined but for NoSQL this is not required,
depending on the database that is used for collection the metrics must be added to the table. All</code>url_<code>columns
are the API requests, not the DB calls made by the app, the length of URL path to be stored is defined in the API module
by the</code>api-url-metrics-` config parameter.</p>
</li>
</ul>

<ul>
<li><p><code>Database tables</code></p>
<pre><code>      bk_collect: {
          id: { primary: 1 },
          mtime: { type: &quot;now&quot;, primary: 1 },
          app: {},
          ip: {},
          type: {},
          instance: {},
          worker: {},
          pid: { type: &quot;int&quot; },
          latency: { type: &quot;int&quot; },
          cpus: { type: &quot;int&quot; },
          mem: { type: &quot;bigint&quot; },
          rss_hmean: { type: &quot;real&quot; },
          heap_hmean: { type: &quot;real&quot; },
          avg_hmean: { type: &quot;real&quot; },
          free_hmean: { type: &quot;real&quot; },
          util_hmean: { type: &quot;real&quot; },
          api_req_rmean: { type: &quot;real&quot; },
          api_req_hmean: { type: &quot;real&quot; },
          api_req_0: { type: &quot;real&quot; },
          api_err_0: { type: &quot;real&quot; },
          api_bad_0: { type: &quot;real&quot; },
          api_400_0: { type: &quot;real&quot; },
          api_401_0: { type: &quot;real&quot; },
          api_403_0: { type: &quot;real&quot; },
          api_417_0: { type: &quot;real&quot; },
          api_429_0: { type: &quot;real&quot; },
          api_que_rmean: { type: &quot;real&quot; },
          api_que_hmean: { type: &quot;real&quot; },
          pool_req_rmean: { type: &quot;real&quot; },
          pool_req_hmean: { type: &quot;real&quot; },
          pool_req_0: { type: &quot;real&quot; },
          pool_err_0: { type: &quot;real&quot; },
          pool_que_rmean: { type: &quot;real&quot; },
          pool_que_hmean: { type: &quot;real&quot; },
          ctime: { type: &quot;mtime&quot; },
      }</code></pre></li>
</ul>

<ul>
<li><p><code>api.initStatistics()</code></p>
<p> Setup statistics collections</p>
</li>
</ul>

<ul>
<li><p><code>api.getStatistics(options)</code></p>
<p> Updates metrics with the current values and returns an object ready to be saved in the database, i.e. flattened ito one object
where all property names of the complex objects are combined into one name separated by comma.</p>
</li>
</ul>

<ul>
<li><p><code>api.sendStatistics()</code></p>
<p> Send collected statistics to the collection server, <code>backend-host</code> must be configured and possibly <code>backend-login</code> and <code>backend-secret</code> in case
the system API is secured, the user can be any valid user registered in the bk_auth table.</p>
</li>
</ul>

<ul>
<li><p><code>api.saveStatistics(obj, options, callback)</code></p>
<p> Save collected statistics in the bk_collect table, this can be called via API or directly by the backend, this wrapper
is supposed to be overrriden by the application with additional logic how the statistics is saved. Columns in the bk_collect table
must be defined for any metrics to be saved, use api.describeTable with additional columns from the api.metrics object in additional to the default ones.</p>
<p>Example, add pool cache stats to the table</p>
<pre><code>    api.describeTable({ bk_collect: { pool_cache_rmean: { type: &quot;real&quot; },
                                      pool_cache_hmean: { type: &quot;real&quot; } });</code></pre></li>
</ul>

<ul>
<li><p><code>api.calcStatistics(query, options, callback)</code></p>
<p> Calculate statistics for a period of time, query and options must confirm to the db.select conventions.</p>
</li>
</ul>

<h2 id="module-app">Module: APP</h2>
<ul>
<li><p><code>app</code></p>
<p> This is a skeleton module to be extended by the specific application logic. It provides all
callbacks and hooks that are called by the core backend modules
during different phases, like initialization, shutting down, etc...</p>
<p>It should be used for custom functions and methods to be defined, the <code>app</code> module is always available.</p>
<p>All app modules in the modules/ subdirectory use the same prototype, i.e. all hooks are available for custom app modules as well.</p>
</li>
</ul>

<ul>
<li><p><code>app.configure(options, callback)</code></p>
<p> Called after all config files are loaded and command line args are parsed, home directory is set but before the db is initialized,
the primary purpose of this early call is to setup environment before connecting to the database. This is called regardless of the server
to be started and intended to initialize the common environment before the database and other subsystems are initialized.</p>
</li>
</ul>

<ul>
<li><p><code>app.configureModule(options, callback)</code></p>
<p> Called after the core.init has been initialized successfully, this can be redefined in the applications to add additional
init steps that all processes require to have. All database pools and other confugration is ready at this point. This hook is
called regardless of what kind of server is about to start, it is always called before starting a server or shell.</p>
</li>
</ul>

<ul>
<li><p><code>app.configureMiddleware(options, callback)</code></p>
<p> This handler is called during the Express server initialization just after the security middleware.</p>
<p>NOTE: <code>api.app</code> refers to the Express instance.</p>
</li>
</ul>

<ul>
<li><p><code>app.configureWeb(options, callback)</code></p>
<p> This handler is called after the Express server has been setup and all default API endpoints initialized but the Web server
is not ready for incoming requests yet. This handler can setup additional API endpoints, add/modify table descriptions.</p>
<p>NOTE: <code>api.app</code> refers to the Express instance</p>
</li>
</ul>

<ul>
<li><p><code>app.shutdownWeb(options, callback)</code></p>
<p> Perform shutdown sequence when a Web process is about to exit</p>
<p>NOTE: <code>api.app</code> refers to the Express instance</p>
</li>
</ul>

<ul>
<li><p><code>app.configureMaster(options, callback)</code></p>
<p> This handler is called during the master server startup, this is the process that monitors the worker jobs and performs jobs scheduling</p>
</li>
</ul>

<ul>
<li><p><code>app.configureServer(options, callback)</code></p>
<p> This handler is called during the Web server startup, this is the master process that creates Web workers for handling Web requests, this process
interacts with the Web workers via IPC sockets between processes and relaunches them if any Web worker dies.</p>
</li>
</ul>

<ul>
<li><p><code>app.configureWorker(options, callback)</code></p>
<p> This handler is called on job worker instance startup after the tables are intialized and it is ready to process the job</p>
</li>
</ul>

<ul>
<li><p><code>app.shutdownWorker(options, callback)</code></p>
<p> Perform last minute operations inside a worker process before exit, the callback must be called eventually which will exit the process.
This method can be overrided to implement custom worker shutdown procedure in order to finish pending tasks like network calls.</p>
</li>
</ul>

<ul>
<li><p><code>app.configureMonitor(options, callback)</code></p>
<p> This callback is called when the monitor process is ready, there is no any other code is supposed to run inside the monitor, but
in case it is needed, this is the hook to be used.</p>
</li>
</ul>

<ul>
<li><p><code>app.configureShell(options, callback)</code></p>
<p> This callback is called by the shell process to setup additional command or to execute a command which is not
supported by the standard shell. Setting options.done to 1 will stop the shell, this is a signal that command has already
been processed.</p>
</li>
</ul>

<h2 id="module-aws">Module: AWS</h2>
<ul>
<li><p><code>aws.configure(options, callback)</code></p>
<p> Initialization of metadata</p>
</li>
</ul>

<ul>
<li><p><code>aws.configureServer(options, callback)</code></p>
<p> Execute on Web server startup</p>
</li>
</ul>

<ul>
<li><p><code>aws.configureMaster(options, callback)</code></p>
<p> Execute on master server startup</p>
</li>
</ul>

<ul>
<li><p><code>aws.readCredentials(profile, callback)</code></p>
<p> Read key and secret from the AWS SDK credentials file, if no profile is given in the config or command line only the default peofile
will be loaded.</p>
</li>
</ul>

<ul>
<li><p><code>aws.getInstanceMeta(path, callback)</code></p>
<p> Retrieve instance meta data</p>
</li>
</ul>

<ul>
<li><p><code>aws.getInstanceCredentials(callback)</code></p>
<p> Retrieve instance credentials using EC2 instance profile and setup for AWS access</p>
</li>
</ul>

<ul>
<li><p><code>aws.getInstanceInfo(callback)</code></p>
<p> Retrieve instance launch index from the meta data if running on AWS instance</p>
</li>
</ul>

<ul>
<li><p><code>aws.parseXMLResponse(err, params, options, callback)</code></p>
<p> Parse AWS response and try to extract error code and message, convert XML into an object.</p>
</li>
</ul>

<ul>
<li><p><code>aws.querySign(region, service, host, method, path, body, headers, credentials, options)</code></p>
<p> Build version 4 signature headers</p>
</li>
</ul>

<ul>
<li><p><code>aws.queryPrepare(action, version, obj, options)</code></p>
<p> Return a request object ready to be sent to AWS, properly formatted</p>
</li>
</ul>

<ul>
<li><p><code>aws.querySigner()</code></p>
<p> It is called in the context of a http request</p>
</li>
</ul>

<ul>
<li><p><code>aws.queryAWS(region, service, proto, host, path, obj, options, callback)</code></p>
<p> Make AWS request, return parsed response as Javascript object or null in case of error</p>
</li>
</ul>

<ul>
<li><p><code>aws.queryEndpoint(service, version, action, obj, options, callback)</code></p>
<p> AWS generic query interface</p>
</li>
</ul>

<ul>
<li><p><code>aws.copyCredentials(obj, options)</code></p>
<p> Copy all credentials properties from the options into the obj</p>
</li>
</ul>

<ul>
<li><p><code>aws.querySQS(action, obj, options, callback)</code></p>
<p> AWS SQS API request</p>
</li>
</ul>

<ul>
<li><p><code>aws.queryIAM(action, obj, options, callback)</code></p>
<p> AWS AIM API request</p>
</li>
</ul>

<ul>
<li><p><code>aws.querySTS(action, obj, options, callback)</code></p>
<p> AWS STS API request</p>
</li>
</ul>

<ul>
<li><p><code>aws.querySES(action, obj, options, callback)</code></p>
<p> AWS SES API request</p>
</li>
</ul>

<ul>
<li><p><code>aws.queryCFN(action, obj, options, callback)</code></p>
<p> AWS CFN API request</p>
</li>
</ul>

<ul>
<li><p><code>aws.queryCW(action, obj, options, callback)</code></p>
<p> AWS CloudWatch API request</p>
</li>
</ul>

<ul>
<li><p><code>aws.queryElastiCache(action, obj, options, callback)</code></p>
<p> AWS Elastic Cache API request</p>
</li>
</ul>

<ul>
<li><p><code>aws.queryAS(action, obj, options, callback)</code></p>
<p> AWS Autoscaling API request</p>
</li>
</ul>

<ul>
<li><p><code>aws.queryRoute53(method, path, data, options, callback)</code></p>
<p> Make a request to Route53 service</p>
</li>
</ul>

<ul>
<li><p><code>aws.queryRekognition(action, obj, options, callback)</code></p>
<p> Make a request to the Rekognition service</p>
</li>
</ul>

<ul>
<li><p><code>aws.querySSM(action, obj, options, callback)</code></p>
<p> AWS SSM API request</p>
</li>
</ul>

<ul>
<li><p><code>aws.queryCWL(action, obj, options, callback)</code></p>
<p> AWS CloudWatch Log API request</p>
</li>
</ul>

<ul>
<li><p><code>aws.queryACM(action, obj, options, callback)</code></p>
<p> AWS ACM API request</p>
</li>
</ul>

<ul>
<li><p><code>aws.getTagValue(obj, key)</code></p>
<p> Returns a tag value by key, default key is Name</p>
</li>
</ul>

<ul>
<li><p><code>aws.stsAssumeRole(options, callback)</code></p>
<p> Assume a role and return new credentials that can be used in other API calls</p>
</li>
</ul>

<ul>
<li><p><code>aws.sqsReceiveMessage(url, options, callback)</code></p>
<p> Receive message(s) from the SQS queue, the callback will receive a list with messages if no error.
The following options can be specified:</p>
<ul>
<li>count - how many messages to receive</li>
<li>timeout - how long to wait, in milliseconds, this is for Long Poll</li>
<li>visibilityTimeout - the duration (in milliseconds) that the received messages are hidden from subsequent retrieve requests
after being retrieved by a ReceiveMessage request.</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>aws.sqsSendMessage(url, body, options, callback)</code></p>
<p> Send a message to the SQS queue.
The options can specify the following:</p>
<ul>
<li>delay - how long to delay this message in milliseconds</li>
<li>attrs - an object with additional message attributes to send, use only string, numbers or binary values,
all other types will be converted into strings</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>aws.sesSendEmail(to, subject, body, options, callback)</code></p>
<p> Send an email via SES
The following options supported:</p>
<ul>
<li>from - an email to use in the From: header</li>
<li>cc - list of email to use in CC: header</li>
<li>bcc - list of emails to use in Bcc: header</li>
<li>replyTo - list of emails to ue in ReplyTo: header</li>
<li>returnPath - email where to send bounces</li>
<li>charset - charset to use, default is UTF-8</li>
<li>html - if set the body is sent as MIME HTML</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>aws.sesSendRawEmail(body, options, callback)</code></p>
<p> Send raw email
The following options accepted:</p>
<ul>
<li>to - list of email addresses to use in RCPT TO</li>
<li>from - an email to use in from header</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>aws.cwPutMetricAlarm(options, callback)</code></p>
<p> Creates or updates an alarm and associates it with the specified Amazon CloudWatch metric.
The options specify the following:</p>
<ul>
<li>name - alarm name, if not specified metric name and dimensions will be used to generate alarm name</li>
<li>metric - metric name, default is <code>CPUUtilization</code></li>
<li>namespace - AWS namespace, default is <code>AWS/EC2</code></li>
<li>op - comparison operator, one of =&gt; | &lt;= | &gt; | &lt; | GreaterThanOrEqualToThreshold | GreaterThanThreshold | LessThanThreshold | LessThanOrEqualToThreshold. Default is <code>&gt;=</code>.</li>
<li>statistic - one of SampleCount | Average | Sum | Minimum | Maximum, default is <code>Average</code></li>
<li>period - collection period in seconds, default is <code>60</code></li>
<li>evaluationPeriods - the number of periods over which data is compared to the specified threshold, default is <code>15</code></li>
<li>threshold - the value against which the specified statistic is compared, default is <code>90</code></li>
<li>ok - ARN(s) to be notified on OK state</li>
<li>alarm - ARN(s) to be notified on ALARM state</li>
<li>insufficient_data - ARN(s) to be notified on INSUFFICIENT_DATA state</li>
<li>dimensions - the dimensions for the alarm&#39;s associated metric.</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>aws.cwPutMetricData(namespace, data, options, callback)</code></p>
<p> Publishes metric data points to Amazon CloudWatch.
The argumernts specify the following:</p>
<ul>
<li>namespace - custome namespace, cannot start with <code>AWS</code></li>
<li>data - an object with metric data:
{ metricName: value }, ...
{ metricName: {<pre><code>   value: Number,
   dimension1: name1,
   ..
},</code></pre>}, ...
{ metricName: {<pre><code>   value: [min, max, sum, sample],
   dimension1: ...
},</code></pre>}, ...</li>
</ul>
<p>The options can specify the following:</p>
<ul>
<li>storageResolution - 1 to use 1 second resolution</li>
<li>timestamp - ms to be used as the timestamp instead of the current time</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>aws.cwListMetrics(options, callback)</code></p>
<p> Return metrics for the given query, the options can be specified:</p>
<ul>
<li>name - a metric name</li>
<li>namespace - limit by namespace: AWS/AutoScaling, AWS Billing, AWS/CloudFront, AWS/DynamoDB, AWS/ElastiCache, AWS/EBS, AWS/EC2, AWS/ELB, AWS/ElasticMapReduce, AWS/Kinesis, AWS/OpsWorks, AWS/Redshift, AWS/RDS, AWS/Route53, AWS/SNS, AWS/SQS, AWS/SWF, AWS/StorageGateway</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>aws.route53Change(names, options, callback)</code></p>
<p> Create or update a host in the Route53 database.</p>
<ul>
<li><code>names</code> is a host name to be set with the current IP address or a list with objects in the format
   [ { name: &quot;..&quot;, value: &quot;1.1.1.1&quot;, type: &quot;A&quot;, ttl: 300 } ...]</li>
</ul>
<p>The <code>options</code> may contain the following:</p>
<ul>
<li>type - default record type, A</li>
<li>ttl - default TTL, 300 seconds</li>
<li>op - an operation, default is UPSERT</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>aws.detectLabels(name, options, callback)</code></p>
<p> Detect image featires using AWS Rekognition service, the <code>name</code> can be a Buffer, a local file or an url to the S3 bucket. In the latter case
the url can be just apath to the file inside a bucket if <code>options.bucket</code> is specified, otherwise it must be a public S3 url with the bucket name
to be the first part of the host name. For CDN/CloudFront cases use the <code>option.bucket</code> option.</p>
</li>
</ul>

<h2 id="module-aws_dynamodb">Module: AWS_DYNAMODB</h2>
<ul>
<li><p><code>aws._queryDDB(target, service, action, obj, options, callback)</code></p>
<p> DynamoDB requests</p>
</li>
</ul>

<ul>
<li><p><code>aws.toDynamoDB(value, level)</code></p>
<p> Convert a Javascript object into DynamoDB object</p>
</li>
</ul>

<ul>
<li><p><code>aws.fromDynamoDB(value, level)</code></p>
<p> Convert a DynamoDB object into Javascript object</p>
</li>
</ul>

<ul>
<li><p><code>aws.queryExpression(params, obj, options, join)</code></p>
<p> Build a condition expression for the given object, all properties in the obj are used</p>
</li>
</ul>

<ul>
<li><p><code>aws.ddbListTables(options, callback)</code></p>
<p> Return list of tables in .TableNames property of the result</p>
<p>Example:</p>
<pre><code>    { TableNames: [ name, ...] }</code></pre></li>
</ul>

<ul>
<li><p><code>aws.ddbDescribeTable(name, options, callback)</code></p>
<p> Return table definition and parameters in the result structure with property of the given table name</p>
<p>Example:</p>
<pre><code>    { name: { AttributeDefinitions: [], KeySchema: [] ...} }</code></pre></li>
</ul>

<ul>
<li><p><code>aws.ddbCreateTable(name, attrs, options, callback)</code></p>
<p> Create a table</p>
<ul>
<li>attrs can be an array in native DDB JSON format or an object with name:type properties, type is one of S, N, NN, NS, BS</li>
<li>options may contain any valid native property if it starts with capital letter and the following:<ul>
<li>waitTimeout - number of milliseconds to wait for ACTIVE status</li>
<li>waitDelay - how often to pool for table status, default is 250ms</li>
<li>keys is an array of column ids used for the primary key or a string with the hash key. if omitted, the first attribute will be used for the primary key</li>
<li>local - an object with each property for a local secondary index name defining key format the same way as for primary keys, all Uppercase properties are added to the top index object</li>
<li>global - an object for global secondary indexes, same format as for local indexes</li>
<li>projections - an object with index name and list of projected properties to be included in the index or &quot;ALL&quot; for all properties, if omitted then default KEYS_ONLY is assumed</li>
<li>readCapacity - read capacity units for provisioned throughput</li>
<li>writeCapacity - write capacity units</li>
<li>onDemand - billing mode, auto provision capacity and pay per request, if no read/write capacity is configured on-demand is the default</li>
<li>stream - enable stream support</li>
</ul>
</li>
</ul>
</li>
</ul>
<p> Example:</p>
<pre><code>      ddbCreateTable(&#39;users&#39;, { id: &#39;S&#39;, mtime: &#39;N&#39;, name: &#39;S&#39;},
                              { keys: [&quot;id&quot;, &quot;name&quot;],
                                local: { mtime: { mtime: &quot;HASH&quot; } },
                                global: { name: { name: &#39;HASH&#39;, ProvisionedThroughput: { ReadCapacityUnits: 50 } } },
                                projections: { mtime: [&#39;gender&#39;,&#39;age&#39;],
                                               name: [&#39;name&#39;,&#39;gender&#39;] },
                                stream: &quot;NEW_IMAGE&quot;,
                                readCapacity: 10,
                                writeCapacity: 10 });</code></pre>
<ul>
<li><p><code>aws.ddbUpdateTable(options, callback)</code></p>
<p> Update tables provisioned throughput settings, options is used instead of table name so this call can be used directly in the cron jobs to adjust
provisionined throughput on demand.
Options must provide the following properties:</p>
<ul>
<li>name - table name</li>
<li>readCapacity and writeCapacity - new povisioned throughtput settings, both must be specified</li>
<li>stream - null to disable or one of the NEW_IMAGE | OLD_IMAGE | NEW_AND_OLD_IMAGES | KEYS_ONLY</li>
<li>add - an object with indexes to create</li>
<li>del - delete a global secondary index by name, a string or a list with multiple indexes</li>
<li>update - an object with indexes to update</li>
<li>waitTimeout - how long to wait in ms until the table is active again</li>
<li>onDemand - true to switch to pat per request mode, false to switch to provisioning mode</li>
</ul>
<p>Example</p>
<pre><code>        aws.ddbUpdateTable({ name: &quot;users&quot;, add: { name_id: { name: &quot;S&quot;, id: &#39;N&#39;, readCapacity: 20, writeCapacity: 20, projections: [&quot;mtime&quot;,&quot;email&quot;] } })
        aws.ddbUpdateTable({ name: &quot;users&quot;, add: { name: { name: &quot;S&quot;, readCapacity: 20, writeCapacity: 20, projections: [&quot;mtime&quot;,&quot;email&quot;] } })
        aws.ddbUpdateTable({ name: &quot;users&quot;, del: &quot;name&quot; })
        aws.ddbUpdateTable({ name: &quot;users&quot;, update: { name: { readCapacity: 10, writeCapacity: 10 } })</code></pre><p>Example of crontab job in etc/crontab:</p>
<pre><code>        [
        { &quot;type&quot;: &quot;server&quot;, &quot;cron&quot;: &quot;0 0 1 * * *&quot;, &quot;job&quot;: { &quot;aws.ddbUpdateTable&quot;: { &quot;name&quot;: &quot;bk_account&quot;, &quot;readCapacity&quot;: 1000, &quot;writeCapacity&quot;: 1000 } } },
        { &quot;type&quot;: &quot;server&quot;, &quot;cron&quot;: &quot;0 0 6 * * *&quot;, &quot;job&quot;: { &quot;aws.ddbUpdateTable&quot;: { &quot;name&quot;: &quot;bk_account&quot;, &quot;readCapacity&quot;: 2000, &quot;writeCapacity&quot;: 2000 } } }
        ]</code></pre></li>
</ul>

<ul>
<li><p><code>aws.ddbUpdateTimeToLive(options, callback)</code></p>
<p> Update TTL attribute.
The options properties:</p>
<ul>
<li>name - table name</li>
<li>attribute - the attribute name</li>
<li>enabled - true or false</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>aws.ddbDescribeTimeToLive(name, options, callback)</code></p>
<p> Returns status of Time to live attribute for a table</p>
</li>
</ul>

<ul>
<li><p><code>aws.ddbDeleteTable(name, options, callback)</code></p>
<p> Remove a table from the database.
By default the callback will ba callled only after the table is deleted, specifying <code>options.nowait</code> will return immediately</p>
</li>
</ul>

<ul>
<li><p><code>aws.ddbWaitForTable(name, item, options, callback)</code></p>
<p> Call the callback after specified period of time or when table status become different from the given waiting status.
if options.waitTimeout is not specified calls the callback immediately. options.waitStatus is checked if given and keeps waiting
while the status is equal to it. options.waitDelay can be specified how often to request new status, default is 250ms.</p>
</li>
</ul>

<ul>
<li><p><code>aws.ddbPutItem(name, item, options, callback)</code></p>
<p> Put or add an item</p>
<ul>
<li>item is an object, type will be inferred from the native js type.</li>
<li>options may contain any valid native property if it starts with capital letter or special properties:<ul>
<li>expected - an object with column names to be used in Expected clause and value as null to set condition to { Exists: false } or<pre><code>any other exact value to be checked against which corresponds to { Exists: true, Value: value }</code></pre></li>
<li>expectedJoin - how to join conditions, default is AND</li>
<li>expr - condition expression</li>
<li>values - an object with values map to be used for in the update and/or condition expressions, to be used<pre><code>for ExpressionAttributeValues parameters</code></pre></li>
<li>names - an object with a map to be used for attribute names in condition and update expressions, to be used<pre><code>for ExpressionAttributeNames parameter</code></pre></li>
<li>returning - values to be returned on success, any value means ALL_OLD</li>
</ul>
</li>
</ul>
<p>Example:</p>
<pre><code>    ddbPutItem(&quot;users&quot;, { id: 1, name: &quot;john&quot;, mtime: 11233434 }, { expected: { name: null } })</code></pre></li>
</ul>

<ul>
<li><p><code>aws.ddbUpdateItem(name, keys, item, options, callback)</code></p>
<p> Update an item</p>
<ul>
<li>keys is an object with primary key attributes name and value.</li>
<li>item is an object with properties where value can be:<ul>
<li>number/string/array - action PUT, replace or add new value</li>
<li>null/empty string - action DELETE</li>
</ul>
</li>
<li>item can be a string with Update expression</li>
<li>options may contain any valid native property if it starts with capital letter or special properties:<ul>
<li>expr - condition expression</li>
<li>values - an object with values map to be used for in the update and/or condition expressions, to be used
  for ExpressionAttributeValues parameters</li>
<li>names - an object with a map to be used for attribute names in condition and update expressions, to be used
  for ExpressionAttributeNames parameter</li>
<li>action - an object with operators to be used for properties, one of the: SET, REMOVE, DELETE, ADD, APPEND, PREPEND, NOT_EXISTS</li>
<li>expected - an object with columns to be used in ConditionExpression, value null means the attribute does not exists,
  any other value to be checked against using regular compare rules. The conditional comparison operator is taken
  from <code>options.ops</code> the same way as for queries.</li>
<li>returning - values to be returned on success, <code>*</code> or <code>new</code> means ALL_NEW, <code>old</code> means ALL_OLD,<pre><code>        `updated` means UPDATED_NEW, `old_updated` means UPDATED_OLD</code></pre></li>
</ul>
</li>
</ul>
<p>Example:</p>
<pre><code>    ddbUpdateItem(&quot;users&quot;, { id: 1, name: &quot;john&quot; }, { gender: &#39;male&#39;, icons: &#39;1.png&#39; }, { action: { icons: &#39;ADD&#39; }, expected: { id: 1 }, ReturnValues: &quot;ALL_NEW&quot; })
    ddbUpdateItem(&quot;users&quot;, { id: 1, name: &quot;john&quot; }, { gender: &#39;male&#39;, icons: &#39;1.png&#39; }, { action: { icons: &#39;ADD&#39; }, expected: { id: null } })
    ddbUpdateItem(&quot;users&quot;, { id: 1, name: &quot;john&quot; }, { gender: &#39;male&#39;, icons: &#39;1.png&#39;, num: 1 }, { action: { num: &#39;ADD&#39;, icons: &#39;ADD&#39; }, expected: { id: null, num: 0 }, ops: { num: &quot;gt&quot; } })</code></pre></li>
</ul>

<ul>
<li><p><code>aws.ddbDeleteItem(name, keys, options, callback)</code></p>
<p> Delete an item from a table</p>
<ul>
<li>keys is an object with name: value for hash/range attributes</li>
<li>options may contain any valid native property if it starts with capital letter and the following special options:<ul>
<li>expr - condition expression</li>
<li>values - an object with values map to be used for in the update and/or condition expressions, to be used
  for ExpressionAttributeValues parameters</li>
<li>names - an object with a map to be used for attribute names in condition and update expressions, to be used
  for ExpressionAttributeNames parameter</li>
<li>returning - values to be returned on success, any value means ALL_OLD</li>
</ul>
</li>
</ul>
<p>Example:</p>
<pre><code>    ddbDeleteItem(&quot;users&quot;, { id: 1, name: &quot;john&quot; }, {})</code></pre></li>
</ul>

<ul>
<li><p><code>aws.ddbBatchWriteItem(items, options, callback)</code></p>
<p> Update items from the list at the same time</p>
<ul>
<li>items is a list of objects with table name as property and list of operations, an operation can be PutRequest or DeleteRequest</li>
<li>options may contain any valid native property if it starts with capital letter.</li>
</ul>
<p>Example:</p>
<pre><code>    { table: [ { put: { id: 1, name: &quot;tt&quot; } }, { del: { id: 2 } }] }</code></pre></li>
</ul>

<ul>
<li><p><code>aws.ddbBatchGetItem(items, options, callback)</code></p>
<p> Retrieve all items for given list of keys</p>
<ul>
<li>items is an object with table name as property name and list of options for GetItem request</li>
<li>options may contain any valid native property if it starts with capital letter.</li>
</ul>
<p>Example:</p>
<pre><code>    { users: { keys: [{ id: 1, name: &quot;john&quot; },{ id: .., name: .. }], select: [&#39;name&#39;,&#39;id&#39;], consistent: true }, ... }</code></pre></li>
</ul>

<ul>
<li><p><code>aws.ddbGetItem(name, keys, options, callback)</code></p>
<p> Retrieve one item by primary key</p>
<ul>
<li><p>keys - an object with primary key attributes name and value.</p>
</li>
<li><p>select - list of columns to return, otherwise all columns will be returned</p>
</li>
<li><p>options may contain any native property allowed in the request or special properties:</p>
<ul>
<li><p>consistent - set consistency level for the request</p>
</li>
<li><p>names - an object with a map to be used for attribute names in condition and update expressions, to be used
  for ExpressionAttributeNames parameter
Example:</p>
<p> ddbGetItem(&quot;users&quot;, { id: 1, name: &quot;john&quot; }, { select: &#39;id,name&#39; })</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>aws.ddbQueryTable(name, condition, options, callback)</code></p>
<p> Query on a table, return all matching items</p>
<ul>
<li>condition is an object with name: value pairs, by default EQ opeartor is used for comparison</li>
<li>options may contain any valid native property if it starts with capital letter or special property:<ul>
<li>start - defines starting primary key when paginating, can be a string/number for hash or an object with hash/range properties</li>
<li>consistent - set consistency level for the request</li>
<li>select - list of attributes to get only</li>
<li>total - return number of matching records</li>
<li>count - limit number of record in result</li>
<li>desc - descending order</li>
<li>sort - index name to use, indexes are named the same as the corresponding column, with index primary keys for Keycondition will be used</li>
<li>ops - an object with operators to be used for properties if other than EQ.</li>
<li>keys - list of primary key columns, if there are other properties in the condition then they will be<pre><code>   put into QueryFilter instead of KeyConditions. If keys are absent, all properties in the condition are treated as primary keys.</code></pre></li>
<li>projection - projection expression</li>
<li>values - an object with values map to be used for in the update and/or condition expressions, to be used
  for ExpressionAttributeValues parameters</li>
<li>names - an object with a map to be used for attribute names in condition and update expressions, to be used
  for ExpressionAttributeNames parameter</li>
<li>expr - filtering expression</li>
</ul>
</li>
</ul>
<p>Example:</p>
<pre><code>    aws.ddbQueryTable(&quot;users&quot;, { id: 1, name: &quot;john&quot; }, { select: &#39;id,name&#39;, ops: { name: &#39;gt&#39; } })
    aws.ddbQueryTable(&quot;users&quot;, { id: 1, name: &quot;john&quot;, status: &quot;ok&quot; }, { keys: [&quot;id&quot;], select: &#39;id,name&#39;, ops: { name: &#39;gt&#39; } })
    aws.ddbQueryTable(&quot;users&quot;, { id: 1 }, { expr: &quot;status=:s&quot;, values: { s: &quot;status&quot; } })</code></pre></li>
</ul>

<ul>
<li><p><code>aws.ddbScanTable(name, condition, options, callback)</code></p>
<p> Scan a table for all matching items</p>
<ul>
<li>condition is an object with name: value pairs or a string with FilterExpression</li>
<li>options may contain any valid native property if it starts with capital letter or special property:<ul>
<li>start - defines starting primary key</li>
<li>ops - an object with operators to be used for properties if other than EQ.</li>
<li>projection - projection expression</li>
<li>values - an object with values map to be used for in the update and/or condition expressions, to be used
  for ExpressionAttributeValues parameters</li>
<li>names - an object with a map to be used for attribute names in condition and update expressions, to be used
  for ExpressionAttributeNames parameter</li>
</ul>
</li>
</ul>
<p>Example:</p>
<pre><code>    aws.ddbScanTable(&quot;users&quot;, { id: 1, name: &#39;a&#39; }, { ops: { name: &#39;gt&#39; }})
    aws.ddbScanTable(&quot;users&quot;, &quot;id=:id AND name=:name&quot;, { values: { id: 1, name: &#39;a&#39; } });</code></pre></li>
</ul>

<ul>
<li><p><code>aws.ddbTransactWriteItems(items, options, callback)</code></p>
<p> Update items from the list at the same time in one transaction, on any failure everything is rolled back</p>
<ul>
<li>items is a list of operations to be performed in the same format as for aws.ddbPutItem, aws.ddbUpdateItem, aws.ddbDeleteItem and aws.ddbQueryItem</li>
<li>options may contain any valid native property if it starts with capital letter.</li>
</ul>
<p>Example:</p>
<pre><code>    { op: &quot;put&quot;: table: &quot;table-name&quot;, obj: { id: 1, name: &quot;tt&quot; } },
    { op: &quot;del&quot;: table: &quot;table-name&quot;, obj: { id: 2 } },
    { op: &quot;update&quot;: table: &quot;table-name&quot;, obj: { id: 1, name: &quot;test&quot; }, options: { expected: { status: &quot;ok&quot; } } },
    { op: &quot;check&quot;: table: &quot;table-name&quot;, obj: { id: 1 }, options: { expected: { status: &quot;ok&quot; } } }</code></pre></li>
</ul>

<h2 id="module-aws_dynamodbstreams">Module: AWS_DYNAMODBSTREAMS</h2>
<h2 id="module-aws_ec2">Module: AWS_EC2</h2>
<ul>
<li><p><code>aws.queryEC2(action, obj, options, callback)</code></p>
<p> AWS EC2 API request</p>
</li>
</ul>

<ul>
<li><p><code>aws.queryELB(action, obj, options, callback)</code></p>
<p> AWS ELB API request</p>
</li>
</ul>

<ul>
<li><p><code>aws.ec2RunInstances(options, callback)</code></p>
<p> Run AWS instances, supports all native EC2 parameters with first capital letter but also accepts simple parameters in the options:</p>
<ul>
<li>min - min number of instances to run, default 1</li>
<li>max - max number of instances to run, default 1</li>
<li>imageId - AMI id, use aws.imageId if not given or options.ImageId attribute</li>
<li>instanceType - instance type, use aws.instanceType if not given or options.InstanceType attribute</li>
<li>keyName - Keypair, use aws.keyName if not given or options.KeyName attribute</li>
<li>data - user data, in clear text</li>
<li>terminate - set instance initiated shutdown behaviour to terminate</li>
<li>stop - set instance initiated shutdown behaviour to stop</li>
<li>groupId - one group id or an array with security group ids</li>
<li>ip - a static private IP adress to assign</li>
<li>publicIp - associate with a public IP address</li>
<li>file - pass contents of a file as user data, contents are read using sync method</li>
<li>noPrepare - even with additional tasks specified do not wai but return the context for aws.ec2PrepareInstance</li>
<li>waitTimeout - how long to wait in ms for instance to be runnable</li>
<li>waitDelay  - now often in ms to poll for status while waiting</li>
<li>waitRunning - if 1 then wait for instance to be in running state, this is implied also by elbName, name, elasticIp properties in the options</li>
<li>name - assign a tag to the instance as <code>Name:</code>, any occurences of %i will be replaced with the instance index</li>
<li>tags - additional tags to be assigned, an object with key:value</li>
<li>elbName - join elastic balancer after the startup</li>
<li>targetGroup - join ELB target group after the startup</li>
<li>elasticIp - asociate with the given Elastic IP address after the start</li>
<li>iamProfile - IAM profile to assign for instance credentials, if not given use aws.iamProfile or options[&#39;IamInstanceProfile.Name&#39;] attribute</li>
<li>availabilityZone - availability zone, if not given use aws.zone or options[&#39;Placement.AvailabilityZone&#39;] attribute</li>
<li>subnetId - subnet id, if not given use aws.subnetId or options.SubnetId attribute</li>
<li>alarms - a list with CloudWatch alarms to create for the instance, each value of the object represent an object with options to be
  passed to the cwPutMetricAlarm method.</li>
</ul>
<p>The callback will take 3 arguments: callback(err, rc, info) where info will contain properties that can be used by `aws.ec2PrepareInstance</p>
</li>
</ul>

<ul>
<li><p><code>aws.ec2AfterRunInstances(options, callback)</code></p>
<p> Perform the final tasks after an instance has been launched like wait for status, assign Elastic IP or tags..</p>
</li>
</ul>

<ul>
<li><p><code>aws.ec2WaitForInstance(instanceId, status, options, callback)</code></p>
<p> Check an instance status and keep waiting until it is equal what we expect or timeout occured.
The <code>status</code> can be one of: pending | running | shutting-down | terminated | stopping | stopped
The options can specify the following:</p>
<ul>
<li>waitTimeout - how long to wait in ms until give up, default is 30 secs</li>
<li>waitDelay - how long in ms between polls</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>aws.ec2DescribeSecurityGroups(options, callback)</code></p>
<p> Describe security groups, optionally if <code>options.filter</code> regexp is provided then limit the result to the matched groups only,
return list of groups to the callback</p>
</li>
</ul>

<ul>
<li><p><code>aws.ec2DescribeInstances(options, callback)</code></p>
<p> Describe instances according to the query filters, returns a list with instances, the following properties
can be used:</p>
<ul>
<li>vpcId - VPC to get instances from</li>
<li>instanceId - list of instances to show only</li>
<li>tagName - filter by tag name(s)</li>
<li>tagKey - filter by tag key(s)</li>
<li>groupName - filter by group name(s)</li>
<li>stateName - instances state(s)</li>
<li>filters - an object with filters to send as is</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>aws.ec2ParseInstances(rc)</code></p>
<p> Pare DescribeInstances results and return a list of instances as a plain list</p>
</li>
</ul>

<ul>
<li><p><code>aws.ec2CreateTags(id, name, options, callback)</code></p>
<p> Create tags for a resource.
The name is a string, an array or an object with tags. The options also may contain tags property which is an object with tag key and value</p>
<p>Example</p>
<pre><code>aws.ec2CreateTags(&quot;i-1234&quot;,&quot;My Instance&quot;, { tags: { tag2 : &quot;val2&quot;, tag3: &quot;val3&quot; } } )
aws.ec2CreateTags(&quot;i-1234&quot;, { tag2: &quot;val2&quot;, tag3: &quot;val3&quot; })
aws.ec2CreateTags(&quot;i-1234&quot;, [ &quot;tag2&quot;, &quot;val2&quot;, &quot;tag3&quot;, &quot;val3&quot; ])</code></pre></li>
</ul>

<ul>
<li><p><code>aws.ec2AssociateAddress(instanceId, elasticIp, options, callback)</code></p>
<p> Associate an Elastic IP with an instance. Default behaviour is to reassociate if the EIP is taken.
The options can specify the following:</p>
<ul>
<li>subnetId - required for instances in VPC, allocation id will be retrieved for the given ip address automatically</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>aws.ec2CreateImage(options, callback)</code></p>
<p> Create an EBS image from the instance given or the current instance running</p>
</li>
</ul>

<ul>
<li><p><code>aws.ec2DeregisterImage(ami_id, options, callback)</code></p>
<p> Deregister an AMI by id. If <code>options.snapshots</code> is set, then delete all snapshots for this image as well</p>
</li>
</ul>

<ul>
<li><p><code>aws.elbRegisterInstances(name, instance, options, callback)</code></p>
<p> Register an instance(s) with ELB, instance can be one id or a list of ids</p>
</li>
</ul>

<ul>
<li><p><code>aws.elbDeregisterInstances(name, instance, options, callback)</code></p>
<p> Deregister an instance(s) from ELB, instance can be one id or a list of ids</p>
</li>
</ul>

<ul>
<li><p><code>aws.elb2RegisterInstances(target, instance, options, callback)</code></p>
<p> Register an instance(s) with ELB, instance can be one id or a list of ids</p>
</li>
</ul>

<ul>
<li><p><code>aws.ssmSendCommand(cmds, instances, options, callback)</code></p>
<p> Run a shell command</p>
</li>
</ul>

<ul>
<li><p><code>aws.ssmWaitForCommand(cmdId, instanceId, options, callback)</code></p>
<p> Return a command details</p>
</li>
</ul>

<h2 id="module-aws_s3">Module: AWS_S3</h2>
<ul>
<li><p><code>aws.signS3(method, bucket, path, body, options)</code></p>
<p> Sign S3 AWS request, returns url to be send to S3 server, options will have all updated headers to be sent as well</p>
</li>
</ul>

<ul>
<li><p><code>aws.queryS3(bucket, path, options, callback)</code></p>
<p> S3 requests
Options may contain the following properties:</p>
<ul>
<li>method - HTTP method</li>
<li>query - query parameters for the url as an object</li>
<li>postdata - any data to be sent with POST</li>
<li>postfile - file to be uploaded to S3 bucket</li>
<li>expires - absolute time when this request is expires</li>
<li>headers - HTTP headers to be sent with request</li>
<li>file - file name where to save downloaded contents</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>aws.s3List(path, options, callback)</code></p>
<p> Retrieve a list of files from S3 bucket, only files inside the path will be returned</p>
</li>
</ul>

<ul>
<li><p><code>aws.s3GetFile(path, options, callback)</code></p>
<p> Retrieve a file from S3 bucket, root of the path is a bucket, path can have a protocol prepended like s3://, it will be ignored</p>
</li>
</ul>

<ul>
<li><p><code>aws.s3PutFile(path, file, options, callback)</code></p>
<p> Upload a file to S3 bucket, <code>file</code> can be a Buffer or a file name</p>
</li>
</ul>

<ul>
<li><p><code>aws.s3ParseUrl(link)</code></p>
<p> Parse an S3 URL and return an object with bucket and path</p>
</li>
</ul>

<ul>
<li><p><code>aws.s3Proxy(res, bucket, path, options, callback)</code></p>
<p> Proxy a file from S3 bucket into the existing HTTP response <code>res</code></p>
</li>
</ul>

<h2 id="module-aws_sns">Module: AWS_SNS</h2>
<ul>
<li><p><code>aws.querySNS(action, obj, options, callback)</code></p>
<p> AWS SNS API request</p>
</li>
</ul>

<ul>
<li><p><code>aws.snsCreatePlatformEndpoint(token, options, callback)</code></p>
<p> Creates an endpoint for a device and mobile app on one of the supported push notification services, such as GCM and APNS.</p>
<p>The following properties can be specified in the options:</p>
<ul>
<li>appArn - an application ARN to be used for push notifications, if not passed, global <code>-sns-app-arn</code> will be used.</li>
<li>data - a user data to be associated with the endpoint arn</li>
</ul>
<p>All capitalized properties in the options will be pased as is. The callback will be called with an error if any and the endpoint ARN</p>
</li>
</ul>

<ul>
<li><p><code>aws.snsSetEndpointAttributes(arn, options, callback)</code></p>
<p> Sets the attributes for an endpoint for a device on one of the supported push notification services, such as GCM and APNS.</p>
<p>The following properties can be specified in the options:</p>
<ul>
<li>token - a device token for the notification service</li>
<li>data - a user data to be associated with the endpoint arn</li>
<li>enabled - true or false to enable/disable the deliver of notifications to this endpoint</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>aws.snsDeleteEndpoint(arn, options, callback)</code></p>
<p> Deletes the endpoint from Amazon SNS.</p>
</li>
</ul>

<ul>
<li><p><code>aws.snsPublish(arn, msg, options, callback)</code></p>
<p> Sends a message to all of a topic&#39;s subscribed endpoints or to a mobile endpoint.
If msg is an object, then it will be pushed as JSON.
The options may take the following properties:</p>
<ul>
<li>subject - optional subject to be included in the message if the target supports it</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>aws.snsCreateTopic(name, options, callback)</code></p>
<p> Creates a topic to which notifications can be published. The callback returns topic ARN on success.</p>
</li>
</ul>

<ul>
<li><p><code>aws.snsSetTopicAttributes(arn, options, callback)</code></p>
<p> Updates the topic attributes.
The following options can be used:</p>
<ul>
<li>name - new topic name</li>
<li>policy - an object with access policy</li>
<li>deliveryPolicy - an object with delivery attributes, can specify all or only the ones that needed to be updated</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>aws.snsDeleteTopic(arn, options, callback)</code></p>
<p> Deletes the topic from Amazon SNS.</p>
</li>
</ul>

<ul>
<li><p><code>aws.snsSubscribe(arn, endpoint, options, callback)</code></p>
<p> Creates a topic to which notifications can be published. The callback returns topic ARN on success, if the topic requires
confirmation the arn returned will be null and a token will be sent to the endpoint for confirmation.</p>
</li>
</ul>

<ul>
<li><p><code>aws.snsConfirmSubscription(arn, token, options, callback)</code></p>
<p> Verifies an endpoint owner&#39;s intent to receive messages by validating the token sent to the
endpoint by an earlier Subscribe action. If the token is valid, the action creates a new subscription
and returns its Amazon Resource Name (ARN) in the callback.</p>
</li>
</ul>

<ul>
<li><p><code>aws.snsSetSubscriptionAttributes(arn, options, callback)</code></p>
<p> Updates the subscription attributes.
The following options can be used:</p>
<ul>
<li>name - new topic name</li>
<li>deliveryPolicy - an object with delivery attributes, can specify all or only the ones that needed to be updated</li>
<li>minDelayTarget - update delivery policy by attribute name</li>
<li>maxDelayTarget</li>
<li>numRetries</li>
<li>numMaxDelayRetries</li>
<li>backoffFunction - one of linear|arithmetic|geometric|exponential</li>
<li>maxReceivesPerSecond</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>aws.snsUnsubscribe(arn, options, callback)</code></p>
<p> Creates a topic to which notifications can be published. The callback returns topic ARN on success.</p>
</li>
</ul>

<ul>
<li><p><code>aws.snsListTopics(options, callback)</code></p>
<p> Creates a topic to which notifications can be published. The callback returns topic ARN on success.</p>
</li>
</ul>

<h2 id="module-core">Module: CORE</h2>
<ul>
<li><p><code>core.init(options, callback)</code></p>
<p> Main initialization, must be called prior to perform any actions.</p>
<p>If options are given they may contain the following properties:</p>
<ul>
<li>noDb - if true do not initialize database</li>
<li>noConfigure - do not run all configure methods</li>
<li>noDns - do not retrieve config from DNS</li>
<li>noWatch - do not watch and reload config files</li>
<li>noModules - do not load modules</li>
<li>noLocales - do not load locales</li>
<li>denyModules - which modules should not be loaded</li>
<li>allowModules - which modules to load</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>core.run(options, callback)</code></p>
<p> Run any backend function after environment has been initialized, this is to be used in shell scripts,
core.init will parse all command line arguments, the simplest case to run from /data directory and it will use
default environment or pass -home dir so the script will reuse same config and paths as the server
context can be specified for the callback, if no then it run in the core context</p>
<ul>
<li>require(&#39;backendjs&#39;).run(function() {}) is one example where this call is used as a shortcut for ad-hoc scripting</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>core.exit(code, msg)</code></p>
<p> Exit the process with possible message to be displayed and status code</p>
</li>
</ul>

<ul>
<li><p><code>core.setHome(home)</code></p>
<p> Switch to new home directory, exit if we cannot, this is important for relative paths to work if used,
no need to do this in worker because we already switched to home directory in the master and all child processes
inherit current directory
Important note: If run with combined server or as a daemon then this MUST be an absolute path, otherwise calling
it in the spawned web master will fail due to the fact that we already set the home and relative path will not work after that.</p>
</li>
</ul>

<ul>
<li><p><code>core.parseConfig(data, pass)</code></p>
<p> Parse config lines for the file or other place</p>
</li>
</ul>

<ul>
<li><p><code>core.parseArgs(argv, pass)</code></p>
<p> Parse command line arguments</p>
</li>
</ul>

<ul>
<li><p><code>core.processArgs(ctx, argv, pass)</code></p>
<p> Config parameters defined in a module as a list of parameter names prefixed with module name, a parameters can be
a string which defines text parameter or an object with the properties: name, type, value, decimals, min, max, separator
type can be bool, number, list, json</p>
</li>
</ul>

<ul>
<li><p><code>core.describeArgs(module, args)</code></p>
<p> Add custom config parameters to be understood and processed by the config parser</p>
<ul>
<li>module - name of the module to add these params to, if it is an empty string or skipped then the module where any
parameter goes is determined by the prefix, for example if name is &#39;aws-elastic-ip&#39; then it will be added to the aws module,
all not matched parameters will be added to the core module.</li>
<li>args - a list of objects in the format: { name: N, type: T, descr: D, min: M, max: M, array: B }, all except name are optional.</li>
</ul>
<p>Example:</p>
<pre><code>core.describeArgs(&quot;api&quot;, [ { name: &quot;num&quot;, type: &quot;int&quot;, descr: &quot;int param&quot; }, { name: &quot;list&quot;, array: 1, descr: &quot;list of words&quot; } ]);
core.describeArgs([ { name: &quot;api-list&quot;, array: 1, descr: &quot;list of words&quot; } ]);</code></pre></li>
</ul>

<ul>
<li><p><code>core.loadConfig(file, callback)</code></p>
<p> Parse the config file, configFile can point to a file or can be skipped and the default file will be loaded</p>
</li>
</ul>

<ul>
<li><p><code>core.loadDnsConfig(options, callback)</code></p>
<p> Load configuration from the DNS TXT records</p>
</li>
</ul>

<ul>
<li><p><code>core.loadLocales(options, callback)</code></p>
<p> Load configured locales</p>
</li>
</ul>

<ul>
<li><p><code>core.runMethods(name, options, callback)</code></p>
<p> Run a method for every module, a method must conform to the following signature: <code>function(options, callback)</code> and
call the callback when finished. The callback second argument will be the options, so it is possible to pass anything
in the options back to the caller.</p>
<p>The following properties can be specified in the options:</p>
<ul>
<li>allowModules - a regexp of the modules names to be called only</li>
<li>stopOnError - on first error stop and return, otherwise all errors are ignored and all modules are processed</li>
<li>errorLogger - options for the logger on error, if not specified an error with status 200 will be reported with
log level &#39;info&#39; and other errors with level &#39;error&#39;, the level is specified by the &#39;log&#39; property, the rest will be passed to <code>lib.objDescr</code></li>
<li>stopFilter - a function to be called after each pass to check if the processing must be stopped, it must return true to stop</li>
<li>parallel - if true run methods for all modules in parallel</li>
<li>concurrency - if a number greater than 1 run that many methods in parallel</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>core.runMethod(module, name, options, callback)</code></p>
<p> Run a method for the given module</p>
</li>
</ul>

<ul>
<li><p><code>core.addModule()</code></p>
<p> Adds reference to the objects in the core for further access, specify module name, module reference pairs.
This is used the the core itcore to register all internal modules and makes it available in the shell and in the <code>core.modules</code> object.</p>
<p>Also this is used when cresting modular backend application by separating the logic into different modules, by registering such
modules with the core it makes the module a first class citizen in the backendjs core and exposes all the callbacks and methods.</p>
<p>For example, the module below will register API routes and some methods</p>
<pre><code> var bkjs = require(&quot;backendjs&quot;);
 var mymod = {}
 exports.module = mymod;
 core.addModule(&quot;mymod&quot;, mymod);
 mymod.configureWeb = function(options, callback) {
    bkjs.api.app.all(&quot;/mymod&quot;, function(req, res) {
         res.json({});
    });
 }</code></pre></li>
</ul>
<p> In the main app.js just load it and the rest will be done automatically, i.e. routes will be created ...</p>
<pre><code>   var mymod = require(&quot;./mymod.js&quot;);</code></pre><p> Running the shell will make the object <code>mymod</code> available</p>
<pre><code>   ./app.sh -shell
   &gt; mymod
     {}</code></pre>
<ul>
<li><p><code>core.loadModules(dir, options, callback)</code></p>
<p> Dynamically load services from the specified directory.</p>
<p>The modules are loaded using <code>require</code> as a normal nodejs module but in addition if the module exports
<code>init</code> method it is called immediately with options passed as an argument. This is a synchronous function so it is supposed to be
called on startup, not dynamically during a request processing. Only top level .js files are loaded, not subdirectories. <code>core.addModule</code> is called
automatically.</p>
<p>Modules can be sorted by a priority, if .priority property is defined in the module it will be used to sort the modules, the higher priority the
closer to the top the module will be. The position of a module in the <code>core.modules</code> will define the order <code>runMethods</code> will call.</p>
<p><strong>Caution must be taken for module naming, it is possible to override any default bkjs module which will result in unexpected behaviour</strong></p>
<p>The following options properties can be specified:</p>
<ul>
<li>denyModules - a regexp with modules name(s) to be excluded from loading, the basename of a file is checked only</li>
<li>allowModules - a regexp with modules name(s) to be loaded only</li>
</ul>
<p>Example, to load all modules from the local relative directory</p>
<pre><code> core.loadModules(&quot;modules&quot;)</code></pre></li>
</ul>

<ul>
<li><p><code>core.httpGet(uri, params, callback)</code></p>
<p> Make a HTTP request, see <code>httpGet</code> module for more details.</p>
</li>
</ul>

<ul>
<li><p><code>core.sendRequest(options, callback)</code></p>
<p> Make a HTTP request using <code>httpGet</code> with ability to sign requests.</p>
<p>The POST request is made, if data is an object, it is converted into string.</p>
<p>Returns params as in <code>httpGet</code> with .json property assigned with an object from parsed JSON response.</p>
<p><em>When used with API endpoints, the <code>backend-host</code> parameter must be set in the config or command line to the base URL of the backend,
like <a href="http://localhost:8000">http://localhost:8000</a>, this is when <code>uri</code> is relative URL. Absolute URLs do not need this parameter.</em></p>
<p>Special parameters for options:</p>
<ul>
<li>url - url if options is first argument</li>
<li>login - login to use for access credentials instead of global credentials</li>
<li>secret - secret to use for access instead of global credentials</li>
<li>checksum - calculate checksum from the data</li>
<li>obj - return just the result object, not the whole params</li>
</ul>
</li>
</ul>

<h2 id="module-core_utils">Module: CORE_UTILS</h2>
<ul>
<li><p><code>core.processName()</code></p>
<p> Return unique process name based on the cluster status, worker or master and the role. This is can be reused by other workers within the role thus
making it usable for repeating environments or storage solutions.</p>
</li>
</ul>

<ul>
<li><p><code>core.showHelp(options)</code></p>
<p> Print help about command line arguments and exit</p>
</li>
</ul>

<ul>
<li><p><code>core.sendmail(options, callback)</code></p>
<p> Send email</p>
</li>
</ul>

<ul>
<li><p><code>core.killBackend(name, signal, callback)</code></p>
<p> Kill all backend processes that match name and not the current process</p>
</li>
</ul>

<ul>
<li><p><code>core.shutdown()</code></p>
<p> Shutdown the machine now</p>
</li>
</ul>

<ul>
<li><p><code>core.setTimeout(name, callback, timeout)</code></p>
<p> Set or reset a timer</p>
</li>
</ul>

<ul>
<li><p><code>core.createServer(options, callback)</code></p>
<p> Create a Web server with options and request handler, returns a server object.</p>
<p>Options can have the following properties:</p>
<ul>
<li>port - port number is required</li>
<li>bind - address to bind</li>
<li>restart - name of the processes to restart on address in use error, usually &quot;web&quot;</li>
<li>ssl - an object with SSL options for TLS createServer call</li>
<li>timeout - number of milliseconds for the request timeout</li>
<li>name - server name to be assigned</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>core.createRepl(options)</code></p>
<p> Create REPL interface with all modules available</p>
</li>
</ul>

<ul>
<li><p><code>core.startRepl(port, bind, options)</code></p>
<p> Start command prompt on TCP socket, context can be an object with properties assigned with additional object to be accessible in the shell</p>
</li>
</ul>

<ul>
<li><p><code>core.watchTmp(dir, options, callback)</code></p>
<p> Watch temp files and remove files that are older than given number of seconds since now, remove only files that match pattern if given
Options properties:</p>
<ul>
<li>match - a regexp that specifies only files to be watched</li>
<li>ignore - a regexp of files to be ignored</li>
<li>seconds - number of seconds a file to be older to be deleted</li>
<li>nodirs - if 1 skip deleting directories</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>core.watchLogs(options, callback)</code></p>
<p> Watch log files for errors and report via email or POST url, see config parameters starting with <code>logwatcher-</code> about how this works</p>
</li>
</ul>

<ul>
<li><p><code>core.cookieGet(domain, callback)</code></p>
<p> Return cookies that match given domain</p>
</li>
</ul>

<ul>
<li><p><code>core.cookieSave(cookiejar, setcookies, hostname, callback)</code></p>
<p> Save new cookies arrived in the request,
merge with existing cookies from the jar which is a list of cookies before the request</p>
</li>
</ul>

<h2 id="module-db">Module: DB</h2>
<ul>
<li><p><code>db</code></p>
<p> The Database API, a thin abstraction layer on top of SQLite, PostgreSQL, DynamoDB and Cassandra.
The idea is not to introduce new abstraction layer on top of all databases but to make
the API usable for common use cases. On the source code level access to all databases will be possible using
this API but any specific usage like SQL queries syntax or data types available only for some databases will not be
unified or automatically converted but passed to the database directly. Only conversion between JavaScript types and
database types is unified to some degree meaning JavaScript data type will be converted into the corresponding
data type supported by any particular database and vice versa.</p>
<p>Basic operations are supported for all database and modelled after NoSQL usage, this means no SQL joins are supported
by the API, only single table access. SQL joins can be passed as SQL statements directly to the database using low level db.query
API call, all high level operations like add/put/del perform SQL generation for single table on the fly.</p>
<p>The common convention is to pass options object with flags that are common for all drivers along with specific,
this options object can be modified with new properties but all driver should try not to
modify or delete existing properties, so the same options object can be reused in subsequent operations.</p>
<p>All queries and update operations ignore properties that starts with underscore.</p>
<p>Before the DB functions can be used the <code>core.init</code> MUST be called first, the typical usage:</p>
<pre><code>    var backend = require(&quot;backendjs&quot;), core = backend.core, db = backend.db;
    core.init(function(err) {
        db.add(...
        ...
    });</code></pre><p>All database methods can use default db pool or any other available db pool by using <code>pool: name</code> in the options. If not specified,
then default db pool is used, sqlite is default if no -db-pool config parameter specified in the command line or the config file.
Even if the specified pool does not exist, the default pool will be returned, this allows to pre-confgure the app with different pools
in the code and enable or disable any particular pool at any time.</p>
<p>Example, use PostgreSQL db pool to get a record and update the current pool</p>
<pre><code>    db.get(&quot;bk_account&quot;, { id: &quot;123&quot; }, { pool: &quot;pgsql&quot; }, function(err, row) {
        if (row) db.update(&quot;bk_account&quot;, row);
    });</code></pre><p>Most database pools can be configured with options <code>min</code> and <code>max</code> for number of connections to be maintained, so no overload will happen and keep warm connection for
faster responses. Even for DynamoDB which uses HTTPS this can be configured without hitting provisioned limits which will return an error but
put extra requests into the waiting queue and execute once some requests finished.</p>
<p>Example:</p>
<pre><code>    db-pgsql-pool-max = 100
    db-dynamodb-pool-max = 100</code></pre><p>Also, to spread functionality between different databases it is possible to assign some tables to the specific pools using <code>db-X-pool-tables</code> parameters
thus redirecting the requests to one or another databases depending on the table, this for example can be useful when using fast but expensive
database like DynamoDB for real-time requests and slower SQL database running on some slow instance for rare requests, reports or statistics processing.</p>
<p>Example, run the backend with default PostgreSQL database but keep all config parametrs in the DynamoDB table for availability:</p>
<pre><code>    db-pool = pgsql
    db-dynamodb-pool = default
    db-dynamodb-pool-tables = bk_config</code></pre></li>
</ul>
<p> The following databases are supported with the basic db API methods:
 Sqlite, PostgreSQL, MySQL, DynamoDB, MongoDB, Elasticsearch, Cassandra, Redis, LMDB, LevelDB, Riak, CouchDB</p>
<p> All these drivers fully support all methods and operations, some natively, some with emulation in the user space except Redis driver cannot perform sorting
 due to using Hash items for records, sorting can be done in memory but with pagination it is not possible so this part must be mentioned specifically. But the rest of the
 operations on top of Redis are fully supported which makes it a good candidate to use for in-memory tables like sessions with the same database API, later moving to
 other database will not require any application code changes.</p>
<p> Multiple connections of the same type can be opened, just add -n suffix to all database config parameters where n is a number, referer to such pools in the code as <code>poolN</code>.</p>
<p> Example:</p>
<pre><code>      db-sqlite1-pool = billing
      db-sqlite1-pool-max = 10
      db-sqlite1-pool-options-path = /data/db
      db-sqlite1-pool-options-journal_mode = OFF

      in the Javascript:

      db.select(&quot;bills&quot;, { status: &quot;ok&quot; }, { pool: &quot;sqlite1&quot; }, lib.log)</code></pre>
<ul>
<li><p><code>Database tables</code></p>
<pre><code>      // Configuration store, same parameters as in the commandline or config file, can be placed in separate config groups
      // to be used by different backends or workers
      bk_config: {
          name: { primary: 1 },            // name of the parameter
          type: { primary: 1 },            // config type or tag
          value: {},                       // the value
          status: { value: &quot;ok&quot; },         // ok - availaible
          ttl: { type: &quot;int&quot; },            // refresh interval in seconds since last read
          mtime: { type: &quot;now&quot; }
      },

      // General purpose properties, can be used to store arbitrary values
      bk_property: {
          name: { primary: 1 },
          value: {},
          count: { type: &quot;counter&quot; },      // general purpose counter value
          ttl: { type: &quot;int&quot; },            // time to live, seconds since last update
          mtime: { type: &quot;now&quot; }
      },</code></pre></li>
</ul>

<ul>
<li><p><code>createPool:(opts)</code></p>
<p> None database driver</p>
</li>
</ul>

<ul>
<li><p><code>db.init(options, callback)</code></p>
<p> Initialize all database pools. the options may containt the following properties:</p>
<ul>
<li>localTables - only initialize default, local and config db pools, other pools are ignored, if not given
 global value is used. Currently it can be set globally from the app only, no config parameter.</li>
<li>createTables - if true then create new tables or upgrade tables with new columns</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>db.initConfig(options, callback)</code></p>
<p> Load configuration from the config database, must be configured with <code>db-config-type</code> pointing to the database pool where bk_config table contains
configuration parameters.</p>
<p>The priority of the paramaters is fixed and goes from the most broad to the most specific, most specific always wins, this allows
for very flexible configuration policies defined by the app or place where instances running and separated by the run mode.</p>
<p>The following list of properties will be queried from the config database and the sorting order is very important, the last values
will override values received for the earlier properties, for example, if two properties defined in the <code>bk_config</code> table with the
types <code>myapp</code> and <code>prod-myapp</code>, then the last value will be used only.</p>
<p>The major elements are the following:</p>
<ul>
<li>the run mode specified in the command line <code>-run-mode: production</code></li>
<li>the application name from the package.json: <code>myapp</code></li>
<li>the process role: <code>-worker</code></li>
<li>the instance tag, AWS name tag or other name: <code>-nat</code></li>
</ul>
<p>The modifiers which are appended to each major attributes:</p>
<ul>
<li>the network where the instance is running, first 2 octets from the current IP address: <code>-192.168</code></li>
<li>the region where the instance is running, AWS region or other: <code>us-east-1</code></li>
<li>the zone where the instance is running, AWS availability zone or other: <code>-us-east-1a</code></li>
</ul>
<p>The top level list is the following:</p>
<ul>
<li>runMode</li>
<li>appName</li>
<li>runMode-appName</li>
<li>runMode-role</li>
<li>runMode-tag</li>
<li>runMode-appName-role</li>
<li>runMode-appName-tag</li>
</ul>
<p>All modifiers are appended for every item in the list like <code>runMode-network</code>, <code>runMode-appName-tag-region</code>,...</p>
<p>The options takes the following properties:</p>
<ul>
<li>force - if true then force to refresh and reopen all db pools</li>
<li>delta - if true then pull only records updated since the last config pull using the max mtime from received records.</li>
<li>table - a table where to read the config parameters, default is bk_config</li>
</ul>
<p><strong>NOTE: The config parameters from the DB always take precedence even over config.local.</strong></p>
<p>On return, the callback second argument will receive all parameters received form the database as a list: -name value ...</p>
</li>
</ul>

<ul>
<li><p><code>db.getConfig(options, callback)</code></p>
<p> Return all config records for the given instance, the result will be sorted most relevant at the top</p>
</li>
</ul>

<ul>
<li><p><code>db.refreshConfig(options, callback)</code></p>
<p> Refresh parameters which are configured with a TTL</p>
</li>
</ul>

<ul>
<li><p><code>db.createTables(options, callback)</code></p>
<p> Create or upgrade the tables for the given pool</p>
</li>
</ul>

<ul>
<li><p><code>db.dropTables(tables, options, callback)</code></p>
<p> Delete all specified tables from the specific pool or all active pools if <code>options.pool</code> is empty, <code>tables</code> can be a list of tables or an
object with table definitions</p>
</li>
</ul>

<ul>
<li><p><code>db.query(req, options, callback)</code></p>
<p> Execute query using native database driver, the query is passed directly to the driver.</p>
<ul>
<li>req - can be a string or an object with the following properties:<ul>
<li>text - SQL statement or other query in the format of the native driver, can be a list of statements</li>
<li>values - parameter values for SQL bindings or other driver specific data</li>
<li>op - operations to be performed, used by non-SQL drivers</li>
<li>obj - actual object with data for non-SQL drivers</li>
<li>table - table name for the operation</li>
</ul>
</li>
<li>options may have the following properties:<ul>
<li>pool - name of the database pool where to execute this query.
The difference with the high level functions that take a table name as their firt argument, this function must use pool
explicitely if it is different from the default. Other functions can resolve
the pool by table name if some tables are assigned to any specific pool by configuration parameters <code>db-pool-tables</code>.</li>
<li>unique - perform sorting the result and eliminate any duplicate rows by the column name specified in the <code>unique</code> property</li>
<li>filterrows - function to filter rows not to be included in the result, returns a new result set, args are: function(req, rows)</li>
<li>processrows - function to process rows in the result, returns a new result, args are: function(req, rows), this result will be put in cache
if requested so this may be used for preparing cached results, it must return an array</li>
<li>processasync - function to process result rows via async callback, return a new result in the callback, the function is: function(req, rows, callback),
the callback is function(err, rows)</li>
<li>quiet - report errors in debug level</li>
<li>first - return the first row from the result</li>
<li>logger_db - log results at the end with this level or debug by default</li>
<li>logger_error - a log level to report about the errors, default is &#39;error&#39;, if an object it can specify different log levels by err.code, * is default level for not matched codes</li>
<li>ignore_error - clear errors occured as it never happen, do not report in the log, if an array then only matched codes will be cleared</li>
<li>noprocessrows - if true then skip post processing result rows, return the data as is, this will result in returning combined columns as it is</li>
<li>noconvertrows - if true skip converting the data from the database format into Javascript data types, it uses column definitions
for the table to convert values returned from the db into the the format defined by the column</li>
<li>cached - if true perform cache invalidation for the operations that resulted in modification of the table record(s)</li>
<li>total - if true then it is supposed to return only one record with property <code>count</code>, skip all post processing and convertion</li>
<li>info_obj - to return the record just processed in the info object as <code>obj</code> property, it will include all generated and updated columns</li>
<li>result_obj - to return the query record as result including all post processing and new generated columns, this is not what <code>returning</code> property does, it only
returns the query record with new columns from memory</li>
</ul>
</li>
<li>callback(err, rows, info) where<ul>
<li>info is an object with information about the last query: inserted_oid,affected_rows,next_token,consumed_capacity</li>
<li>rows is always returned as a list, even in case of error it is an empty list</li>
</ul>
</li>
</ul>
<p>Example with SQL driver</p>
<pre><code>    db.query({ text: &quot;SELECT a.id,c.type FROM bk_account a,bk_connection c WHERE a.id=c.id and a.id=?&quot;, values: [&#39;123&#39;] }, { pool: &#39;pgsql&#39; }, function(err, rows, info) {
    });</code></pre></li>
</ul>

<ul>
<li><p><code>db.get(table, query, options, callback)</code></p>
<p> Retrieve one record from the database by primary key, returns found record or null if not found
Options can use the following special properties:</p>
<ul>
<li>select - a list of columns or expressions to return, default is to return all columns</li>
<li>ops - operators to use for comparison for properties, see <code>db.select</code></li>
<li>cached - if specified it runs getCached version</li>
<li>nocache - disable caching even if configured for the table</li>
</ul>
<p>NOTE: On return the <code>info.cached</code> will be set to 1 if the record was retrieved from cache or was put in the cache.</p>
<p>Example</p>
<pre><code>    db.get(&quot;bk_account&quot;, { id: &#39;12345&#39; }, function(err, row) {
       if (row) console.log(row.name);
    });</code></pre></li>
</ul>

<ul>
<li><p><code>db.select(table, query, options, callback)</code></p>
<p> Select objects from the database that match supplied conditions.</p>
<ul>
<li>query - can be an object with properties for the condition, all matching records will be returned,
also can be a list where each item is an object with primary key condition. Only records specified in the list must be returned.</li>
<li>options can use the following special properties:<ul>
<li>ops - operators to use for comparison for properties, an object with column name and operator. The following operators are available:
 <code>&gt;, gt, &lt;, lt, =, !=, &lt;&gt;, &gt;=, ge, &lt;=, le, in, between, regexp, iregexp, begins_with, like%, ilike%</code></li>
<li>opsMap - operator mapping between supplied operators and actual operators supported by the db</li>
<li>typesMap - type mapping between supplied and actual column types, an object</li>
<li>select - a list of columns or expressions to return or all columns if not specified, only existing columns will be returned</li>
<li>select_all - a list of columns or expressions to return, passed as is to the underlying driver</li>
<li>start - start records with this primary key, this is the next_token passed by the previous query</li>
<li>count - how many records to return</li>
<li>first - a convenient option to return the first record from the result or null (similar to <code>db.get</code> method)</li>
<li>join - how to join condition expressions, default is AND</li>
<li>sort - sort by this column. if null then no sorting must be done at all, records will be returned in the order they are kept in the DB.
 <em>NOTE: For DynamoDB this may affect the results if columns requsted are not projected in the index, with sort
  <code>select</code> property might be used to get all required properties. For Elasticsearch if sort is null then scrolling scan will be used,
  if no <code>timeout</code> or <code>scroll</code> are given the default is 1m.</em></li>
<li>sort_timeout - for pagination how long to keep internal state in millisecons, depends on the DB, for example for Elasticsearch it corresponds
 to the scroll param and defaults to 60000 (1m)</li>
<li>desc - if sorting, do in descending order</li>
<li>page - starting page number for pagination, uses count to find actual record to start, for SQL databases mostly</li>
<li>unique - specified the column name to be used in determining unique records, if for some reasons there are multiple records in the location
 table for the same id only one instance will be returned</li>
<li>cacheKey - exlicit key for caching, return from the cche or from the DB and then cache it with this key, works the same as <code>get</code></li>
<li>cacheKeyName - a name of one of the cache keys to use, it must be defined by a <code>db-cache-keys-table-name</code> parameter</li>
<li>nocache - do not use cache even if cche key is given</li>
</ul>
</li>
</ul>
<p>On return, the callback can check third argument which is an object with some predefined properties along with driver specific state returned by the query:</p>
<ul>
<li>affected_rows - how many records this operation affected, for add/put/update</li>
<li>inserted_oid - last created auto generated id</li>
<li>next_token - next primary key or offset for pagination by passing it as .start property in the options, if null it means there are no more pages availabe for this query</li>
</ul>
<p>Example: get by primary key, refer above for default table definitions</p>
<pre><code>  db.select(&quot;bk_message&quot;, { id: &#39;123&#39; }, { count: 2 }, function(err, rows) {

  });</code></pre><p>Example: get all icons with type greater or equal to 2</p>
<pre><code>  db.select(&quot;bk_icon&quot;, { id: &#39;123&#39;, type: &#39;2&#39; }, { select: &#39;id,type&#39;, ops: { type: &#39;ge&#39; } }, function(err, rows) {

  });</code></pre><p>Example: get unread msgs sorted by time, recent first</p>
<pre><code>  db.select(&quot;bk_message&quot;, { id: &#39;123&#39;, status: &#39;N:&#39; }, { sort: &quot;status&quot;, desc: 1, ops: { status: &quot;begins_with&quot; } }, function(err, rows) {

  });</code></pre><p>Example: allow all accounts icons to be visible</p>
<pre><code>  db.select(&quot;bk_account&quot;, {}, function(err, rows) {
      rows.forEach(function(row) {
          row.acl_allow = &#39;auth&#39;;
          db.update(&quot;bk_icon&quot;, row);
      });
  });</code></pre><p>Example: scan accounts with custom filter, not by primary key: all females</p>
<pre><code>  db.select(&quot;bk_account&quot;, { gender: &#39;f&#39; }, function(err, rows) {

  });</code></pre><p>Example: select connections using primary key and other filter columns: all likes for the last day</p>
<pre><code>  db.select(&quot;bk_connection&quot;, { id: &#39;123&#39;, type: &#39;like&#39;, mtime: Date.now()-86400000 }, { ops: { type: &quot;begins_with&quot;, mtime: &quot;gt&quot; } }, function(err, rows) {

  });</code></pre></li>
</ul>

<ul>
<li><p><code>db.search(table, query, options, callback)</code></p>
<p> Perform full text search on the given table, the database implementation may ignore table name completely
in case of global text index.</p>
<p>Query in general is a text string with the format that is supported by the underlying driver,
the db module <em>DOES NOT PARSE</em> the query at all if the driver supports full text search, otherwise it behaves like <code>select</code>.</p>
<p>Options make take the same properties as in the <code>select</code> method.</p>
<p>A special query property <code>q</code> may be used for generic search in all fields.</p>
<p>Without full text search support in the driver this may return nothing or an error.</p>
<p>Example</p>
<pre><code>      db.search(&quot;bk_account&quot;, { type: &quot;admin&quot;, q: &quot;john*&quot; }, { pool: &quot;elasticsearch&quot; }, lib.log);
      db.search(&quot;bk_account&quot;, &quot;john*&quot;, { pool: &quot;elasticsearch&quot; }, lib.log);</code></pre></li>
</ul>

<ul>
<li><p><code>db.add(table, obj, options, callback)</code></p>
<p> Insert new object into the database</p>
<ul>
<li>obj - an JavaScript object with properties for the record, primary key properties must be supplied</li>
<li>options may contain the following properties:<ul>
<li>no_columns - do not check for actual columns defined in the pool tables and add all properties from the obj, only will work for NoSQL dbs,
by default all properties in the obj not described in the table definition for the given table will be ignored.</li>
<li>skip_columns - ignore properties by name listed in the this array</li>
<li>mtime - if set, mtime column will be added automatically with the current timestamp, if mtime is a
string then it is used as a name of the column instead of default mtime name</li>
<li>skip_null - if set, all null values will be skipped, otherwise will be written into the DB as NULLs</li>
</ul>
</li>
</ul>
<p>On return the <code>obj</code> will contain all new columns generated before adding the record</p>
<p>Note: SQL, DynamoDB, MongoDB, Redis drivers are fully atomic but other drivers may be subject to race conditions</p>
<p>Example</p>
<pre><code> db.add(&quot;bk_account&quot;, { id: &#39;123&#39;, name: &#39;test&#39;, gender: &#39;m&#39; }, function(err, rows, info) {
 });</code></pre></li>
</ul>

<ul>
<li><p><code>db.incr(table, obj, options, callback)</code></p>
<p> Counter operation, increase or decrease column values, similar to update but all specified columns except primary
key will be incremented, use negative value to decrease the value.</p>
<p>If no <code>options.updateOps</code> object specified or no &#39;incr&#39; operations are provided then
all columns with type &#39;counter&#39; will be used for the action <code>incr</code></p>
<p><em>Note: The record must exist already for SQL databases, for DynamoDB and Cassandra a new record will be created
if does not exist yet.</em> To disable upsert pass <code>noupsert</code> in the options.</p>
<p>Example</p>
<pre><code> db.incr(&quot;bk_counter&quot;, { id: &#39;123&#39;, like0: 1, invite0: 1 }, function(err, rows, info) {
 });</code></pre></li>
</ul>

<ul>
<li><p><code>db.put(table, obj, options, callback)</code></p>
<p> Add/update an object in the database, if object already exists it will be replaced with all new properties from the obj</p>
<ul>
<li>obj - an object with record properties, primary key properties must be specified</li>
<li>options - same properties as for <code>db.add</code> method</li>
</ul>
<p>Example</p>
<pre><code> db.put(&quot;bk_account&quot;, { id: &#39;123&#39;, name: &#39;test&#39;, gender: &#39;m&#39; }, function(err, rows, info) {
 });</code></pre></li>
</ul>

<ul>
<li><p><code>db.update(table, obj, options, callback)</code></p>
<p> Update existing object in the database.</p>
<ul>
<li>obj - is an actual record to be updated, primary key properties must be specified</li>
<li>options - same properties as for <code>db.add</code> method with the following additional properties:<ul>
<li>ops - object for comparison operators for primary key, default is equal operator</li>
<li>opsMap - operator mapping into supported by the database</li>
<li>typesMap - type mapping for properties to be used in the condition</li>
<li>aliases - an object to map column aliases in the query in case the same column is used ultiple times</li>
<li>expected - an object with the condition for the update, it is used in addition to the primary keys condition from the <code>obj</code>,
 a property named $or or $and will be treated as a sub-expression if it is an object.</li>
<li>expectedJoin - how to join expected expressions: OR, AND, default is AND</li>
<li>upsert - create a new record if it does not exist</li>
<li>updateOps - an object with column names and operations to be performed on the named column<ul>
<li>incr - increment by given value</li>
<li>set - to update as it is, for reseting counters forexample</li>
<li>concat - concatenate given value, for strings if the database supports it</li>
<li>append - append to the list of values, only for lists if the database supports it</li>
<li>prepend - insert at the beginning of the list, depends on the database</li>
<li>not_exists - only update if not exists or null</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>Note: not all database drivers support atomic update with conditions, all drivers for SQL, DynamoDB, MongoDB, Redis fully atomic, but other drivers
perform get before put and so subject to race conditions</p>
<p>Example</p>
<pre><code>    db.update(&quot;bk_account&quot;, { id: &#39;123&#39;, gender: &#39;m&#39; }, function(err, rows, info) {
        console.log(&#39;updated:&#39;, info.affected_rows);
    });

    db.update(&quot;bk_account&quot;, { id: &#39;123&#39;, gender: &#39;m&#39;, prefix: &#39;Mr&#39; }, { pool: pgsql&#39; }, function(err, rows, info) {
        console.log(&#39;updated:&#39;, info.affected_rows);
    });

    db.update(&quot;bk_account&quot;, { id: &#39;123&#39;, gender: &#39;m&#39;, prefix: &#39;Mr&#39; }, { expected: { gender: &quot;f&quot; } }, function(err, rows, info) {
        console.log(&#39;updated:&#39;, info.affected_rows);
    });

    db.update(&quot;bk_account&quot;, { id: &#39;123&#39;, gender: &#39;m&#39;, prefix: &#39;Mr&#39; }, { expected: { &quot;$or&quot;: { gender: &quot;f&quot;, g1: null }, aliases: { g1: &quot;gender&quot; } }, function(err, rows, info) {
        console.log(&#39;updated:&#39;, info.affected_rows);
    });</code></pre></li>
</ul>

<ul>
<li><p><code>db.updateAll(table, query, obj, options, callback)</code></p>
<p> Update all records that match given condition in the <code>query</code>, one by one, the input is the same as for <code>db.select</code> and every record
returned will be updated using <code>db.update</code> call by the primary key, so make sure options.select include the primary key for every row found by the select.</p>
<p>All properties from the <code>obj</code> will be set in every matched record.</p>
<p>The callback will receive on completion the err and all rows found and updated. This is mostly for non-SQL databases and for very large range it may take a long time
to finish due to sequential update every record one by one.
Special properties that can be in the options for this call:</p>
<ul>
<li>updateOptions - options to be passed to the db.update if needed, this is useful so select and update options will not be mixed up</li>
<li>updateCollect - if true return all updated rows in the callback otherwise just the number of updated rows</li>
<li>factorCapacity - write capacity factor for update operations, default is 0.25</li>
<li>op - by default it uses db.update but the <code>op</code> can be set to <code>put</code> or <code>add</code></li>
<li>updateProcess - a function callback that will be called for each row before updating it, this is for some transformations of the record properties
 in case of complex columns that may contain concatenated values as in the case of using DynamoDB. The callback will be called
 as <code>options.updateProcess(row, options)</code>. If it returns non-empty value the update will stop and return it as the error.</li>
<li>updateFilter - a function that must return something to the callback in order to skip the current record. <code>options.updateFilter(row, options, (skip) =&gt; {})</code></li>
</ul>
<p>If no <code>options.select</code> is specified only the primary keys will be returned or collected</p>
<p>Example, update birthday format if not null</p>
<pre><code>    db.updateAll(&quot;bk_account&quot;,
                { birthday: 1 },
                { mtime: Date.now() },
                { ops: { birthday: &quot;not null&quot; },
                  updateProcess: function(r, o) {
                     r.birthday = lib.strftime(new Date(r.birthday, &quot;%Y-%m-D&quot;));
                  },
                  updateFilter: function(r, o, cb) {
                     cb(r.status == &#39;ok&#39;);
                  } },
    function(err, count) {
       console.log(count, &quot;rows updated&quot;);
    });</code></pre></li>
</ul>

<ul>
<li><p><code>db.del(table, obj, options, callback)</code></p>
<p> Delete an object in the database, no error if the object does not exist</p>
<ul>
<li>obj - an object with primary key properties only, other properties will be ignored</li>
<li>options - same properties as for <code>db.update</code> method</li>
</ul>
<p>Example</p>
<pre><code> db.del(&quot;bk_account&quot;, { id: &#39;123&#39; }, function(err, rows, info) {
     console.log(&#39;updated:&#39;, info.affected_rows);
 });</code></pre></li>
</ul>

<ul>
<li><p><code>db.delAll(table, query, options, callback)</code></p>
<p> Delete all records that match given condition, one by one, the input is the same as for <code>db.select</code> and every record
returned will be deleted using <code>db.del</code> call. The callback will receive on completion the err and all rows found and deleted.
Special properties that can be in the options for this call:</p>
<ul>
<li>ops - query operations to retrieve records to be deleted</li>
<li>count - how many matching records to delete</li>
<li>delOptions - options to be passed to the db.del if needed, this is useful so select and del options will not be mixed up</li>
<li>delCollect - if true return all deleted rows in the callback, oherwise just the number of rows deleted</li>
<li>factorCapacity - write capqcity factor for delete operations, default is 0.35</li>
<li>concurrency - how many delete requests to execute at the same time by using lib.forEachLimit.</li>
<li>ignore_error - continue deleting records even after an error</li>
<li>delProcess - a function callback that will be called for each row before deleting it, this is for some transformations of the record properties
in case of complex columns that may contain concatenated values as in the case of using DynamoDB. The callback will be called
as <code>options.delProcess(row, options)</code>. If it returns non-empty value the scan will stop and return it as the error.</li>
<li>delFilter - a function that must return something to the callback in order to skip the current record. <code>options.delFilter(row, options, (skip) =&gt; {})</code></li>
</ul>
<p>If no <code>options.select</code> is specified only the primary keys will be returned or collected</p>
<p>If <code>db-skip-drop</code> matches the table name and there is no query provided it will exit with error</p>
</li>
</ul>

<ul>
<li><p><code>db.replace(table, obj, options, callback)</code></p>
<p> Add/update the object, check existence by the primary key. This is not equivalent of REPLACE INTO, it does <code>db.get</code>
to check if the object exists in the database and performs <code>db.add</code> or <code>db.update</code> depending on the existence.</p>
<ul>
<li>obj is a JavaScript object with properties that correspond to the table columns</li>
<li>options define additional flags that may<ul>
<li>check_mtime - defines a column name to be used for checking modification time and skip if not modified, must be a date value</li>
<li>check_data - verify every value in the given object with actual value in the database and skip update if the record is the same,
if it is an array then check only specified columns</li>
</ul>
</li>
</ul>
<p>Example: updates record 123 only if gender is not &#39;m&#39; or adds new record</p>
<pre><code>    db.replace(&quot;bk_account&quot;, { id: &#39;123&#39;, gender: &#39;m&#39; }, { check_data: true });</code></pre><p>Example: updates record 123 only if mtime of the record is less or equal yesterday</p>
<pre><code>    db.replace(&quot;bk_account&quot;, { id: &#39;123&#39;, mtime: Date.now() - 86400000 }, { check_mtime: &#39;mtime&#39; });</code></pre></li>
</ul>

<ul>
<li><p><code>db.list(table, query, options, callback)</code></p>
<p> Convenient helper to retrieve all records by primary key, the obj must be a list with key property or a string with list of primary key column
Example</p>
<pre><code>db.list(&quot;bk_account&quot;, [&quot;id1&quot;, &quot;id2&quot;], function(err, rows) { console.log(err, rows) });
db.list(&quot;bk_account&quot;, &quot;id1,id2&quot;, function(err, rows) { console.log(err, rows) });</code></pre></li>
</ul>

<ul>
<li><p><code>db.batch(list, options, callback)</code></p>
<p> Perform a batch of operations at the same time, all operations for the same table will be run
together one by one but different tables will be updated in parallel.</p>
<ul>
<li><code>list</code> an array of objects to put/delete from the database in the format:<ul>
<li>op - is one of add, incr, put, update, del</li>
<li>table - which table to use</li>
<li>obj - an object with data</li>
<li>options - params for the operation, optional</li>
</ul>
</li>
<li>options can have the follwoing:<ul>
<li>concurrency - number of how many operations to run at the same time, 1 means sequential</li>
<li>no_errors - will stop on first error, because operations will be run in parallel some operations still may be performed</li>
<li>factorCapacity - a capacity factor to apply to the write capacity if present, by default it is used write capacity at 100%</li>
</ul>
</li>
</ul>
<p>On return the second arg to the callback is a list of records with errors, same input record with added property <code>errstatus</code> and <code>errmsg</code></p>
<p>Example:</p>
<pre><code>    var ops = [ { op: &quot;add&quot;, table: &quot;bk_counter&quot;, obj: { id:1, like:1 } },
                { op: &quot;add&quot;, table: &quot;bk_auth&quot;, obj: { login: &quot;test&quot;, id:1, name:&quot;test&quot; }]
    db.batch(ops, { factorCapacity: 0.5 }, lib.log);</code></pre></li>
</ul>

<ul>
<li><p><code>db.bulk(list, options, callback)</code></p>
<p> Bulk operations, it will be noop if the driver does not support it.
The input format is the same as for the <code>db.batch</code> method.</p>
<p>On return the second arg to the callback is a list of records with errors, same input record with added property <code>errstatus</code> and <code>errmsg</code></p>
<p>NOTE: DynamoDB only supports add, put, del</p>
<p>Example</p>
<pre><code>    var ops = [ { op: &quot;add&quot;, table: &quot;bk_counter&quot;, obj: { id:1, like:1 } },
                { op: &quot;del&quot;, table: &quot;bk_auth&quot;, obj: { login: &quot;test1&quot; } },
                { op: &quot;incr&quot;, table: &quot;bk_counter&quot;, obj: { id:2, like:1 } },
                { op: &quot;add&quot;, table: &quot;bk_auth&quot;, obj: { login: &quot;test2&quot;, id:2, name:&quot;test2&quot; } }]
    db.bulk(ops, { pool: &quot;elasticsearch&quot; }, lib.log);</code></pre></li>
</ul>

<ul>
<li><p><code>db.scan(table, query, options, rowCallback, endCallback)</code></p>
<p> Convenient helper for scanning a table for some processing, rows are retrieved in batches and passed to the callback until there are no more
records matching given criteria. The obj is the same as passed to the <code>db.select</code> method which defined a condition which records to get.
The rowCallback must be present and is called for every row or batch retrieved and second parameter which is the function to be called
once the processing is complete. At the end, the callback will be called just with 1 argument, err, this indicates end of scan operation.
Basically, db.scan is the same as db.select but can be used to retrieve large number of records in batches and allows async processing of such records.
To hint a driver that scanning is in progress the <code>options.scanning</code> will be set to true.</p>
<p>Parameters:</p>
<ul>
<li>table - table to scan</li>
<li>query - an object with query conditions, same as in <code>db.select</code></li>
<li>options - same as in <code>db.select</code>, with the following additions:<ul>
<li>count - size of every batch, default is 100</li>
<li>limit - total number of records to scan</li>
<li>start - the primary key to start the scan from</li>
<li>search - use search instead of select, for ElasticSearch,...</li>
<li>batch - if true rowCallback will be called with all rows from the batch, not every row individually, batch size is defined by the count property</li>
<li>concurrency - how many rows to process at the same time, if not given process sequentially</li>
<li>noscan - if 1 no scan will be performed if no prmary keys are specified</li>
<li>fullscan - if 1 force to scan full table without using any primary key conditons, use all query properties for all records (DynamoDB)</li>
<li>useCapacity - triggers to use specific capacity, default is <code>read</code></li>
<li>factorCapacity - a factor to apply for the read capacity limit and triggers the capacity check usage, default is <code>0.9</code></li>
<li>tableCapacity - use a different table for capacity throttling instead of the <code>table</code>, useful for cases when the row callback performs
 writes into that other table and capacity is different</li>
<li>capacity - a full capacity object to pass to select calls</li>
</ul>
</li>
<li>rowCallback - process records when called like this `callback(rows, next)</li>
<li>endCallback - end of scan when called like this: `callback(err)</li>
</ul>
<p>Example:</p>
<pre><code>    db.scan(&quot;bk_account&quot;, {}, { count: 10, pool: &quot;dynamodb&quot; }, function(row, next) {
        // Copy all accounts from one db into another
        db.add(&quot;bk_account&quot;, row, { pool: &quot;pgsql&quot; }, next);
    }, function(err) { });</code></pre></li>
</ul>

<ul>
<li><p><code>db.copy(table, query, options, callback)</code></p>
<p> Copy records from one table to another between different DB pools or regions</p>
<p>Parameters:</p>
<ul>
<li>table - name of the table to copy</li>
<li>query - a query condition for the table</li>
<li>options properties<ul>
<li>sort - index to use for query</li>
<li>minCapacity - capacity minimum for read/writes, it will override actual DB capacity</li>
<li>factorCapacity - factor the actual capacity for reads/writes</li>
<li>stopOnError - stop the copy on first DB error, otherwise ignore errors</li>
<li>region - other region where to copy</li>
<li>pool - other DB pool</li>
<li>preprocess - a function(table, row, options) to be called before the update, if it returns true the record will be skipped</li>
<li>posprocess - a function(table, row, options, next) to be called after the record is copied, for recursive or joined cases</li>
<li>reget - if set the actual record will read using db.get, for cases when db.scan returns only partial record as in DynamoDB cases with indexes</li>
<li>incremental - if set, try to read the latest record in the other table and continue from there, uses <code>sort</code> index in desc order</li>
<li>batch - a number of records to copy at once using the bulk operation</li>
</ul>
</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>db.join(table, rows, options, callback)</code></p>
<p> Join the given list of records with the records from other table by primary key.
The properties from the joined table will be merged with the original rows preserving the existing properties</p>
<ul>
<li>options.keys defines custom primary key to use instead of table&#39;s primary key</li>
<li>options.keysMap - an object that defines which property should be used for a key in the given rows, this is
for cases when actual primary keys in the table are different from the rows properties.</li>
<li>options.columnsMap - save properties with a different name using this mapping object</li>
<li>options.existing is 1 then return only joined records.</li>
<li>options.override - joined table properties will replace the original table existing properties</li>
<li>options.attach - specifies a property name which will be used to attach joined record to the original record, no merging will occur, for
non-existing records an empty object will be attached</li>
<li>options.incr can be a list of property names that need to be summed up with each other, not overriden</li>
<li>options.nomerge - do not merge lists, just return new rows as is</li>
</ul>
<p>A special case when table is empty <code>db.join</code> just returns same rows to the callback, this is
for convenience of doing joins on some conditions and trigger it by setting the table name or skip the join completely.</p>
<p>Example:</p>
<pre><code>    db.join(&quot;bk_account&quot;, [{id:&quot;123&quot;,key1:1},{id:&quot;234&quot;,key1:2}], lib.log)
    db.join(&quot;bk_account&quot;, [{aid:&quot;123&quot;,key1:1},{aid:&quot;234&quot;,key1:2}], { keysMap: { id: &quot;aid&quot; }}, lib.log)
    db.join(&quot;bk_account&quot;, [{id:&quot;123&quot;,state:&quot;NY&quot;},{id:&quot;234&quot;,state:&quot;VA&quot;}], { columnsMap: { state: &quot;astate&quot; }}, lib.log)</code></pre></li>
</ul>

<ul>
<li><p><code>db.create(table, columns, options, callback)</code></p>
<p> Create a table using column definitions represented as a list of objects. Each column definition can
contain the following properties:</p>
<ul>
<li><code>name</code> - column name</li>
<li><code>type</code> - column type: int, bigint, real, string, now, counter or other supported type</li>
<li><code>primary</code> - column is part of the primary key</li>
<li><code>unique</code> - column is part of an unique key</li>
<li><code>index</code> - column is part of an index</li>
<li><code>value</code> - default value for the column</li>
<li><code>len</code> - column length</li>
<li><code>pub</code> - columns is public, <em>this is very important property because it allows anybody to see it when used in the default API functions, i.e. anybody with valid
credentials can retrieve all public columns from all other tables, and if one of the other tables is account table this may expose some personal infoamtion,
so by default only a few columns are marked as public in the bk_account table</em></li>
<li><code>secure</code> - an opposite for the pub property, if defined this property should never be returned to the client by the API handlers</li>
<li><code>admin</code> - if defined this property can only be updated an admin account</li>
<li><code>admins</code> - if defined this property can be visible by the owner and an admin if result is returned by <code>api.sendJSON</code></li>
<li><code>hidden</code> - completely ignored by all update operations but could be used by the public columns cleaning procedure, if it is computed and not stored in the db
it can contain pub property to be returned to the client</li>
<li><code>readonly</code> - only add/put operations will use the value, incr/update will not affect the value</li>
<li><code>writeonly</code> - only incr/update can change this value, add/put will ignore it</li>
<li><code>noresult</code> - delete this property from the result, mostly for joined artificial columns which used for indexes only</li>
<li><code>random</code> - add a random number between 0 and this value, useful with type: &quot;now&quot;</li>
<li>`lower&#39; - make string value lowercase</li>
<li>`upper&#39; - make string value uppercase</li>
<li><code>list</code> - splits the column value into an array, optional <code>separator</code> property can be used, default separator is <code>,|</code></li>
<li><code>autoexpire</code> - for supported databases, mark the column to be auto expired, the value must be time in the future as Epoch seconds</li>
<li><code>autoincr</code> - for counter tables, mark the column to be auto-incremented by the connection API if the connection type has the same name as the column name</li>
<li><code>join</code> - a list with property names that must be joined together before performing a db operation, it will use the given record to produce new property,
 this will work both ways, to the db and when reading a record from the db it will split joined property and assign individual
 properties the value from the joined value.</li>
<li><code>join_ops</code> - an array with operations for which perform columns join only, if not specified it applies for all operations,
 allowed values: add, put, incr, update, del, get, select</li>
</ul>
<p><em>Some properties may be defined multiple times with number suffixes like: unique1, unique2, index1, index2 to create more than one index for the table, same
properties define a composite key in the order of definition or sorted by the property value, for example: <code>{ a: { index:2 }, b: { index:1 } }</code> will create index (b,a)
because of the <code>index:</code> property value being not the same. If all index properties are set to 1 then a composite index will use the order of the properties.</em></p>
<p><em>Special column types</em>:</p>
<ul>
<li><code>uuid</code> - autogenerate the column value with UUID, optional <code>prefix</code> property will be prepended, <code>{ type: &quot;uuid&quot;, prefix: &quot;u_&quot; }</code></li>
<li><code>now</code> - defines a column to be automatically filled with the current timestamp, <code>{ type: &quot;now&quot; }</code></li>
<li><code>counter</code> - defines a columns that will be automatically incremented by the <code>db.incr</code> command, on creation it is set with 0</li>
<li><code>uid</code> - defines a columns to be automatically filled with the current user id, this assumes that account object is passed in the options from the API level</li>
<li><code>uname</code> - defines a columns to be automatically filled with the current user name, this assumes that account object is passed in the options from the API level</li>
</ul>
<p>NOTE: Index creation is not required and all index properties can be omitted, it can be done more effectively using native tools for any specific database,
this format is for simple and common use cases without using any other tools but it does not cover all possible variations for every database. But all indexes and
primary keys created outside of the backend application will be detected properly by <code>db.cacheColumns</code> and by each pool <code>cacheIndexes</code> methods.</p>
<p>Each database pool also can support native options that are passed directly to the driver in the options, these properties are
defined in the object with the same name as the db driver, all properties are combined, for example to define provisioned throughput for the DynamoDB index:</p>
<pre><code>    db.create(&quot;test_table&quot;, { id: { primary: 1, type: &quot;int&quot;, index: 1, dynamodb: { readCapacity: 50, writeCapacity: 50 } },
                              type: { primary: 1, pub: 1, projections: 1 },
                              name: { index: 1, pub: 1 } }
                            });</code></pre><p>Create DynamoDB table with global secondary index, the first index property if not the same as primary key hash defines global index, if it is the same then local,
or if the second key column contains <code>global</code> property then it is a global index as well, below we create global secondary index on property &#39;name&#39; only,
in the example above it was local secondary index for id and name. Also a local secondary index is created on <code>id,title</code>.</p>
<p>DynamoDB projection is defined by a <code>projections</code> property, can be a number/boolean or an array with index numbers:</p>
<pre><code>    db.create(&quot;test_table&quot;, { id: { primary: 1, type: &quot;int&quot;, index1: 1 },
                              type: { primary: 1, projections: [0] },
                              name: { index: 1, projections: 1 },
                              title: { index1: 1, projections: [1] } },
                              descr: { index: 1, projections: [0, 1] },
                            });</code></pre><p>When using real DynamoDB creating a table may take some time, for such cases if <code>options.waitTimeout</code> is not specified it defaults to 1min,
so the callback is called as soon as the table is active or after the timeout whichever comes first.</p>
</li>
</ul>
<p> Pass MongoDB options directly:
        db.create(&quot;test_table&quot;, { id: { primary: 1, type: &quot;int&quot;, mongodb: { w: 1, capped: true, max: 100, size: 100 } },
                                  type: { primary: 1, pub: 1 },
                                  name: { index: 1, pub: 1, mongodb: { sparse: true, min: 2, max: 5 } }
                                });</p>

<ul>
<li><p><code>db.upgrade(table, columns, options, callback)</code></p>
<p> Upgrade a table with missing columns from the definition list, if after the upgrade new columns must be re-read from the database
then <code>info.affected_rows</code> must be non zero.</p>
</li>
</ul>

<ul>
<li><p><code>db.drop(table, options, callback)</code></p>
<p> Drop a table</p>
</li>
</ul>

<ul>
<li><p><code>db.convertError(pool, table, op, err, options)</code></p>
<p> Convert native database error in some generic human readable string</p>
</li>
</ul>

<ul>
<li><p><code>db.describeTables(tables, callback)</code></p>
<p> Define new tables or extend/customize existing tables. Table definitions are used with every database operation,
on startup, the backend read all existing table columns from the database and cache them in the memory but some properties
like public columns are only specific to the backend so to mark such columns the table with such properties must be described
using this method. Only columns with changed properties need to be specified, other columns will be left as it is.</p>
<p>Example</p>
<pre><code>    db.describeTables({ bk_account: { name: { pub: 1 } },
                        test: { id: { primary: 1, type: &quot;int&quot; },
                                name: { pub: 1, index: 1 } });</code></pre></li>
</ul>

<ul>
<li><p><code>db.refreshColumns(options, callback)</code></p>
<p> Refresh columns for all polls which need it</p>
</li>
</ul>

<ul>
<li><p><code>db.cacheColumns(options, callback)</code></p>
<p> Reload all columns into the cache for the pool, options can be a pool name or an object like <code>{ pool: name }</code>.
if <code>tables</code> property is an arary it asks to refresh only specified tables if that is possible.</p>
</li>
</ul>

<ul>
<li><p><code>db.prepare(op, table, obj, options)</code></p>
<p> Prepare for execution for the given operation: add, del, put, update,...
Returns prepared object to be passed to the driver&#39;s .query method. This method is a part of the driver
helpers and is not used directly in the applications.</p>
</li>
</ul>

<ul>
<li><p><code>db.prepareRow(pool, req)</code></p>
<p> Preprocess an object for a given operation, convert types, assign defaults...</p>
</li>
</ul>

<ul>
<li><p><code>db.prepareForUpdate(pool, req)</code></p>
<p> Keep only columns from the table definition if we have it
Go over all properties in the object and makes sure the types of the values correspond to the column definition types,
this is for those databases which are very sensitive on the types like DynamoDB.</p>
</li>
</ul>

<ul>
<li><p><code>db.joinColumn(req, obj, name, col, orig)</code></p>
<p> Join several columns to produce a combined property if configured, given a column description and an object record
it replaces the column value with joined value if needed. Empty properties will be still joined as empty strings.
It always uses the original value even if one of the properties has been joined already.</p>
<p>Checks for <code>join</code> and <code>join_ops</code> properties in the column definition.</p>
<p>The <code>options.skip_join</code> can be used to restrict joins, it is a list with columns that should not be joined</p>
<p>The <code>col.join_pools</code> can be an array with pool names which are allowed to do the join, other pools will skip joining this column.</p>
<p>The <code>col.nojoin_pools</code> can be an array with pool names which are not allowed to do the join, other pools will skip joining this column</p>
<p>The <code>options.join_strict</code> can be used to perform join only if all columns in the list are not empty, so the join
is for all columns or none</p>
<p>The <code>options.join_hash</code> can be used to store a hash of the joined column to reduce the sspace and make the result value easier to use</p>
</li>
</ul>

<ul>
<li><p><code>db.unjoinColumns(rows, name, col, options)</code></p>
<p> Split joined columns for all rows</p>
</li>
</ul>

<ul>
<li><p><code>db.convertRows(pool, req, rows, options)</code></p>
<p> Convert rows returned by the database into the Javascript format or into the format
defined by the table columns.
The following special properties in the column definition chnage the format:</p>
<ul>
<li><p>type = json - if a column type is json and the value is a string returned will be converted into a Javascript object</p>
</li>
<li><p>dflt property is defined for a json type and record does not have a value it will be set to specified default value</p>
</li>
<li><p>list - split the value into an array, optional .separator property can be specified</p>
</li>
<li><p>unjoin - a true value or a list of names, it produces new properties by splitting the value by a separator and assigning pieces to
  separate properties using names from the list, this is the opposite of the <code>join</code> property and is used separately if
  splitting is required, if joined properties already in the record then no need to split it. If not a list
  the names are used form the join property.</p>
<p>  Example:</p>
<pre><code>      db.describeTables([ { user: { id: {}, name: {}, pair: { join: [&quot;left&quot;,&quot;right&quot;], unjoin: 1 } } ]);

      db.put(&quot;test&quot;, { id: &quot;1&quot;, type: &quot;user&quot;, name: &quot;Test&quot;, left: &quot;123&quot;, right: &quot;000&quot; })
      db.select(&quot;test&quot;, {}, lib.log)</code></pre></li>
</ul>
</li>
</ul>

<ul>
<li><p><code>db.setProcessColumns(callback)</code></p>
<p> Add a callback to be called after each cache columns event, it will be called for each pool separately.
The callback to be called may take options argument and it is called in the context of the pool.</p>
<p>The primary goal for this hook is to allow management of the existing tables which are not own by the
backendjs application. For such tables, because we have not created them, we need to define column properties
after the fact and to keep column definitions in the app for such cases is not realistic. This callback will
allow to handle such situations and can be used to set necessary propeties to the table columns.</p>
<p>Example, a few public columns, allow an admin to see all the columns</p>
<pre><code>   db.setProcessColumns(function() {
       var cols = db.getColumns(&quot;users&quot;, { pool: this.name });
       for (var p in  cols) {
           if ([&quot;id&quot;,&quot;name&quot;].indexOf(p) &gt; -1) cols[p].pub = 1; else cols[p].admin = 1;
       }
   })</code></pre></li>
</ul>

<ul>
<li><p><code>db.getProcessRows(type, table, options)</code></p>
<p> Returns a list of hooks to be used for processing rows for the given table</p>
</li>
</ul>

<ul>
<li><p><code>db.runProcessRows(type, table, req, rows)</code></p>
<p> Run registered pre- or post- process callbacks.</p>
<ul>
<li><code>type</code> is one of the <code>pre</code> or &#39;post`</li>
<li><code>table</code> - the table to run the hooks for, usually the same as req.table but can be &#39;*&#39; for global hooks</li>
<li><code>req</code> is the original db request object with the following required properties: <code>op, table, obj, options, info</code>,</li>
<li><code>rows</code> is the result rows for post callbacks and the same request object for pre callbacks.</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>db.setProcessRow(type, table, options, callback)</code></p>
<p> Assign a processRow callback for a table, this callback will be called for every row on every result being retrieved from the
specified table thus providing an opportunity to customize the result.</p>
<p>type defines at what time the callback will be called:</p>
<ul>
<li><code>pre</code> - making a request to the db on the query record</li>
<li><code>post</code> - after the request finished to be called on the result rows</li>
</ul>
<p>All assigned callback to this table will be called in the order of the assignment.</p>
<p>The callback accepts 2 arguments: function(req, row)
 where:</p>
<ul>
<li><code>req</code> - the original request for a db operation with required<ul>
<li><code>op</code> - current db operation, like add, put, ....</li>
<li><code>table</code> -  current table being updated</li>
<li><code>obj</code> - the record with data</li>
<li><code>pool</code> - current request db pool name</li>
<li><code>options</code> - current request db options</li>
<li><code>info</code> - an object returned with special properties like affected_rows, next_token, only passed to the <code>post</code> callbacks</li>
</ul>
</li>
<li><code>row</code> - a row from the result</li>
</ul>
<p>When producing complex properties by combining other properties it needs to be synchronized using both pre and post
callbacks to keep the record consistent.</p>
<p><strong>For queries returning rows, if the callback returns true for a row it will be filtered out and not included in the final result set.</strong></p>
</li>
</ul>
<p>  Example</p>
<pre><code>  db.setProcessRow(&quot;post&quot;, &quot;bk_account&quot;, function(req, row) {
      if (row.birthday) row.age = Math.floor((Date.now() - lib.toDate(row.birthday))/(86400000*365));
  });

  db.setProcessRow(&quot;post&quot;, &quot;bk_icon&quot;, function(req, row) {
      if (row.type == &quot;private&quot; &amp;&amp; row.id != req.options.account.id) return true;
  });</code></pre>
<ul>
<li><p><code>db.existsPool(name)</code></p>
<p> Returns true if a pool exists</p>
</li>
</ul>

<ul>
<li><p><code>db.existsTable(table, options)</code></p>
<p> Returns true if a table exists</p>
</li>
</ul>

<ul>
<li><p><code>db.getPool(table, options)</code></p>
<p> Return database pool by table name or default pool, options can be a pool name or an object with { pool: name } to return
the pool by given name. This call always returns a valid pool object, in case no requested pool found, it returns
the default pool, in case of invalid pool name it returns <code>none</code> pool.
A special pool <code>none</code> always returns empty result and no errors.
Pools specific tables will not be returned even if a different pool name is provided, the none pool will be returned.</p>
</li>
</ul>

<ul>
<li><p><code>db.getPoolTables(name, options)</code></p>
<p> Return all tables know to the given pool, returned tables are in the object with
column information merged from cached columns from the database with description columns
given by the application. If <code>options.names</code> is 1 then return just table names as a list.</p>
</li>
</ul>

<ul>
<li><p><code>db.getPools()</code></p>
<p> Return a list of all active database pools, returns list of objects with name: and type: properties</p>
</li>
</ul>

<ul>
<li><p><code>db.getColumns(table, options)</code></p>
<p> Return columns for a table or null, columns is an object with column names and objects for definition.</p>
</li>
</ul>

<ul>
<li><p><code>db.getColumn(table, name, options)</code></p>
<p> Return the column definition for a table, for non-existent columns it returns an empty object</p>
</li>
</ul>

<ul>
<li><p><code>db.getCapacity(table, options)</code></p>
<p> Return an object with capacity property which is the max write capacity for the table, for DynamoDB only.
By default it checks <code>writeCapacity</code> property of all table columns and picks the max.</p>
<p>The options can specify the capacity explicitely:</p>
<ul>
<li>useCapacity - what to use for capacity rating, can be <code>write</code>, <code>read</code> or a number with max capacity to use</li>
<li>factorCapacity - a number between 0 and 1 to multiple the rate capacity</li>
<li>rateCapacity - if set it will be used for rate capacity limit</li>
<li>maxCapacity - if set it will be used as the max burst capacity limit</li>
<li>minCapacity - if set it will be used as the minimum threshold</li>
<li>intervalCapacity - default is 1000 ms</li>
<li>sort - an index to use for capacity, for systems like DynamoDB which has different capacity for
global indexes, it makes sense for indexed reads or partial updates where only global index is affected and not the whole record</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>db.checkCapacity(cap, consumed, callback)</code></p>
<p> Check if number of requests exceeds the capacity per second, delay if necessary, for DynamoDB only but can be used for pacing
requests with any database or can be used generically. The <code>cap</code> must be initialized with <code>db.getCapacity</code> call.</p>
</li>
</ul>

<ul>
<li><p><code>db.getSelectedColumns(table, options)</code></p>
<p> Return list of selected or allowed only columns, empty list if no <code>options.select</code> is specified or it is equal to <code>*</code>. By default only allowed or existing
columns will be returned, to pass the list as is to the driver just use <code>options.select_all</code> instead.</p>
</li>
</ul>

<ul>
<li><p><code>db.getFilteredColumns(table, filter, options)</code></p>
<p> Return table columns filtered by a property filter, only return columns that contain(or not)
any property from the filter list. If the filter is an object then values must match, null means if exists.
! at the beginning of the name means empty or does not exist.</p>
<p>Example:</p>
<pre><code>db.getFilteredColumns(&quot;bk_account&quot;, &quot;pub&quot;)
db.getFilteredColumns(&quot;bk_account&quot;, &quot;!internal&quot;)
db.getFilteredColumns(&quot;bk_account&quot;, { pub: null, index: 2 })
db.getFilteredColumns(&quot;bk_account&quot;, { type: &quot;now&quot; }, { list: 1 })</code></pre></li>
</ul>

<ul>
<li><p><code>db.checkCustomColumn(req, name)</code></p>
<p> Returns type for a global cusgtom column if exists otherwise null, all resolved
columns will be saved in <code>req.allow</code> for further reference as name: type.
For request specific custom columns pass <code>options.custom_columns</code> object in the format: [ RegExp, type, ...]</p>
</li>
</ul>

<ul>
<li><p><code>db.skipColumn(name, val, options, columns)</code></p>
<p> Verify column against common options for inclusion/exclusion into the operation, returns 1 if the column must be skipped</p>
<ul>
<li>to enable all properties to be saved in the record without column definition set <code>options.no_columns=1</code></li>
<li>to skip all null values set <code>options.skip_null=1</code></li>
<li>to skip by value set <code>options.skip_values</code> to a regexp</li>
<li>to skip specific columns define <code>options.skip_columns=[&quot;a&quot;,&quot;b&quot;]</code></li>
<li>to restrict to specific columns only define <code>options.allow_columns=[&quot;a&quot;,&quot;b&quot;]</code></li>
<li>to skip columns based on matched properties define <code>options.skip_matched=[{ admin: 1 }, { owner: 1 }]</code></li>
<li>to allow only columns based on matched properties define <code>options.allow_matched={ admin: null }</code></li>
<li>to restrict to specific DB pools only define <code>options.allow_pools=[&quot;sqlite&quot;,&quot;mysql&quot;]</code></li>
<li>to skip specific DB pools define <code>options.skip_pools=[&quot;sqlite&quot;,&quot;mysql&quot;]</code></li>
<li>to restrict to specific DB pools for this columns only define <code>name: { allow_pools: [&quot;sqlite&quot;,&quot;mysql&quot;] }</code></li>
</ul>
</li>
</ul>

<ul>
<li><p><code>db.filterRows(obj, rows, options)</code></p>
<p> Given object with data and list of keys perform comparison in memory for all rows, return only rows that match all keys. This method is used
by custom filters in <code>db.select</code> by the drivers which cannot perform comparisons with non-indexes columns like DynamoDb, Cassandra.
The rows that satisfy primary key conditions are returned and then called this function to eliminate the records that do not satisfy non-indexed column conditions.</p>
<p>Options support the following propertis:</p>
<ul>
<li>keys - list of columns to check, these may or may not be the primary keys, any columns to be compared</li>
<li>cols - an object with columns definition</li>
<li>ops - operations for columns</li>
<li>typesMap - types for the columns if different from the actual Javascript type</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>db.getKeys(table, options)</code></p>
<p> Return primary keys for a table or empty array, if <code>allkeys</code> is given in the options then return
a list of all properties involed in primary keys including joined columns</p>
</li>
</ul>

<ul>
<li><p><code>db.getIndexes(table, options)</code></p>
<p> Return indexes for a table or empty object, each item in the object is an array with index columns</p>
</li>
</ul>

<ul>
<li><p><code>db.getIndexColumns(table, options)</code></p>
<p> Return columns for all indexes as alist</p>
</li>
</ul>

<ul>
<li><p><code>db.getIndexForKeys(table, keys, options)</code></p>
<p> Return an index name that can be used for searching for the given keys, the index match is performed on the index columns
from the left to right  and stop on the first missing key, for example for given keys { id: &quot;1&quot;, name: &quot;2&quot;, state: &quot;VA&quot; }
the index [&quot;id&quot;, &quot;state&quot;] or [&quot;id&quot;,&quot;name&quot;] will be returned but the index [&quot;id&quot;,&quot;city&quot;,&quot;state&quot;] will not.</p>
</li>
</ul>

<ul>
<li><p><code>db.getSearchKeys(table, options)</code></p>
<p> Return keys for the table search, if options.keys provided and not empty it will be used otherwise
table&#39;s primary keys will be returned. This is a wrapper that makes sure that valid keys are used and
deals with input errors like empty keys list to be consistent between different databases.
This function always returns an Array even if it is empty.</p>
</li>
</ul>

<ul>
<li><p><code>db.getSearchQuery(table, obj, options)</code></p>
<p> Return query object based on the keys specified in the options or primary keys for the table, only search properties
will be returned in the query object</p>
</li>
</ul>

<ul>
<li><p><code>db.getQueryForKeys(keys, obj, options)</code></p>
<p> Returns an object based on the list of keys, basically returns a subset of properties.
<code>options.keysMap</code> defines an object to map record properties with the actual names to be returned.</p>
</li>
</ul>

<ul>
<li><p><code>db.getBindValue(table, options, val, info)</code></p>
<p> Return possibly converted value to be used for inserting/updating values in the database,
is used for SQL parameterized statements</p>
<p>Parameters:</p>
<ul>
<li>options - standard pool parameters with pool: property for specific pool</li>
<li>val - the JavaScript value to convert into bind parameter</li>
<li>info - column definition for the value from the cached columns</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>db.getColumnValue(table, options, val, info)</code></p>
<p> Return transformed value for the column value returned by the database, same parameters as for getBindValue</p>
</li>
</ul>

<ul>
<li><p><code>db.getCached(op, table, query, options, callback)</code></p>
<p> Retrieve cached result or put a record into the cache prefixed with table:key[:key...]
Options accept the same parameters as for the usual get action but it is very important that all the options
be the same for every call, especially <code>select</code> parameters which tells which columns to retrieve and cache.
Additional options:</p>
<ul>
<li>prefix - prefix to be used for the key instead of table name</li>
</ul>
<p>Example:</p>
<pre><code>db.getCached(&quot;get&quot;, &quot;bk_account&quot;, { id: req.query.id }, { select: &quot;latitude,longitude&quot; }, function(err, row) {
    var distance = lib.geoDistance(req.query.latitude, req.query.longitude, row.latitude, row.longitudde);
});</code></pre></li>
</ul>

<ul>
<li><p><code>db.getCache(table, query, options, callback)</code></p>
<p> Retrieve an object from the cache by key, sets <code>cacheKey</code> in the options for later use</p>
</li>
</ul>

<ul>
<li><p><code>db.putCache(table, query, options)</code></p>
<p> Store a record in the cache</p>
</li>
</ul>

<ul>
<li><p><code>db.delCache(table, query, options)</code></p>
<p> Notify or clear cached record, this is called after del/update operation to clear cached version by primary keys</p>
</li>
</ul>

<ul>
<li><p><code>db.getCacheKey(table, query, options)</code></p>
<p> Returns concatenated values for the primary keys, this is used for caching records by primary key</p>
</li>
</ul>

<ul>
<li><p><code>db.getCacheOptions(table, options)</code></p>
<p> Setup common cache properties</p>
</li>
</ul>

<ul>
<li><p><code>db.getCache2Ttl(table, options)</code></p>
<p> Return TTL for level 2 cache, negative means use js cache</p>
</li>
</ul>

<ul>
<li><p><code>db.getCacheKeys(table, query, name)</code></p>
<p> Return a list of global cache keys, if a name is given only returns the matching key</p>
</li>
</ul>

<ul>
<li><p><code>db.delCacheKeys(req, result, options, callback)</code></p>
<p> Delete all global cache keys for the table record</p>
</li>
</ul>

<ul>
<li><p><code>db.parsePoolOptions(val, options)</code></p>
<p> Callback to be called by the config parser to preprocess a config parameter for generic options, the options
is an object with all the info about the parameter, all values can be modified to change the behavior, if not used
the config parameter will be a string value assigned in the pool&#39;s <code>configOptions</code>.</p>
</li>
</ul>

<ul>
<li><p><code>db.Pool(options)</code></p>
<p> Create a new database pool with default methods and properties</p>
<ul>
<li>options - an object with default pool properties<ul>
<li>type - pool type, this is the db driver name</li>
<li>pool or name - pool name</li>
<li>watchfile - file path to be watched for changes, all clients will be destroyed gracefully</li>
<li>min - min number of open database connections</li>
<li>max - max number of open database connections, all attempts to run more will result in clients waiting for the next available db connection, if set to 0 no<pre><code>  pooling will be enabled and will result in the unlimited connections, this is default for DynamoDB</code></pre></li>
<li>max_queue - how many db requests can be in the waiting queue, above that all requests will be denied instead of putting in the waiting queue</li>
</ul>
</li>
</ul>
<p>The db methods cover most use cases but in case native driver needs to be used this is how to get the client and use it with its native API,
it is required to call <code>pool.release</code> at the end to return the connection back to the connection pool.</p>
<pre><code>    var pool = db.getPool(&quot;&quot;, { pool: &quot;mongodb&quot; });
    pool.get(function(err, client) {
        var collection = client.collection(&#39;bk_account&#39;);
        collection.findOne({ id: &#39;123&#39; }, function() {
            pool.release(client);
        });
    });</code></pre></li>
</ul>

<ul>
<li><p><code>db.Pool.prototype.configure(options)</code></p>
<p> Reconfigure properties, only subset of properties are allowed here so it is safe to apply all of them directly,
this is called during realtime config update</p>
</li>
</ul>

<ul>
<li><p><code>db.Pool.prototype.open(callback)</code></p>
<p> Open a connection to the database, default is to return an empty object as a client</p>
</li>
</ul>

<ul>
<li><p><code>db.Pool.prototype.close(client, callback)</code></p>
<p> Close a connection, default is do nothing</p>
</li>
</ul>

<ul>
<li><p><code>db.Pool.prototype.query(client, req, options, callback)</code></p>
<p> Query the database, always return an array as a result (i.e. the second argument for the callback)</p>
</li>
</ul>

<ul>
<li><p><code>db.Pool.prototype.cacheColumns(options, callback)</code></p>
<p> Cache columns for all tables</p>
</li>
</ul>

<ul>
<li><p><code>db.Pool.prototype.cacheIndexes(options, callback)</code></p>
<p> Cache indexes for all tables</p>
</li>
</ul>

<ul>
<li><p><code>db.Pool.prototype.nextToken(client, req, rows)</code></p>
<p> Return next token from the client object</p>
</li>
</ul>

<ul>
<li><p><code>db.Pool.prototype.prepareOptions(options)</code></p>
<p> Update the options with pool config parameters if needed, the options is from the request</p>
</li>
</ul>

<ul>
<li><p><code>db.Pool.prototype.prepareRow(req)</code></p>
<p> Default prepareRow is to perform pool specific actions for prepared row before passing it to the op specific columns filterting</p>
</li>
</ul>

<ul>
<li><p><code>db.Pool.prototype.prepare(req)</code></p>
<p> Default prepare is to return all parameters in an object</p>
</li>
</ul>

<ul>
<li><p><code>db.Pool.prototype.bindValue(value, info, options)</code></p>
<p> Return the value to be used in binding, mostly for SQL drivers, on input value and col info are passed, this callback
may convert the value into something different depending on the DB driver requirements, like timestamp as string into milliseconds</p>
</li>
</ul>

<ul>
<li><p><code>db.Pool.prototype.convertError(table, op, err, options)</code></p>
<p> Converts native DB driver error into other human readable format</p>
</li>
</ul>

<ul>
<li><p><code>db.Pool.prototype.processColumns(pool)</code></p>
<p> that is called after this pool cached columms from the database, it is called sychnroniously inside the <code>db.cacheColumns</code> method.</p>
</li>
</ul>

<ul>
<li><p><code>db.Pool.prototype.resolveTable(op, table, obj, options)</code></p>
<p> Return possible different table at the time of the query, it is called by the <code>db.prepare</code> method
and if exist it must return the same or new table name for the given query parameters.</p>
</li>
</ul>

<ul>
<li><p><code>db.SqlPool(options)</code></p>
<p> Create a database pool for SQL like databases</p>
<ul>
<li>options - an object defining the pool, the following properties define the pool:<ul>
<li>pool - pool name/type, if not specified the SQLite is used</li>
<li>max - max number of clients to be allocated in the pool</li>
<li>idle - after how many milliseconds an idle client will be destroyed</li>
</ul>
</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>db.SqlPool.prototype.cacheColumns(options, callback)</code></p>
<p> Call column caching callback with our pool name</p>
</li>
</ul>

<ul>
<li><p><code>db.SqlPool.prototype.prepare(req)</code></p>
<p> Prepare for execution, return an object with formatted or transformed SQL query for the database driver of this pool</p>
</li>
</ul>

<ul>
<li><p><code>db.SqlPool.prototype.query(client, req, options, callback)</code></p>
<p> Execute a query or if req.text is an Array then run all queries in sequence</p>
</li>
</ul>

<ul>
<li><p><code>db.SqlPool.prototype.nextToken(client, req, rows)</code></p>
<p> Support for pagination, for SQL this is the OFFSET for the next request</p>
</li>
</ul>

<h2 id="module-db_dynamodb">Module: DB_DYNAMODB</h2>
<ul>
<li><p><code>Pool.prototype.convertError(table, op, err, options)</code></p>
<p> Convert into recognizable error codes</p>
</li>
</ul>

<ul>
<li><p><code>Pool.prototype.query(client, req, options, callback)</code></p>
<p> Simulate query as in SQL driver but performing AWS call, text will be a table name and values will be request options</p>
</li>
</ul>

<h2 id="module-db_elasticsearch">Module: DB_ELASTICSEARCH</h2>
<ul>
<li><p><code>pool</code></p>
<p> Create a database pool that works with ElasticSearch server, only the hostname and port will be used, by default each table
is stored in its own index.</p>
<p>To define shards and replicas per index:</p>
<ul>
<li><code>-db-elasticsearch-pool-options-shards-INDEX_NAME=NUM</code></li>
<li><code>-db-elasticsearch-pool-options-replicas-INDEX_NAME=NUM</code></li>
</ul>
<p>To support multiple seed nodes a parameter <code>-db-elasticsearch-pool-options-servers=1.1.1.1,2.2.2.2</code> can be specified, if the primary node
fails it will switch to other configured nodes. To control the switch retries and timeout there are options:</p>
<ul>
<li><code>-db-elasticsearch-pool-options-retry-count=3</code></li>
<li><code>-db-elasticsearch-pool-options-retry-timeout=250</code></li>
</ul>
<p>On successful connect to any node the driver retrieves full list of nodes in the cluster and switches to a random node, this happens
every <code>discovery-interval</code> in milliseconds, default is 1h, it can be specified as <code>-db-elasticserch-pool-options-discovery-interval=300000</code></p>
</li>
</ul>

<h2 id="module-db_sql">Module: DB_SQL</h2>
<ul>
<li><p><code>db.sqlQuery(pool, client, req, options, callback)</code></p>
<p> Execute one or more SQL statements</p>
</li>
</ul>

<ul>
<li><p><code>db.sqlCacheColumns(pool, options, callback)</code></p>
<p> Cache columns using the information_schema</p>
</li>
</ul>

<ul>
<li><p><code>db.sqlPrepare(pool, req)</code></p>
<p> Prepare SQL statement for the given operation</p>
</li>
</ul>

<ul>
<li><p><code>db.sqlQuote(val)</code></p>
<p> Quote value to be used in SQL expressions</p>
</li>
</ul>

<ul>
<li><p><code>db.sqlValue(value, options)</code></p>
<p> Return properly quoted value to be used directly in SQL expressions, format according to the type</p>
</li>
</ul>

<ul>
<li><p><code>db.sqlValueIn(list, type)</code></p>
<p> Return list in format to be used with SQL IN ()</p>
</li>
</ul>

<ul>
<li><p><code>db.sqlExpr(pool, name, value, options)</code></p>
<p> Build SQL expressions for the column and value
options may contain the following properties:</p>
<ul>
<li>op - SQL operator, default is =</li>
<li>type - can be data, string, number, float, expr, default is string</li>
<li>value - default value to use if passed value is null or empty</li>
<li>min, max - are used for numeric values for validation of ranges</li>
<li>expr - for op=expr, contains sprintf-like formatted expression to be used as is with all &#39;%s&#39; substituted with actual value</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>db.sqlTime(d)</code></p>
<p> Return time formatted for SQL usage as ISO, if no date specified returns current time</p>
</li>
</ul>

<ul>
<li><p><code>db.sqlLimit(pool, req)</code></p>
<p> Build SQL orderby/limit/offset conditions, config can define defaults for sorting and paging</p>
</li>
</ul>

<ul>
<li><p><code>db.sqlWhere(pool, req, query, keys, join)</code></p>
<p> Build SQL where condition from the keys and object values, returns SQL statement to be used in WHERE</p>
<ul>
<li>query - properties for the condition, in case of an array the primary keys for IN condition will be used only,
a property named $or or $and will be treated as a sub-expression if it is an object.</li>
<li>keys - a list of columns to use for the condition, other properties will be ignored</li>
<li>options may contains the following properties:<ul>
<li>pool - pool to be used for driver specific functions</li>
<li>ops - an object for comparison operators for primary key, default is equal operator</li>
<li>aliases - an object with column aliases, for cases when more than one time the same column mut be used</li>
</ul>
</li>
<li>join - how to join all expressions, default is AND</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>db.sqlCreate(pool, req)</code></p>
<p> Create SQL table using table definition</p>
<ul>
<li>table - name of the table to create</li>
<li>obj - object with properties as column names and each property value is an object:<ul>
<li>name - column name</li>
<li>type - type of the column, default is TEXT, options: int, real or other supported types</li>
<li>value - default value for the column</li>
<li>primary - part of the primary key</li>
<li>index - indexed column, part of the composite index</li>
<li>unique - must be combined with index property to specify unique composite index</li>
<li>len - max length of the column</li>
<li>notnull - true if should be NOT NULL</li>
<li>auto - true for AUTO_INCREMENT column</li>
</ul>
</li>
<li>options may contains:<ul>
<li>upgrade - perform alter table instead of create</li>
<li>typesMap - type mapping, convert lowercase type into other type supported by any specific database</li>
<li>noDefaults - ignore default value if not supported (Cassandra)</li>
<li>noNulls - NOT NULL restriction is not supported (Cassandra)</li>
<li>noMultiSQL - return as a list, the driver does not support multiple SQL commands</li>
<li>noLengths - ignore column length for columns (Cassandra)</li>
<li>noIfExists - do not support IF EXISTS on table or indexes</li>
<li>noCompositeIndex - does not support composite indexes (Cassandra)</li>
<li>noAuto - no support for auto increment columns</li>
<li>skipNull - object with operations which dont support null(empty) values (DynamoDB cannot add/put empty/null values)</li>
</ul>
</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>db.sqlUpgrade(pool, req)</code></p>
<p> Create ALTER TABLE ADD COLUMN statements for missing columns</p>
</li>
</ul>

<ul>
<li><p><code>db.sqlDrop(pool, req)</code></p>
<p> Create SQL DROP TABLE statement</p>
</li>
</ul>

<ul>
<li><p><code>db.sqlGet(pool, req)</code></p>
<p> Get one object from the database,
options may define the following properties:</p>
<ul>
<li>select is list of columns or expressions to return</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>db.sqlSelect(pool, req)</code></p>
<p> Select object from the database,
options may define the following properties:</p>
<ul>
<li>keys is a list of columns for the condition</li>
<li>select is list of columns or expressions to return</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>db.sqlInsert(pool, req)</code></p>
<p> Build SQL insert statement</p>
</li>
</ul>

<ul>
<li><p><code>db.sqlUpdate(pool, req)</code></p>
<p> Build SQL statement for update</p>
</li>
</ul>

<ul>
<li><p><code>db.sqlDelete(pool, req)</code></p>
<p> Build SQL statement for delete</p>
</li>
</ul>

<h2 id="module-db_sqlite">Module: DB_SQLITE</h2>
<h2 id="module-http_get">Module: HTTP_GET</h2>
<h2 id="module-index">Module: INDEX</h2>
<h2 id="module-ipc">Module: IPC</h2>
<ul>
<li><p><code>Ipc.prototype.handleServerMessages(worker, msg)</code></p>
<p> To be used in messages processing that came from the clients or other way</p>
</li>
</ul>

<ul>
<li><p><code>Ipc.prototype.sendReplPort(role, worker)</code></p>
<p> Send REPL port to a worker if needed</p>
</li>
</ul>

<ul>
<li><p><code>Ipc.prototype.newMsg(op, msg, options)</code></p>
<p> Returns an IPC message object, <code>msg</code> must be an object if given. To support JSON text messages
it can be represented as a string of JSON array: [op,msg]</p>
</li>
</ul>

<ul>
<li><p><code>Ipc.prototype.emitMsg(op, msg, options)</code></p>
<p> Wrapper around EventEmitter <code>emit</code> call to send unified IPC messages in the same format</p>
</li>
</ul>

<ul>
<li><p><code>Ipc.prototype.sendMsg(op, msg, options, callback)</code></p>
<p> Send a message to the master process via IPC messages, callback is used for commands that return value back</p>
<ul>
<li>the <code>timeout</code> property can be used to specify a timeout for how long to wait the reply, if not given the default is used</li>
<li>the rest of the properties are optional and depend on the operation.</li>
</ul>
<p>If called inside the server, it process the message directly, reply is passed in the callback if given.</p>
<p>Examples:</p>
<pre><code>  ipc.sendMsg(&quot;op1&quot;, { data: &quot;data&quot; }, { timeout: 100 })
  ipc.sendMsg(&quot;op1&quot;, { name: &quot;name&quot;, value: &quot;data&quot; }, function(data) { console.log(data); })
  ipc.sendMsg(&quot;op1&quot;, { 1: 1, 2: 2 }, { timeout: 100 })
  ipc.sendMsg(&quot;op1&quot;, { 1: 1, 2: 2 }, function(data) { console.log(data); })
  ipc.newMsg(JSON.stringify([ &quot;op1&quot;, { name: &quot;test&quot; } ]))</code></pre></li>
</ul>

<ul>
<li><p><code>Ipc.prototype.broadcast(channel, msg, options, callback)</code></p>
<p> Send a message to a channel, this is high level routine that uses the corresponding queue, it uses eventually
ipc.publish.</p>
</li>
</ul>

<ul>
<li><p><code>Ipc.prototype.initServer()</code></p>
<p> This function is called by a master server process to setup IPC channels and support for cache and messaging</p>
</li>
</ul>

<ul>
<li><p><code>Ipc.prototype.initWorker()</code></p>
<p> This function is called by a worker process to setup IPC channels and support for cache and messaging</p>
</li>
</ul>

<ul>
<li><p><code>Ipc.prototype.createClient(url, options)</code></p>
<p> Return a new client for the given host or null if not supported</p>
</li>
</ul>

<ul>
<li><p><code>Ipc.prototype.getClient(prefix, options)</code></p>
<p> Return a cache or queue client by name if specified in the options or use default client for the prefix which always exists,
supported prefixes are: queue, cache</p>
</li>
</ul>

<ul>
<li><p><code>Ipc.prototype.getQueue(name)</code></p>
<p> Return queue object by name, if name is wrong the default queue us returned</p>
</li>
</ul>

<ul>
<li><p><code>Ipc.prototype.getCache(name)</code></p>
<p> Retirn cache object by name, if name is wrong the default cache is returned</p>
</li>
</ul>

<ul>
<li><p><code>Ipc.prototype.initClients(prefix)</code></p>
<p> Initialize a client for cache or queue purposes, previous client will be closed.</p>
</li>
</ul>

<ul>
<li><p><code>Ipc.prototype.stats(options, callback)</code></p>
<p> Returns the cache statistics, the format depends on the cache type used</p>
</li>
</ul>

<ul>
<li><p><code>Ipc.prototype.clear(pattern, options, callback)</code></p>
<p> Clear all or only items that match the given pattern</p>
</li>
</ul>

<ul>
<li><p><code>Ipc.prototype.get(key, options, callback)</code></p>
<p> Retrieve an item from the cache by key.</p>
<ul>
<li><code>options.set</code> is given and no value exists in the cache it will be set as the initial value, still
nothing will be returned to signify that a new value assigned.</li>
<li><code>options.mapName</code> defines a map from which the key will be retrieved if the cache supports maps, to get the whole map
the key must be set to *</li>
</ul>
<p>If the <code>key</code> is an array then it returns an array with values for each key, for non existent keys an empty
string will be returned. For maps only if the <code>key</code> is * it will return the whole object, otherise only value(s)
are returned.</p>
</li>
</ul>
<p> Example</p>
<pre><code>ipc.get([&quot;my:key1&quot;, &quot;my:key2&quot;], function(err, data) { console.log(data) });
ipc.get(&quot;my:key&quot;, function(err, data) { console.log(data) });
ipc.get(&quot;my:counter&quot;, { set: 10 }, function(err, data) { console.log(data) });
ipc.get(&quot;*&quot;, { mapName: &quot;my:map&quot; }, function(err, data) { console.log(data) });
ipc.get(&quot;key1&quot;, { mapName: &quot;my:map&quot; }, function(err, data) { console.log(data) });
ipc.get([&quot;key1&quot;, &quot;key2&quot;], { mapName: &quot;my:map&quot; }, function(err, data) { console.log(data) });</code></pre>
<ul>
<li><p><code>Ipc.prototype.del(key, options, callback)</code></p>
<p> Delete an item by key(s),  if <code>key</code> is an array all keys will be deleted at once atomically if supported</p>
<ul>
<li><code>options.mapName</code> defines a map from which the counter will be deleted if the cache supports maps, to delete the whole map
the key must be set to *</li>
</ul>
<p>Example:</p>
<pre><code>  ipc.del(&quot;my:key&quot;)
  ipc.del(&quot;key1&quot;, { mapName: &quot;my:map&quot; })
  ipc.del(&quot;*&quot;, { mapName: &quot;my:map&quot; })</code></pre></li>
</ul>

<ul>
<li><p><code>Ipc.prototype.put(key, val, options, callback)</code></p>
<p> Replace or put a new item in the cache.</p>
<ul>
<li><code>options.ttl</code> can be passed in milliseconds if the driver supports it</li>
<li><code>options.mapName</code> defines a map where the counter will be stored if the cache supports maps, to store the whole map in one
operation the <code>key</code> must be set to * and the <code>val</code> must be an object</li>
<li><code>options.setmax</code> if not empty tell the driver to set this new number only if there is no existing
value or it is less that the new number, only works for numeric values</li>
</ul>
<p>Example:</p>
<pre><code> ipc.put(&quot;my:key&quot;, 2)
 ipc.put(&quot;my:key&quot;, 1, { setmax: 1 })
 ipc.put(&quot;*&quot;, { key1: 1, key2: 2 }, { mapName: &quot;my:map&quot; })</code></pre></li>
</ul>

<ul>
<li><p><code>Ipc.prototype.incr(key, val, options, callback)</code></p>
<p> Increase/decrease a counter in the cache by <code>val</code>, non existent items are treated as 0, if a callback is given an
error and the new value will be returned.</p>
<ul>
<li><code>options.ttl</code> in milliseconds can be used if the driver supports it</li>
<li><code>options.mapName</code> defines a map where the counter will be stored if the cache supports maps</li>
</ul>
<p>Example:</p>
<pre><code>  ipc.incr(&quot;my:key&quot;, 1)
  ipc.incr(&quot;key1&quot;, 1, { mapName: &quot;my:map&quot; })</code></pre></li>
</ul>

<ul>
<li><p><code>Ipc.prototype.subscribe(channel, options, callback)</code></p>
<p> Subscribe to a publish server for messages for the given channel, the callback will be called only on new message received.</p>
<p>Example:</p>
<pre><code>    ipc.subscribe(&quot;alerts&quot;, function(msg) {
        req.res.json(data);
    }, req);</code></pre></li>
</ul>

<ul>
<li><p><code>Ipc.prototype.unsubscribe(channel, options, callback)</code></p>
<p> Close a subscription for the given channel, no more messages will be delivered.</p>
</li>
</ul>

<ul>
<li><p><code>Ipc.prototype.publish(channel, msg, options, callback)</code></p>
<p> Publish an event to the channel to be delivered to all subscribers.</p>
<ul>
<li><code>options.queueName</code> defines the queue, if not specified then it is sent to the default queue</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>Ipc.prototype.listen(options, callback)</code></p>
<p> Listen for messages from the given queue, the callback will be called only on new message received.</p>
<p>The callback accepts 2 arguments, a message and optional next callback, if it is provided it must be called at the end to confirm or reject the message processing.
Only errors with code&gt;=500 will result in rejection, not all drivers support the next callback if the underlying queue does not support message acknowledgement.</p>
<p>Depending on the implementation, this can work as fan-out, delivering messages to all subscribed to the same channel or
can implement job queue model where only one subscriber receives a message.</p>
<p>For cases when the <code>next</code> callback is provided this means the queue implementation requires an acknowledgement of successful processing,
returning an error with <code>err.status &gt;= 500</code> will keep the message in the queue to be processed later. Special code <code>600</code> means to keep the job
in the queue and report as warning in the log.</p>
<p>Example:</p>
<pre><code>    ipc.listen({ queueName: &quot;jobs&quot; }, function(msg, next) {
        req.res.json(data);
        if (next) next();
    }, req);</code></pre></li>
</ul>

<ul>
<li><p><code>Ipc.prototype.unlisten(channel, options, callback)</code></p>
<p> Stop listening for message, if no callback is provided all listeners for the key will be unsubscribed, otherwise only the specified listener.
The callback will not be called.
It keeps a count how many subscribe/unsubscribe calls been made and stops any internal listeners once nobody is
subscribed. This is specific to a queue which relies on polling.</p>
</li>
</ul>

<ul>
<li><p><code>Ipc.prototype.submit(msg, options, callback)</code></p>
<p> Submit a message to the queue</p>
<ul>
<li><code>options.queueName</code> defines the queue, if not specified then it is sent to the default queue</li>
<li><code>options.stime</code> defines when the message should be processed, it will be held in the queue until the time comes</li>
<li><code>options.etime</code> defines when the message expires, i.e. will be dropped if not executed before this time.</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>Ipc.prototype.monitor(options)</code></p>
<p> Queue specific monitor services that must be run in the master process, this is intended to perform
queue cleanup or dealing with stuck messages</p>
</li>
</ul>

<ul>
<li><p><code>Ipc.prototype.limiter(options, callback)</code></p>
<p> Check for rate limit using the default or specific queue, by default TokenBucket using local LRU cache is
used unless a queue client provides its own implementation.</p>
<p>The options must have the following properties:</p>
<ul>
<li>name - unique id, can be IP address, account id, etc...</li>
<li>max - the maximum burst capacity</li>
<li>rate - the rate to refill tokens</li>
<li>interval - interval for the bucket refills, default 1000 ms</li>
</ul>
<p>The callback takes 1 argument, <code>delay</code> which is a number of milliseconds till the bucket can be used again if not consumed, i.e. 0 means consumed.</p>
</li>
</ul>

<ul>
<li><p><code>Ipc.prototype.checkLimiter(options, callback)</code></p>
<p> Keep checking the limiter until it is clear to proceed with the operation, if there is no available tokens in the bucket
it will wait and try again until the bucket is filled.</p>
</li>
</ul>

<ul>
<li><p><code>Ipc.prototype.lock(name, options, callback)</code></p>
<p> Implementation of a lock with optional ttl, only one instance can lock it, can be for some period of time and will expire after timeout.
A lock must be uniquely named and the ttl period is specified by <code>options.ttl</code> in milliseconds.</p>
<p>This is intended to be used for background job processing or something similar when
only one instance is needed to run. At the end of the processing <code>ipc.unlock</code> must be called to enable another instance immediately,
otherwise it will be available after the ttl only.</p>
<p>if <code>options.timeout</code> is given the function will keep trying to lock for the <code>timeout</code> milliseconds.</p>
<p>if <code>options.set</code> is given it will unconditionally set the lock for the specified ttl, this is for cases when
the lock must be active for longer because of the long running task</p>
<p>The callback must be passed which will take an error and a boolean value, if true is returned it means the timer has been locked by the caller,
otherwise it is already locked by other instance. In case of an error the lock is not supposed to be locked by the caller.</p>
<p>Example:</p>
<pre><code>    ipc.lock(&quot;my-lock&quot;, { ttl: 60000, timeout: 30000 }, function(err, locked) {
         if (locked) {
             ...
             ipc.unlock(&quot;my-lock&quot;);
         }
    });</code></pre></li>
</ul>

<ul>
<li><p><code>Ipc.prototype.unlock(name, options, callback)</code></p>
<p> Unconditionally unlock the lock, any client can unlock any lock.</p>
</li>
</ul>

<h2 id="module-ipc_client">Module: IPC_CLIENT</h2>
<ul>
<li><p><code>IpcClient.prototype.close()</code></p>
<p> Close current connection, ports.... not valid after this call</p>
</li>
</ul>

<ul>
<li><p><code>IpcClient.prototype.applyOptions()</code></p>
<p> Prepare options to be used safely</p>
</li>
</ul>

<ul>
<li><p><code>IpcClient.prototype.stats(options, callback)</code></p>
<p> Returns the cache statistics to the callback as the forst argument, the object tructure is specific to each cache implementstion</p>
</li>
</ul>

<ul>
<li><p><code>IpcClient.prototype.clear(pattern, callback)</code></p>
<p> Clear all or only matched keys from the cache</p>
</li>
</ul>

<ul>
<li><p><code>IpcClient.prototype.get(key, options, callback)</code></p>
<p> Returns an item from the cache by a key, callback is required and it acceptes only the item,
on any error null or undefined will be returned</p>
</li>
</ul>

<ul>
<li><p><code>IpcClient.prototype.put(key, val, options, callback)</code></p>
<p> Store an item in the cache, <code>options.ttl</code> can be used to specify TTL in milliseconds</p>
</li>
</ul>

<ul>
<li><p><code>IpcClient.prototype.incr(key, val, options, callback)</code></p>
<p> Add/substract a number from the an item, returns new number in the callback if provided, in case of an error null/indefined should be returned</p>
</li>
</ul>

<ul>
<li><p><code>IpcClient.prototype.del(key, options, callback)</code></p>
<p> Delete an item from the cache</p>
</li>
</ul>

<ul>
<li><p><code>IpcClient.prototype.poller()</code></p>
<p> This is called to configure and setup event processing on first subscribe call, if nobody subscribes this
will never be called and thus no initialization for event processing either, for clients that only publish this may save
lots of processing and memory.</p>
<p>This method must take care how to keep the poller running via interval or timeout as long as the <code>this._polling=1</code>.</p>
</li>
</ul>

<ul>
<li><p><code>IpcClient.prototype.schedulePoller(timeout)</code></p>
<p> Schedule next poller iteration immediately or after timeout, check configured polling rate, make sure it polls no more than
configured number of times per second</p>
</li>
</ul>

<ul>
<li><p><code>IpcClient.prototype.monitor()</code></p>
<p> Queue monitor or cleanup service, when poller is involved this will be started and can be used for cleaning up stale messages or other
maintainence work the requires.</p>
</li>
</ul>

<ul>
<li><p><code>IpcClient.prototype.isPolling()</code></p>
<p> Returns 1 if the poller is active</p>
</li>
</ul>

<ul>
<li><p><code>IpcClient.prototype.limiter(options, callback)</code></p>
<p> Rate limit check, by default it uses the master LRU cache meaning it works within one physical machine only.</p>
<p>The options must have the following properties:</p>
<ul>
<li>name - unique id, can be IP address, account id, etc...</li>
<li>rate, max, interval - same as for <code>metrics.TokenBucket</code> rate limiter.</li>
</ul>
<p>The callback argument will be called with an object where the property <code>consumed</code> set to true if consumed or false otherwise,
and the property <code>delay</code> with number of milliseconds till the bucket can be used again.</p>
</li>
</ul>

<ul>
<li><p><code>IpcClient.prototype.subscribe(channel, options, callback)</code></p>
<p> Subscribe to receive notification from the given channel</p>
</li>
</ul>

<ul>
<li><p><code>IpcClient.prototype.unsubscribe(channel, options, callback)</code></p>
<p> Stop receiving notifications on the given channel</p>
</li>
</ul>

<ul>
<li><p><code>IpcClient.prototype.publish(channel, msg, options, callback)</code></p>
<p> Publish an event</p>
</li>
</ul>

<ul>
<li><p><code>IpcClient.prototype.listen(options, callback)</code></p>
<p> Listen for incoming messages</p>
</li>
</ul>

<ul>
<li><p><code>IpcClient.prototype.unlisten(options, callback)</code></p>
<p> Stop receiving messages</p>
</li>
</ul>

<ul>
<li><p><code>IpcClient.prototype.submit(msg, options, callback)</code></p>
<p> Submit a message to a queue</p>
</li>
</ul>

<ul>
<li><p><code>IpcClient.prototype.lock(name, options, callback)</code></p>
<p> By default always lock because of lack of actual implementtion</p>
</li>
</ul>

<h2 id="module-ipc_local">Module: IPC_LOCAL</h2>
<h2 id="module-ipc_memcache">Module: IPC_MEMCACHE</h2>
<ul>
<li><p><code>client</code></p>
<p> Cache client based on Memcached server using <a href="https://github.com/3rd-Eden/node-memcached">https://github.com/3rd-Eden/node-memcached</a></p>
<p>To support more than one server use either one:</p>
<p>   ipc-cache=memcache://host1?bk-servers=host2,host3</p>
<p>   ipc-cache-memcache=memcache://host1
   ipc-cache-memcache-options-servers=host1,host2</p>
<p>To pass memcached module specific options:</p>
<p>   ipc-cache-options-failures=5
   ipc-cache-options-maxValue=1024</p>
</li>
</ul>

<h2 id="module-ipc_redis">Module: IPC_REDIS</h2>
<h2 id="module-ipc_sqs">Module: IPC_SQS</h2>
<h2 id="module-jobs">Module: JOBS</h2>
<ul>
<li><p><code>jobs</code></p>
<p> Job queue processor</p>
<p>When launched with <code>jobs-workers</code> parameter equal or greater than 0, the master spawns a number of workers which subscribe to
configured job queues or the default queue and listen for messsges.
A job message is an object that defines what method from which module to run with the options as the first argument and a callback as the second.</p>
<p>Multiple job queues can be defined and processed at the same time.</p>
</li>
</ul>

<ul>
<li><p><code>jobs.configureMaster(options, callback)</code></p>
<p> Initialize jobs processing in the master process</p>
</li>
</ul>

<ul>
<li><p><code>jobs.configureWorker(options, callback)</code></p>
<p> Initialize a worker to be ready for jobs to execute, in instance mode setup timers to exit on no activity.</p>
</li>
</ul>

<ul>
<li><p><code>jobs.shutdownWorker(options, callback)</code></p>
<p> Perform graceful worker shutdown, to be used for workers restart</p>
</li>
</ul>

<ul>
<li><p><code>jobs.initServer(options, callback)</code></p>
<p> Initialize a master that will manage jobs workers</p>
</li>
</ul>

<ul>
<li><p><code>jobs.initWorker(options, callback)</code></p>
<p> Initialize a worker for processing jobs</p>
</li>
</ul>

<ul>
<li><p><code>jobs.exitWorker(options)</code></p>
<p> Perform graceful worker shutdown and then exit the process</p>
</li>
</ul>

<ul>
<li><p><code>jobs.isCancelled(name, tag)</code></p>
<p> Returns true if a task with given name must be cancelled, this flag is set from the jobs master and
stoppable tasks must check it from time to time to terminate gracefully</p>
</li>
</ul>

<ul>
<li><p><code>jobs.cancelTask(name, options)</code></p>
<p> Send cancellation request to a worker or all workers, this has to be called from the jobs master.
<code>options.workers</code> can be a single worker id or a list of worker ids, if not given the request will be sent to all workers for the current process cluster.
<code>options.tag</code> is an opaque data that will be used to verifying which task should be cancelled, without it all tasks with given name will be cancelled.</p>
</li>
</ul>

<ul>
<li><p><code>jobs.getMaxRuntime()</code></p>
<p> Find the max runtime allowed in seconds</p>
</li>
</ul>

<ul>
<li><p><code>jobs.checkTimes()</code></p>
<p> Check how long we run a job and force kill if exceeded, check if total life time is exceeded</p>
</li>
</ul>

<ul>
<li><p><code>jobs._badJob(jobspec)</code></p>
<p> Make sure the job is valid and has all required fields, returns a normalized job object or an error, the jobspec
must be in the following formats:</p>
<pre><code>  &quot;module.method&quot;
  { job: &quot;module.method&quot; }
  { job: { &quot;module.method&quot;: {}, .... } }
  { job: [ &quot;module.method&quot;, { &quot;module.method&quot;: {} ... } ...] }</code></pre><p>any task in string format &quot;module.method&quot; will be converted into { &quot;module.method: {} } automatically</p>
</li>
</ul>

<ul>
<li><p><code>jobs.submitJob(jobspec, options, callback)</code></p>
<p> Submit a job for execution, it will be saved in a queue and will be picked up later and executed.
The queue and the way how it will be executed depends on the configured queue. See <code>isJob</code> for
the format of the job objects.</p>
<p>if <code>jobspec.uniqueTtl</code> is greater than zero it defines number of milliseconds for this job to stay in the queue or run,
it creates a global lock using the job object as the hash key, no other job can be run until the ttl expires or the job
finished, non unique jobs will be kept in the queue and repeated later according to the <code>visibilityTimeeout</code> setting.</p>
<p><code>jobspec.uniqueKey</code> can define an alternative unique key for this job for cases when different jobs must be run sequentially</p>
<p>if <code>jobspec.uniqueKeep</code> is true then keep the unique lock after the jobs finished, otherwise it is cleared</p>
<p>if <code>jobspec.uniqueDrop</code> if true will make non-unique jobs to be silently dropped instead of keeping them in the queue</p>
<p><code>jobspec.logger</code> defines the logger level which will be used to log when the job is finished, default is debug</p>
<p><code>jobspec.maxRuntime</code> defines max number of seconds this job can run, if not specified then the queue default is used</p>
<p><code>jobspec.uniqueTag</code> defines additional tag to be used for job cancelling, for cases when multiple jobs are running with the same method</p>
<p>if <code>jobspec.uniqueOnce</code> is true than the visibility timeout is not kept alive while the job is running</p>
<p><code>jobspec.noWait</code> will run the job and delete it from the queue immediately, not at the end, for one-off jobs</p>
<p><code>jobspec.startTime</code> and/or <code>jobspec.endTime</code> will define the time period during whihc this job is allowed to run, if
outside the period it will be dropped</p>
<p><code>options.delay</code> is only supported by SQS currently, it delays the job execution for the specified amount of ms</p>
<p>Special queueName: <code>jobs.selfQueue</code> is reserved to run the job immediately inside the current process,
it will call the <code>runJob</code> directly, this is useful in cases when already inside a worker and instead of submitting a new job
just run it directly. Any queue can be configured to run in <code>selfQueue</code> by setting <code>-ipc-queue[-NAME]-options-self-queue 1</code>.</p>
</li>
</ul>

<ul>
<li><p><code>jobs.runJob(jobspec, options, callback)</code></p>
<p> Run all tasks in the job object, all errors will be just logged, but if <code>noerrors</code> is defined in the top
level job object then the whole job will stop on first error returned by any task.</p>
</li>
</ul>

<ul>
<li><p><code>jobs._runJob(jobspec, options, callback)</code></p>
<p> Sequentially execute all tasks in the list, run all subtasks in parallel</p>
</li>
</ul>

<ul>
<li><p><code>jobs.runTask(name, jobspec, options, callback)</code></p>
<p> Execute a task by name, the <code>options</code> will be passed to the function as the first argument, calls the callback on finish or error</p>
</li>
</ul>

<ul>
<li><p><code>jobs._finishTask(err, name, jobspec, options, callback)</code></p>
<p> Complete task execution, cleanup and update the status</p>
</li>
</ul>

<ul>
<li><p><code>jobs.scheduleCronjob(jobspec)</code></p>
<p> Create a new cron job, for remote jobs additional property args can be used in the object to define
arguments for the instance backend process, properties must start with -</p>
<p>Example:</p>
<pre><code>    { &quot;cron&quot;: &quot;0 */10 * * * *&quot;, &quot;job&quot;: &quot;server.processQueue&quot; },
    { &quot;cron&quot;: &quot;0 */30 * * * *&quot;, &quot;job&quot;: { &quot;server.processQueue&quot;: { name: &quot;queue1&quot; } } },
    { &quot;cron&quot;: &quot;0 5 * * * *&quot;, &quot;job&quot;: [ { &quot;scraper.run&quot;: { &quot;url&quot;: &quot;host1&quot; } }, { &quot;scraper.run&quot;: { &quot;url&quot;: &quot;host2&quot; } } ] }</code></pre></li>
</ul>

<ul>
<li><p><code>jobs.scheduleCronjobs(type, list)</code></p>
<p> Schedule a list of cron jobs, types is used to cleanup previous jobs for the same type for cases when
a new list needs to replace the existing jobs. Empty list does nothing, to reset the jobs for the particular type and
empty invalid jobs must be passed, like: <code>[ {} ]</code></p>
<p>Returns number of cron jobs actually scheduled.</p>
</li>
</ul>

<ul>
<li><p><code>jobs.loadCronjobs()</code></p>
<p> Load crontab from JSON file as list of job specs:</p>
<ul>
<li>cron - cron time interval spec: &#39;second&#39; &#39;minute&#39; &#39;hour&#39; &#39;dayOfMonth&#39; &#39;month&#39; &#39;dayOfWeek&#39;</li>
<li>job - a string as obj.method or an object with job name as property name and the value is an object with
additional jobspec for the job passed as first argument, a job callback always takes jobspec and callback as 2 arguments</li>
<li>disabled - disable the job but keep in the cron file, it will be ignored</li>
<li>queueName - name of the queue where to submit this job, if not given it uses cron-queue</li>
<li>uniqueTtl - defines that this job must be the only one in the queue for the number of milliseconds specified, after that
time another job with the same arguments can be submitted.</li>
</ul>
<p>Example:</p>
<pre><code>    [ { cron: &quot;0 0 * * * *&quot;, job: &quot;scraper.run&quot; }, ..]</code></pre></li>
</ul>

<ul>
<li><p><code>jobs.parseCronjobs(type, data)</code></p>
<p> Parse a JSON data with cron jobs and schedule for the given type, this can be used to handle configuration properties</p>
</li>
</ul>

<h2 id="module-lib">Module: LIB</h2>
<ul>
<li><p><code>lib.tryCall(callback)</code></p>
<p> Run a callback if a valid function, all arguments after the callback will be passed as is</p>
</li>
</ul>

<ul>
<li><p><code>lib.tryCatch(callback)</code></p>
<p> Run a callback inside try..catch block, all arguments after the callback will be passed as is, in case of error
all arguments will be printed in the log</p>
</li>
</ul>

<ul>
<li><p><code>lib.log()</code></p>
<p> Print all arguments into the console, for debugging purposes</p>
</li>
</ul>

<ul>
<li><p><code>lib.__()</code></p>
<p> Simple i18n translation method compatible with other popular modules, supports the following usage:</p>
<ul>
<li>__(name)</li>
<li>__(fmt, arg,...)</li>
<li>__({ phrase: &quot;&quot;, locale: &quot;&quot; }, arg...</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>lib.loadLocale(file, callback)</code></p>
<p> Load a file with locale translations into memory</p>
</li>
</ul>

<ul>
<li><p><code>lib.getArg(name, dflt)</code></p>
<p> Return commandline argument value by name</p>
</li>
</ul>

<ul>
<li><p><code>lib.getArgInt(name, dflt)</code></p>
<p> Return commandline argument value as a number</p>
</li>
</ul>

<ul>
<li><p><code>lib.typeName(v)</code></p>
<p> Return object type, try to detect any distinguished type</p>
</li>
</ul>

<ul>
<li><p><code>lib.isArg(name)</code></p>
<p> Returns true of given arg(s) are present in the command line, name can be a string or an array of strings.</p>
</li>
</ul>

<ul>
<li><p><code>lib.toVersion(str)</code></p>
<p> Returns a floating number from the version string, it assumes common semver format as major.minor.patch, all non-digits will
be removed, underscores will be treated as dots. Returns a floating number which can be used in comparing versions.</p>
<p>Example</p>
<pre><code>&gt; lib.toVersion(&quot;1.0.3&quot;)
1.000003
&gt; lib.toVersion(&quot;1.0.3.4&quot;)
1.000003004
&gt; lib.toVersion(&quot;1.0.3.4&quot;) &gt; lib.toVersion(&quot;1.0.3&quot;)
true
&gt; lib.toVersion(&quot;1.0.3.4&quot;) &gt; lib.toVersion(&quot;1.0.0&quot;)
true
&gt; lib.toVersion(&quot;1.0.3.4&quot;) &gt; lib.toVersion(&quot;1.1.0&quot;)
false</code></pre></li>
</ul>

<ul>
<li><p><code>lib.encodeURIComponent(str)</code></p>
<p> Encode with additional symbols, convert these into percent encoded:</p>
<pre><code>    ! -&gt; %21, * -&gt; %2A, &#39; -&gt; %27, ( -&gt; %28, ) -&gt; %29</code></pre></li>
</ul>

<ul>
<li><p><code>lib.toTitle(name)</code></p>
<p> Convert text into capitalized words</p>
</li>
</ul>

<ul>
<li><p><code>lib.toCamel(name, chars)</code></p>
<p> Convert into camelized form, optional chars can define the separators, default is -, _ and .</p>
</li>
</ul>

<ul>
<li><p><code>lib.toUncamel(str, sep)</code></p>
<p> Convert Camel names into names separated by the given separator or dash if not.</p>
</li>
</ul>

<ul>
<li><p><code>lib.toNumber(val, options)</code></p>
<p> Safe version, uses 0 instead of NaN, handle booleans, if float specified, returns as float.</p>
<p>Options:</p>
<ul>
<li>dflt - default value</li>
<li>float - treat as floating number</li>
<li>min - minimal value, clip</li>
<li>max - maximum value, clip</li>
<li>incr - a number to add before checking for other conditions</li>
<li>mult - a number to multiply before checking for other conditions</li>
<li>novalue - replace this number with default</li>
<li>zero - replace with this number if result is 0</li>
</ul>
<p>Example:</p>
<pre><code>         lib.toNumber(&quot;123&quot;)
         lib.toNumber(&quot;1.23&quot;, { float: 1, dflt: 0, min: 0, max: 2 })</code></pre></li>
</ul>

<ul>
<li><p><code>lib.toClamp(num, min, max)</code></p>
<p> Return a number clamped between the range</p>
</li>
</ul>

<ul>
<li><p><code>lib.toBool(val, dflt)</code></p>
<p> Return true if value represents true condition, i.e. non empty value</p>
</li>
</ul>

<ul>
<li><p><code>lib.toDate(val, dflt, invalid)</code></p>
<p> Return Date object for given text or numeric date representation, for invalid date returns 1969 unless <code>invalid</code> parameter is given,
in this case invalid date returned as null. If <code>dflt</code> is NaN, null or 0 returns null as well.</p>
</li>
</ul>

<ul>
<li><p><code>lib.toMtime(val, dflt)</code></p>
<p> Return milliseconds from the date or date string, only number as dflt is supported, for invalid dates returns 0</p>
</li>
</ul>

<ul>
<li><p><code>lib.toBase62(num, alphabet)</code></p>
<p> Return base62 representation for a number</p>
</li>
</ul>

<ul>
<li><p><code>lib.toValue(val, type, options)</code></p>
<p> Convert value to the proper type</p>
</li>
</ul>

<ul>
<li><p><code>lib.toRegexp(str, options)</code></p>
<p> Safely create a regexp object, if invalid returns undefined, the options can be a string with srandard RegExp
flags or an object with the following properties:</p>
<ul>
<li>ingoreCase - similar to i</li>
<li>globalMatch - similar to m</li>
<li>multiLine - similar to m</li>
<li>unicode - similar to u</li>
<li>sticky - similar to y</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>lib.toRegexpMap(obj, val, options)</code></p>
<p> Add a regexp to the list of regexp objects, this is used in the config type <code>regexpmap</code>.</p>
</li>
</ul>

<ul>
<li><p><code>lib.toRegexpObj(obj, val, options)</code></p>
<p> Add a regexp to the object that consist of list of patterns and compiled regexp, this is used in the config type <code>regexpobj</code></p>
</li>
</ul>

<ul>
<li><p><code>lib.toDuration(mtime, options)</code></p>
<p> Return duration in human format, mtime is msecs</p>
</li>
</ul>

<ul>
<li><p><code>lib.toAge(mtime, options)</code></p>
<p> Given time in msecs, return how long ago it happened</p>
</li>
</ul>

<ul>
<li><p><code>lib.toParams(query, schema, options)</code></p>
<p> Process incoming query and convert parameters according to the type definition, the schema contains the definition of the paramaters against which to
validate incoming data. It is an object with property names and definitoons that at least must specify the type, all other options are type specific.</p>
<p>The options can define the following properties:</p>
<ul>
<li>null - always return null on any error</li>
<li>data - to pass realtime or other custom options for the validation or convertion utilities as the first argument if not defined in the definition.
This is the place to customize/add/override global parameter conditions without changing it.</li>
<li>prefix - prefix to be used when searching for the parameters in the query, only properties with this prefix will be processed. The resulting
 object will not have this prefix in the properties.</li>
<li>name - to save a value with different name than in the original query</li>
</ul>
<p>If any of the properties have <code>required:1</code> and the value will not be resolved then the function returns a string with the <code>errmsg</code> message
or the default message, this is useful for detection of invalid or missing input data.</p>
<p>Example:</p>
<pre><code>  var account = lib.toParams(req.query, { id: { type: &quot;int&quot; },
                                          count: { type: &quot;int&quot;, min: 1, max: 10, dflt: 5 },
                                          page: { type: &quot;int&quot;, min: 1, max: 10, dflt: NaN, required: 1, errmsg: &quot;Page number between 1 and 10 is required&quot; },
                                          name: { type: &quot;string&quot;, max: 32, trunc: 1 },
                                          pair: { type: &quot;map&quot;, separator: &quot;|&quot; },
                                          code: { type: &quot;string&quot;, regexp: /^[a-z]-[0-9]+$/, errmsg: &quot;Valid code is required&quot; },
                                          start: { type: &quot;token&quot;, required: 1 },
                                          email1: { type: &quot;email&quot;, required: { email: null } },
                                          data: { type: &quot;json&quot;, datatype: &quot;obj&quot; },
                                          mtime: { type: &quot;mtime&quot;, name: &quot;timestamp&quot; },
                                          flag: { type: &quot;bool&quot;, novalue: false },
                                          descr: { novalue: { name: &quot;name&quot;, value: &quot;test&quot; },
                                          email: { type: &quot;list&quot;, datatype: &quot;email&quot;, novalue: [&quot;a@a&quot;] } },
                                          internal: { ignore: 1 },
                                          tm: { type:&quot; timestamp&quot;, optional: 1 },
                                          status: { value: &quot;ready&quot; },
                                          state: { values: [&quot;ok&quot;,&quot;bad&quot;,&quot;good&quot;] },
                                          status: { value: [{ name: &quot;state&quot;, value: &quot;ok&quot;, set: &quot;1&quot; }, { name: &quot;state&quot;, value: [&quot;bad&quot;,&quot;good&quot;], op: &quot;in&quot; }],
                                          obj: { type: &quot;obj&quot;, params: { id: { type: &quot;int&quot; }, name: {} } },
                                          arr: { type: &quot;array&quot;, params: { id: { type: &quot;int&quot; }, name: {} } },
                                          state: { type: &quot;list&quot;, datatype: &quot;string, values: [&quot;VA&quot;,&quot;DC] } },
                                          ssn: { type: &quot;string&quot;, regexp: /^[0-9]{3}-[0-9]{3}-[0-9]{4}$/, errmsg: &quot;Valid SSN is required&quot; } },
                                          phone: { type: &quot;list&quot;, datatype: &quot;number } },
                                        { data: { start: { secret: req.account.secret },
                                                  name: { dflt: &quot;test&quot; },
                                                  count: { max: 100 },
                                                  email: { ignore: req.account.type != &quot;admin&quot; },
                                                  &#39;*&#39;: { empty: 1, null: 1 },
                                        })
  if (typeof account == &quot;string) return api.sendReply(res, 400, account);</code></pre></li>
</ul>

<ul>
<li><p><code>lib.toFormat(format, data, options)</code></p>
<p> Convert a list of records into the specified format, supported formats are: <code>xml, csv, json</code>.</p>
<ul>
<li>For <code>csv</code> the default separator is comma but can be specified with <code>options.separator</code>. To produce columns header specify <code>options.header</code>.</li>
<li>For <code>json</code> format puts each record as a separate JSON object on each line, so to read it back
it will require to read every line and parse it and add to the list.</li>
<li>For <code>xml</code> format the name of the row tag is <code>&lt;row&gt;</code> but can be
specified with <code>options.tag</code>.</li>
</ul>
<p>All formats support the property <code>options.allow</code> which is a list of property names that are allowed only in the output for each record, non-existent
properties will be replaced by empty strings.</p>
<p>The <code>mapping</code> object property can redefine different tag/header names to be put into the file instead of the exact column names from the records.</p>
</li>
</ul>

<ul>
<li><p><code>lib.toTemplate(text, obj, options)</code></p>
<p> Given a template with @..@ placeholders, replace each placeholder with the value from the obj.
The <code>obj</code> can be an object or an array of objects in which case all objects will be checked for the value until non empty.</p>
<p>To use @ in the template specify it as @@</p>
<p>The options if given may provide the following:</p>
<ul>
<li>allow - placeholders with a name present in this list will be replaced, all other will be replaced with empty string</li>
<li>skip - placeholders with a name present in this list will be ignored, the placeholer will be kept</li>
<li>only - placeholders with a name present in this list will be replaced only, all other will be ignored and kept as placeholders</li>
<li>encoding - can be url or base64, the replaced values will be encoded accordingly</li>
<li>separator1 - left side of the placehoder, default is @</li>
<li>separator2 - right side of the placeholder, default is @</li>
</ul>
<p>Example:</p>
<pre><code>  lib.toTemplate(&quot;http://www.site.com/@code@/@id@&quot;, { id: 123, code: &quot;YYY&quot; }, { encoding: &quot;url&quot; })
  lib.toTemplate(&quot;Hello @name|friend@!&quot;, {})</code></pre></li>
</ul>

<ul>
<li><p><code>lib.escapeUnicode(text)</code></p>
<p> Convert all Unicode binary symbols into Javascript text representation</p>
</li>
</ul>

<ul>
<li><p><code>lib.unescape(str)</code></p>
<p> Convert escaped characters into native symbols</p>
</li>
</ul>

<ul>
<li><p><code>lib.textToXml(str)</code></p>
<p> Convert all special symbols into xml entities</p>
</li>
</ul>

<ul>
<li><p><code>lib.textToEntity(str)</code></p>
<p> Convert all special symbols into html entities</p>
</li>
</ul>

<ul>
<li><p><code>lib.entityToText(str)</code></p>
<p> Convert html entities into their original symbols</p>
</li>
</ul>

<ul>
<li><p><code>lib.isObject(v)</code></p>
<p> Returns true of the argument is a generic object, not a null, Buffer, Date, RegExp or Array</p>
</li>
</ul>

<ul>
<li><p><code>lib.isNumber(val)</code></p>
<p> Return true if the value is a number</p>
</li>
</ul>

<ul>
<li><p><code>lib.isPrefix(val, prefix)</code></p>
<p> Return true if the value is prefixed</p>
</li>
</ul>

<ul>
<li><p><code>lib.isUuid(val, prefix)</code></p>
<p> Returns true if the value represents an UUID</p>
</li>
</ul>

<ul>
<li><p><code>lib.isPositive(val)</code></p>
<p> Returns true if a number is positive, i.e. greater than zero</p>
</li>
</ul>

<ul>
<li><p><code>lib.isArray(val, dflt)</code></p>
<p> Returns the array if the value is non empty array or dflt value if given or undefined</p>
</li>
</ul>

<ul>
<li><p><code>lib.isEmpty(val)</code></p>
<p> Return true of the given value considered empty</p>
</li>
</ul>

<ul>
<li><p><code>lib.isNumeric(val)</code></p>
<p> Returns true if the value is a number or string representing a number</p>
</li>
</ul>

<ul>
<li><p><code>lib.isNumericType(type)</code></p>
<p> Returns true if the given type belongs to the numeric family of data types</p>
</li>
</ul>

<ul>
<li><p><code>lib.isDate(d)</code></p>
<p> Returns true if the given date is valid</p>
</li>
</ul>

<ul>
<li><p><code>lib.isTimeRange(time1, time2, options)</code></p>
<p> Returns 0 if the current time is not within specified valid time range or it is invalid. Only continious time rang eis support, it
does not handle over the midninght ranges, i.e. time1 is always must be greater than time2.</p>
<p><code>options.tz</code> to specify timezone, no timezone means current timezone.
<code>options.date</code> if given must be a list of dates in the format: YYY-MM-DD,...</p>
</li>
</ul>

<ul>
<li><p><code>lib.isTrue(val1, val2, op, type)</code></p>
<p> Evaluate expr, compare 2 values with optional type and operation</p>
</li>
</ul>

<ul>
<li><p><code>lib.isMatched(obj, condition)</code></p>
<p> All properties in the object <code>obj</code> must match all properties in the object <code>condition</code>,
each value in <code>condition</code> is treated as RegExp,
if the value is equal to &#39;null&#39; which means an empty or non-existed value,
if the value begins with ! it means no match is expected, the leading ! will be stripped.</p>
<p>if <code>condition</code> is an array then returns true if any matched</p>
<p>empty condition returns true</p>
<p>Example:</p>
<pre><code>  lib.isMatched({id:1,name:2,notifications0:1,type:&quot;user,admin&quot;}, {notifications0:0})
  lib.isMatched({id:1,name:2,notifications0:1,type:&quot;user,admin&quot;}, {notifications0:1,type:&quot;!admin&quot;})</code></pre></li>
</ul>

<ul>
<li><p><code>lib.isFlag(list, name)</code></p>
<p> Returns true if <code>name</code> exists in the array <code>list</code>, search is case sensitive. if <code>name</code> is an array it will return true if
any element in the array exists in the <code>list</code>.</p>
</li>
</ul>

<ul>
<li><p><code>lib.toFlags(cmd, list, name)</code></p>
<p> Flags command utility, update flags array and returns a new array, the commands are:</p>
<ul>
<li>add - adds the <code>name</code> flags if does not exists</li>
<li>del - removes the flags <code>name</code></li>
<li>present - returns only flags that present in the list <code>name</code></li>
<li>absent - returns only flags that are not present in the list <code>name</code></li>
</ul>
</li>
</ul>

<ul>
<li><p><code>lib.forEachLine(file, options, lineCallback, endCallback)</code></p>
<p> Call callback for each line in the file
options may specify the following parameters:</p>
<ul>
<li>sync - read file synchronously and call callback for every line</li>
<li>abort - signal to stop processing</li>
<li>limit - number of lines to process and exit</li>
<li>count - return this number of lines in an array if greater than 0</li>
<li>skip - number of lines to skip from the start</li>
<li>progress - if &gt; 0 report how many lines processed so far every specified lines</li>
<li>until - skip lines until this regexp matches</li>
<li>ignore - skip lines that match this regexp</li>
<li>header - if true then skip first line because it is the a header, if <code>options.header</code> it is a function
it will be called with the first line as an argument and must return true if this line needs to be skipped</li>
<li>json - each line represents an JSON object, convert and pass it to the line callback if not null</li>
<li>split - split every line before calling the callback, it uses phraseSplit</li>
<li>keepempty - by default is enabled if split is set to keep empty fields in the line array</li>
<li>separator - a string with characters to be used for splitting, default is <code>,</code></li>
<li>quotes - a string with characters to be used for phrase splitting, default is <code>&quot;&#39;</code></li>
</ul>
<p>Properties updated and returned in the options:</p>
<ul>
<li>nlines - number of lines read from the file</li>
<li>ncalls - number of lines passed to the line callback</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>lib.forEach(list, iterator, callback)</code></p>
<p> Apply an iterator function to each item in an array in parallel. Execute a callback when all items
have been completed or immediately if there is an error provided.</p>
<pre><code>    lib.forEach([ 1, 2, 3 ], function (i, next) {
        console.log(i);
        next();
    }, function (err) {
        console.log(&#39;done&#39;);
    });</code></pre></li>
</ul>

<ul>
<li><p><code>lib.forEvery(list, iterator, callback)</code></p>
<p> Same as <code>forEach</code> except that the iterator will be called for every item in the list, all errors will be ignored</p>
</li>
</ul>

<ul>
<li><p><code>lib.forEachSeries(list, iterator, callback)</code></p>
<p> Apply an iterator function to each item in an array serially. Execute a callback when all items
have been completed or immediately if there is is an error provided.</p>
<pre><code>    lib.forEachSeries([ 1, 2, 3 ], function (i, next) {
      console.log(i);
      next();
    }, function (err) {
      console.log(&#39;done&#39;);
    });</code></pre></li>
</ul>

<ul>
<li><p><code>lib.forEverySeries(list, iterator, callback)</code></p>
<p> Same as <code>forEachSeries</code> except that the iterator will be called for every item in the list, all errors will be passed to the next
item with optional additional data argument.</p>
<pre><code>    lib.forEverySeries([ 1, 2, 3 ], function (i, next, err, data) {
      console.log(i, err, data);
      next(err, i, data);
    }, function (err, data) {
      console.log(&#39;done&#39;, err, data);
    });</code></pre></li>
</ul>

<ul>
<li><p><code>lib.forEachLimit(list, limit, iterator, callback)</code></p>
<p> Apply an iterator function to each item in an array in parallel as many as specified in <code>limit</code> at a time. Execute a callback when all items
have been completed or immediately if there is is an error provided.</p>
</li>
</ul>

<ul>
<li><p><code>lib.forEveryLimit(list, limit, iterator, callback)</code></p>
<p> Same as <code>forEachLimit</code> but does not stop on error, all items will be processed and errors will be collected in an array and
passed to the final callback</p>
</li>
</ul>

<ul>
<li><p><code>lib.forEachItem(options, next, iterator, callback)</code></p>
<p> Apply an iterator function to each item returned by the <code>next(item, cb)</code> function until it returns <code>null</code> or the iterator returns an error in the callback,
the final callback will be called after all iterators are finished.</p>
<p>If no item is available the <code>next()</code> should return empty value, it will be called again in <code>options.interval</code> ms if specified or
immediately in the next tick cycle.</p>
<p>The max number of iterators to run at the same time is controlled by <code>options.max</code>, default is 1.</p>
<p>The maximum time waiting for items can be specified by <code>options.timeout</code>, it is not an error condition, just another way to stop
processing if it takes too long because the <code>next()</code> function is a black box just returning items to process. Timeout will send null
to the queue and it will stop after all iterators are finished.</p>
</li>
</ul>
<pre><code>    var list = [1, 2, &quot;&quot;, &quot;&quot;, 3, &quot;&quot;, 4, &quot;&quot;, &quot;&quot;, &quot;&quot;, null];
    lib.forEachItem({ max: 2, interval: 1000, timeout: 30000 },
        function(next) {
            next(list.shift());
        },
        function(item, next) {
            console.log(&quot;item:&quot;, item);
            next();
        },
        (err) =&gt; {
            console.log(&quot;done&quot;, err);
        });</code></pre>
<ul>
<li><p><code>lib.parallel(tasks, callback)</code></p>
<p> Execute a list of functions in parallel and execute a callback upon completion or occurance of an error. Each function will be passed
a callback to signal completion. The callback accepts an error for the first argument. The iterator and callback will be
called via setImmediate function to allow the main loop to process I/O.</p>
</li>
</ul>

<ul>
<li><p><code>lib.everyParallel(tasks, callback)</code></p>
<p> Same as <code>lib.parallel</code> but all functions will be called and any error will be ignored</p>
</li>
</ul>

<ul>
<li><p><code>lib.series(tasks, callback)</code></p>
<p> Execute a list of functions serially and execute a callback upon completion or occurance of an error. Each function will be passed
a callback to signal completion. The callback accepts either an error for the first argument in which case the flow will be aborted
and the final callback will be called immediately or some optional data to be passed to thr next iterator function as a second argument.</p>
<p>The iterator and callback will be
called via setImmediate function to allow the main loop to process I/O.</p>
<pre><code>    lib.series([
       function(next) {
          setTimeout(function () { next(null, &quot;data&quot;); }, 100);
       },
       function(next, data) {
          setTimeout(function () { next(); }, 100);
       },
    ], function(err) {
        console.log(err);
    });</code></pre></li>
</ul>

<ul>
<li><p><code>lib.everySeries(tasks, callback)</code></p>
<p> Same as <code>lib.series</code> but all functions will be called with errors passed to the next task, only the last passed error will be returned</p>
<pre><code>    lib.everySeries([
       function(next, err) {
          setTimeout(function () { next(&quot;error1&quot;, &quot;data1&quot;); }, 100);
       },
       function(next, err, data) {
          setTimeout(function () { next(err, &quot;data2&quot;); }, 100);
       },
    ], function(err, data) {
        console.log(err, data);
    });</code></pre></li>
</ul>

<ul>
<li><p><code>lib.whilst(test, iterator, callback, data)</code></p>
<p> While the test function returns true keep running the iterator, call the callback at the end if specified. All functions are called via setImmediate.</p>
<pre><code>    var count = 0;
    lib.whilst(
        function() {
          return count &lt; 5;
        },
        function (next) {
          count++;
          setTimeout(next, 1000);
        },
        function (err, data) {
          console.log(err, data, count);
        });</code></pre></li>
</ul>

<ul>
<li><p><code>lib.doWhilst(iterator, test, callback, data)</code></p>
<p> Keep running iterator while the test function returns true, call the callback at the end if specified. All functions are called via setImmediate.</p>
</li>
</ul>

<ul>
<li><p><code>lib.deferCallback(parent, msg, callback, timeout)</code></p>
<p> Register the callback to be run later for the given message, the message may have the <code>__id</code> property which will be used for keeping track of the responses or it will be generated.
The <code>parent</code> can be any object and is used to register the timer and keep reference to it.</p>
<p>A timeout is created for this message, if <code>runCallback</code> for this message will not be called in time the timeout handler will call the callback
anyway with the original message.</p>
<p>The callback passed will be called with only one argument which is the message, what is inside the message this function does not care. If
any errors must be passed, use the message object for it, no other arguments are expected.</p>
</li>
</ul>

<ul>
<li><p><code>lib.onDeferCallback(msg)</code></p>
<p> To be called on timeout or when explicitely called by the <code>runCallback</code>, it is called in the context of the message.</p>
</li>
</ul>

<ul>
<li><p><code>lib.runCallback(parent, msg)</code></p>
<p> Run delayed callback for the message previously registered with the <code>deferCallback</code> method.
The message must have <code>id</code> property which is used to find the corresponding callback, if the msg is a JSON string it will be converted into the object.</p>
<p>Same parent object must be used for <code>deferCallback</code> and this method.</p>
</li>
</ul>

<ul>
<li><p><code>lib.deferInterval(parent, interval, name, callback)</code></p>
<p> Assign or clear an interval timer, keep the reference in the given parent object</p>
</li>
</ul>

<ul>
<li><p><code>lib.geoHash(latitude, longitude, options)</code></p>
<p> Return object with geohash for given coordinates to be used for location search</p>
<p>The options may contain the following properties:</p>
<ul>
<li>distance - limit the range key with the closest range smaller than then distance, required for search but for updates may be omitted</li>
<li>minDistance - radius for the smallest bounding box in km containing single location, radius searches will combine neighboring boxes of
 this size to cover the whole area with the given distance request, also this affects the length of geohash keys stored in the bk_location table
 if not specified default <code>min-distance</code> value will be used.</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>lib.geoDistance(latitude1, longitude1, latitude2, longitude2, options)</code></p>
<p> Return distance between two locations</p>
<p>The options can specify the following properties:</p>
<ul>
<li>round - a number how to round the distance</li>
</ul>
<p>Example: round to the nearest full 5 km and use only 1 decimal point, if the distance is 13, it will be 15.0</p>
<pre><code>lib.geoDistance(34, -188, 34.4, -119, { round: 5.1 })</code></pre></li>
</ul>

<ul>
<li><p><code>lib.busyTimer(name, val)</code></p>
<p> Busy timer handler, supports commands:</p>
<ul>
<li>init - start the timer with the given latency in ms</li>
<li>get - returns the latest latency</li>
<li>busy - returns true if busy i.e. latency is greater than configured</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>lib.sortByVersion(list, name)</code></p>
<p> Sort a list be version in descending order, an item can be a string or an object with
a property to sort by, in such case <code>name</code> must be specified which property to use for sorting.
The name format is assumed to be: <code>XXXXX-N.N.N</code></p>
</li>
</ul>

<ul>
<li><p><code>lib.getUser(user)</code></p>
<p> Return an object with user info from the /etc/passwd file, user can be uid or name, if user is ommitted the current user is returned</p>
</li>
</ul>

<ul>
<li><p><code>lib.getGroup(group)</code></p>
<p> Return an object with specified group info for the current user of for the given group id or name</p>
</li>
</ul>

<ul>
<li><p><code>lib.dropPrivileges(uid, gid)</code></p>
<p> Drop root privileges and switch to a regular user</p>
</li>
</ul>

<ul>
<li><p><code>lib.encrypt(key, data, algorithm, encode)</code></p>
<p> Encrypt data with the given key code</p>
</li>
</ul>

<ul>
<li><p><code>lib.decrypt(key, data, algorithm, encode)</code></p>
<p> Decrypt data with the given key code</p>
</li>
</ul>

<ul>
<li><p><code>lib.ip2int(ip)</code></p>
<p> Convert an IP address into integer</p>
</li>
</ul>

<ul>
<li><p><code>lib.int2ip(int)</code></p>
<p> Convert an integer into IP address</p>
</li>
</ul>

<ul>
<li><p><code>lib.inCidr(ip, cidr)</code></p>
<p> Return true if the given IP address is within the given CIDR block</p>
</li>
</ul>

<ul>
<li><p><code>lib.cidrRange(cidr)</code></p>
<p> Return first and last IP addresses for the CIDR block</p>
</li>
</ul>

<ul>
<li><p><code>lib.sign (key, data, algorithm, encode)</code></p>
<p> HMAC signing and base64 encoded, default algorithm is sha1</p>
</li>
</ul>

<ul>
<li><p><code>lib.hash (data, algorithm, encode)</code></p>
<p> Hash and base64 encoded, default algorithm is sha1</p>
</li>
</ul>

<ul>
<li><p><code>lib.uuid(prefix, options)</code></p>
<p> Return unique Id without any special characters and in lower case</p>
</li>
</ul>

<ul>
<li><p><code>lib.getHashid(salt, min, alphabet)</code></p>
<p> Return cached Hashids object for the given configuration</p>
</li>
</ul>

<ul>
<li><p><code>lib.suuid(prefix, options)</code></p>
<p> Returns a short unique id</p>
</li>
</ul>

<ul>
<li><p><code>lib.tuuid(prefix, encode)</code></p>
<p> Returns time sortable unique id, inspired by <a href="https://github.com/paixaop/node-time-uuid">https://github.com/paixaop/node-time-uuid</a></p>
</li>
</ul>

<ul>
<li><p><code>lib.tuuidTime(str)</code></p>
<p> Return time in milliseconds from the time uuid</p>
</li>
</ul>

<ul>
<li><p><code>lib.random(size)</code></p>
<p> Generate random key, size if specified defines how many random bits to generate</p>
</li>
</ul>

<ul>
<li><p><code>lib.randomUShort()</code></p>
<p> Return random number between 0 and USHORT_MAX</p>
</li>
</ul>

<ul>
<li><p><code>lib.randomShort()</code></p>
<p> Return random number between 0 and SHORT_MAX</p>
</li>
</ul>

<ul>
<li><p><code>lib.randomUInt()</code></p>
<p> Return random number between 0 and ULONG_MAX</p>
</li>
</ul>

<ul>
<li><p><code>lib.randomFloat()</code></p>
<p> Returns random number between 0 and 1, 32 bits</p>
</li>
</ul>

<ul>
<li><p><code>lib.randomInt(min, max)</code></p>
<p> Return random integer between min and max inclusive using crypto generator, based on
<a href="https://github.com/joepie91/node-random-number-csprng">https://github.com/joepie91/node-random-number-csprng</a></p>
</li>
</ul>

<ul>
<li><p><code>lib.randomNum(min, max, decs)</code></p>
<p> Generates a random number between given min and max (required)
Optional third parameter indicates the number of decimal points to return:</p>
<ul>
<li>If it is not given or is NaN, random number is unmodified</li>
<li>If &gt;0, then that many decimal points are returned (e.g., &quot;2&quot; -&gt; 12.52</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>lib.clock()</code></p>
<p> Returns current time in microseconds</p>
</li>
</ul>

<ul>
<li><p><code>lib.shuffle(list)</code></p>
<p> Randomize the list items in place</p>
</li>
</ul>

<ul>
<li><p><code>lib.now()</code></p>
<p> Return number of seconds for current time</p>
</li>
</ul>

<ul>
<li><p><code>lib.weekOfYear(date, utc)</code></p>
<p> Return an ISO week number for given date, from <a href="https://www.epochconverter.com/weeknumbers">https://www.epochconverter.com/weeknumbers</a></p>
</li>
</ul>

<ul>
<li><p><code>lib.strftime(date, fmt, options)</code></p>
<p> Format date object</p>
</li>
</ul>

<ul>
<li><p><code>lib.sprintf(fmt, args)</code></p>
<p> C-sprintf alike
based on <a href="http://stackoverflow.com/a/13439711">http://stackoverflow.com/a/13439711</a>
Usage:</p>
<ul>
<li>sprintf(fmt, arg, ...)</li>
<li>sprintf(fmt, [arg, ...]);</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>lib.toRFC3339 (date)</code></p>
<p> Return RFC3339 formatted timestamp for a date or current time</p>
</li>
</ul>

<ul>
<li><p><code>lib.zeropad(n, width)</code></p>
<p> Return a string with leading zeros</p>
</li>
</ul>

<ul>
<li><p><code>lib.matchRegexp(str, rx, index)</code></p>
<p> Perform match on a regexp for a string and returns matched value, if no index is specified returns item 1,
this is a simple case for oneshot match and only single matched element.</p>
</li>
</ul>

<ul>
<li><p><code>lib.testRegexp(str, rx)</code></p>
<p> Perform test on a regexp for a string and returns true only if matched.</p>
</li>
</ul>

<ul>
<li><p><code>lib.replaceRegexp(str, rx, val)</code></p>
<p> Safe version of replace for strings, always returns a string, if <code>val</code> is not provided performs
removal of the matched patterns</p>
</li>
</ul>

<ul>
<li><p><code>lib.unicode2Ascii(str)</code></p>
<p> Replace Unicode symbols with ASCII equivalents</p>
</li>
</ul>

<ul>
<li><p><code>lib.strTrim(str, chars)</code></p>
<p> Remove all whitespace from the begining and end of the given string, if an array with characters is not given then it trims all whitespace</p>
</li>
</ul>

<ul>
<li><p><code>lib.strSplit(str, sep, options)</code></p>
<p> Split string into array, ignore empty items,</p>
<ul>
<li><code>sep</code> is an RegExp to use as a separator instead of default  pattern <code>[,\|]</code>,</li>
<li><code>options</code> is an object with the same properties as for the <code>toParams</code>, <code>datatype&#39; will be used with</code>lib.toValue` to convert the value for each item</li>
</ul>
<p>If <code>str</code> is an array and type is not specified then all non-string items will be returned as is.</p>
</li>
</ul>

<ul>
<li><p><code>lib.strSplitUnique(str, sep, options)</code></p>
<p> Split as above but keep only unique items, case-insensitive</p>
</li>
</ul>

<ul>
<li><p><code>lib.phraseSplit(str, options)</code></p>
<p> Split a string into phrases separated by <code>options.separator</code> character(s) and surrounded by characters in <code>options.quotes</code>. The default separator is space and
default quotes are both double and single quote. If <code>options.keepempty</code> is given all empty parts will be kept in the list.</p>
</li>
</ul>

<ul>
<li><p><code>lib.arrayLength(list)</code></p>
<p> Return the length of an array or 0 if it is not an array</p>
</li>
</ul>

<ul>
<li><p><code>lib.arrayRemove(list, item)</code></p>
<p> Remove the given item from the list in place, returns the same list</p>
</li>
</ul>

<ul>
<li><p><code>lib.arrayUnique(list, key)</code></p>
<p> Returns only unique items in the array, optional <code>key</code> specified the name of the column to use when determining uniqueness if items are objects.</p>
</li>
</ul>

<ul>
<li><p><code>lib.arrayFlatten(list)</code></p>
<p> Flatten array of arrays into a single array</p>
</li>
</ul>

<ul>
<li><p><code>lib.jsonToBase64(data, secret, algorithm)</code></p>
<p> Stringify JSON into base64 string, if secret is given, sign the data with it</p>
</li>
</ul>

<ul>
<li><p><code>lib.base64ToJson(data, secret, algorithm)</code></p>
<p> Parse base64 JSON into JavaScript object, in some cases this can be just a number then it is passed as it is, if secret is given verify
that data is not chnaged and was signed with the same secret</p>
</li>
</ul>

<ul>
<li><p><code>lib.strCompress(data, encoding)</code></p>
<p> From <a href="https://github.com/pieroxy/lz-string/">https://github.com/pieroxy/lz-string/</a></p>
</li>
</ul>

<ul>
<li><p><code>lib.domainName(host)</code></p>
<p> Extract domain from the host name, takes all host parts except the first one</p>
</li>
</ul>

<ul>
<li><p><code>lib.newError(msg, status, code)</code></p>
<p> Return a new Error object, msg can be a string or an object with message, code, status properties.
The default error status is 400 if not specified.</p>
</li>
</ul>

<ul>
<li><p><code>lib.traceError(err)</code></p>
<p> Returns the error stack or the error itself, to be used in error messages</p>
</li>
</ul>

<ul>
<li><p><code>lib.exists(obj, name)</code></p>
<p> Return true if a variable or property in the object exists,</p>
<ul>
<li>if obj is null or undefined return false</li>
<li>if obj is an object, return true if the property is not undefined</li>
<li>if obj is an array then search for the value with indexOf, only simple values supported,</li>
<li>if both are arrays return true if at least one item is in both arrays</li>
</ul>
<p>Example:</p>
<pre><code>   lib.exists({ 1: 1 }, &quot;1&quot;)
   lib.exists([ 1, 2, 3 ], 1)
   lib.exists([ 1, 2, 3 ], [ 1, 5 ])</code></pre></li>
</ul>

<ul>
<li><p><code>lib.callback()</code></p>
<p> Returns first valid function object from the arguments, if no function found a placeholder is returned</p>
</li>
</ul>

<ul>
<li><p><code>lib.objClone()</code></p>
<p> A copy of an object, this is a shallow copy, only arrays and objects are created but all other types are just referenced in the new object</p>
<ul>
<li>first argument is the object to clone, can be null</li>
<li>all additional arguments are treated as name value pairs and added to the cloned object as additional properties
Example:<pre><code>  lib.objClone({ 1: 2 }, &quot;3&quot;, 3, &quot;4&quot;, 4)</code></pre></li>
</ul>
</li>
</ul>

<ul>
<li><p><code>lib.objNew()</code></p>
<p> Return new object using arguments as name value pairs for new object properties</p>
</li>
</ul>

<ul>
<li><p><code>lib.objFlatten(obj, options)</code></p>
<p> Flatten a javascript object into a single-depth object, all nested values will have property names appended separated by comma</p>
<p>The options properties:</p>
<ul>
<li>separator - use something else instead of .</li>
<li>index - initial index for arrays, 0 is default</li>
</ul>
<p>Example</p>
<pre><code>    &gt; lib.objFlatten({ a: { c: 1 }, b: { d: 1 } } )
    { &#39;a.c&#39;: 1, &#39;b.d&#39;: 1 }
   &gt; lib.objFlatten({ a: { c: 1 }, b: { d: [1,2,3] } }, { index: 1 })
    { &#39;a.c&#39;: 1, &#39;b.d.1&#39;: 1, &#39;b.d.2&#39;: 2, &#39;b.d.3&#39;: 3 }</code></pre></li>
</ul>

<ul>
<li><p><code>lib.objClean(obj, options)</code></p>
<p> Cleanup object properties, delete all undefined values in place by default.
Additional options:</p>
<ul>
<li>If <code>null</code> is true then delete all null properties.</li>
<li>If <code>type</code> is a RegExp then all properties that match it by type will be deleted.</li>
<li>If <code>name</code> is a RegExp then all properties that match it by name will be deleted.</li>
<li>If <code>value</code> is a RegExp then all string|number|boolean properties that match it by value will be deleted.</li>
<li>If <code>array</code> is true then process all array items recursivelly</li>
</ul>
<p>Example</p>
<blockquote>
<p>lib.cleanObj({ a: 1, b: true, c: undefined, d: 2, e: null, l: [&quot;a&quot;, &quot;b&quot;, null, undefined, { a: 1, b: undefined } ] },{ null:1, array:1, type: /boolean/})
   { a: 1, d: 2, l: [ &#39;a&#39;, &#39;b&#39;, { a: 1 } ] }</p>
</blockquote>
</li>
</ul>

<ul>
<li><p><code>lib.objExtend(obj, val, options)</code></p>
<p> Add properties to an existing object, two use cases:</p>
<ul>
<li>the first arg is the object, the rest are pairs: name, value,....</li>
<li>the first arg is the object, the second arg is an object to add properties from. In this case
the third argument can be an options object that can control how the properties are merged.</li>
</ul>
<p>Options properties:</p>
<ul>
<li>allow - a regexp which properties are allowed to be merged</li>
<li>ignore - a regexp which properties should be ignored</li>
<li>remove - a regexp to apply to each property name before merging, the matching parts will be removed from the name</li>
</ul>
</li>
</ul>
<pre><code>     lib.objExtend({ a: 1 }, &#39;b&#39;, 2, &#39;c&#39; 3 )
     lib.objExtend({ a: 1 }, { b: 2, c: 3 }, {})
     lib.objExtend({ a: 1 }, { b: 2, _c: 3, _d: 4 }, { remove: /^_/ })</code></pre>
<ul>
<li><p><code>lib.objMerge(obj, val, options)</code></p>
<p> Merge two objects, all properties from the <code>val</code> override existing properties in the <code>obj</code>, returns a new object, shallow copy,
only top level properties are reassigned.</p>
<p>Options properties:</p>
<ul>
<li>allow - a regexp which properties are allowed to be merged</li>
<li>ignore - a regexp which properties should be ignored</li>
<li>remove - a regexp to apply to each property name before merging, the matching parts will be removed from the name</li>
</ul>
<p>Example</p>
<pre><code> var o = lib.objMerge({ a:1, b:2, c:3 }, { c:5, d:1, _e: 4, x: 2 }, { allow: /^(c|d)/, remove: /^_/ })
 o = { a:1, b:2, c:5, d:1 }</code></pre></li>
</ul>

<ul>
<li><p><code>lib.objDel()</code></p>
<p> Delete properties from the object, first arg is an object, the rest are properties to be deleted</p>
</li>
</ul>

<ul>
<li><p><code>lib.objSearch(obj, options)</code></p>
<p> Return an object consisting of properties that matched given criteria in the given object or object of objects.
options can define the following properties:</p>
<ul>
<li>exists - search by property name, return all objects that contain given property</li>
<li>hasvalue - search by value, return all objects that have a property with given value</li>
<li>sort - if set then sort found columns by the property <code>name</code> or if it is a string by the given property</li>
<li>names - if true just return list of column names</li>
<li>flag - if true, return object with all properties set to flag value</li>
<li>value - if given return the value of this property, not the whole matched object</li>
<li>count - if true return just number of found properties</li>
</ul>
<p>Example</p>
<pre><code>    lib.objSearch({id:{index:1},name:{index:3},type:{index:2},descr:{}}, { exists: &#39;index&#39;, sort: 1 });
    { id: { index: 1 }, type: { index: 2 }, name: { index: 3 } }

    lib.objSearch({id:1,name:&quot;test&quot;,type:&quot;test&quot;,descr:&quot;descr&quot;}, { hasvalue: &#39;test&#39;, count: 1});
    2</code></pre></li>
</ul>

<ul>
<li><p><code>lib.objGet(obj, name, options)</code></p>
<p> Return a property from the object, name specifies the path to the property, if the required property belong to another object inside the top one
the name uses . to separate objects. This is a convenient method to extract properties from nested objects easily.
Options may contains the following properties:</p>
<ul>
<li>list - return the value as a list even if there is only one value found</li>
<li>obj - return the value as an object, if the result is a simple type, wrap into an object like { name: name, value: result }</li>
<li>str - return the value as a string, convert any other type into string</li>
<li>num - return the value as a number, convert any other type by using toNumber</li>
<li>func - return the value as a function, if the object is not a function returns null</li>
<li>owner - return the owner object, not the value, i.e. return the object who owns the value specified in the name</li>
</ul>
<p>Example:</p>
<pre><code>    &gt; lib.objGet({ response: { item : { id: 123, name: &quot;Test&quot; } } }, &quot;response.item.name&quot;)
    &quot;Test&quot;
    &gt; lib.objGet({ response: { item : { id: 123, name: &quot;Test&quot; } } }, &quot;response.item.name&quot;, { list: 1 })
    [ &quot;Test&quot; ]
    &gt; lib.objGet({ response: { item : { id: 123, name: &quot;Test&quot; } } }, &quot;response.item.name&quot;, { owner: 1 })
    { item : { id: 123, name: &quot;Test&quot; } }</code></pre></li>
</ul>

<ul>
<li><p><code>lib.objSet(obj, name, value, options)</code></p>
<p> Set a property of the object, name can be an array or a string with property path inside the object, all non existent intermediate
objects will be create automatically. The options can have the folowing properties:</p>
<ul>
<li>incr - increment a numeric property with the given number or 1, non-existing propertties will be initilaized with 0</li>
<li>mult - multiply a numeric property with the given number, non-existing properties will be initialized with 0</li>
<li>push - add to the array, if it is not an array a new empty aray is created</li>
<li>append - append to a string</li>
<li>unique - only push if not in the list</li>
<li>separator - separator for object names, default is <code>.</code></li>
<li>result - &quot;new&quot; - new value, &quot;old&quot; - old value otherwise the object itself</li>
</ul>
<p>Example</p>
<pre><code>    var a = lib.objSet({}, &quot;response.item.count&quot;, 1)
    lib.objSet(a, &quot;response.item.count&quot;, 1, { incr: 1 })</code></pre></li>
</ul>

<ul>
<li><p><code>lib.objIncr(obj, name, count, result)</code></p>
<p> Increment a property by the specified number, if the property does not exist it will be created,
returns new incremented value or the value specified by the <code>result</code> argument.
It uses <code>lib.objSet</code> so the property name can be a nested path.</p>
</li>
</ul>

<ul>
<li><p><code>lib.objMult(obj, name, count, result)</code></p>
<p> Similar to <code>objIncr</code> but does multiplication</p>
</li>
</ul>

<ul>
<li><p><code>lib.objKeys(obj)</code></p>
<p> Return all property names for an object</p>
</li>
</ul>

<ul>
<li><p><code>lib.objDescr(obj, options)</code></p>
<p> Return an object structure as a string object by showing primitive properties only,
for arrays it shows the length and <code>options.count</code> or 15 first items,
strings are limited by <code>options.length</code> or 256 bytes, if truncated the full string length is shown.
the object depth is limited by <code>options.depth</code> or 5 levels deep, the number of properties are limited by options.count or 15,
all properties that match <code>options.ignore</code> will be skipped from the output, if <code>options.allow</code> is a regexp, only properties that
match it will be output. Use <code>options.replace</code> for replacing anything in the final string.</p>
</li>
</ul>

<ul>
<li><p><code>lib.stringify(obj, filter)</code></p>
<p> JSON stringify without exceptions, on error just returns an empty string and logs the error</p>
</li>
</ul>

<ul>
<li><p><code>lib.jsonFormat(obj, options)</code></p>
<p> Nicely format an object with indentations, optional <code>indentlevel</code> can be used to control until which level deep
to use newlines for objects.</p>
</li>
</ul>

<ul>
<li><p><code>lib.jsonParse(obj, options)</code></p>
<p> Silent JSON parse, returns null on error, no exceptions raised.</p>
<p>options can specify the following properties:</p>
<ul>
<li>datatype - make sure the result is returned as type: obj, list, str</li>
<li>logger - report in the log with the specified level, log, debug, ...</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>lib.xmlParse(obj, options)</code></p>
<p> Same arguments as for <code>jsonParse</code></p>
</li>
</ul>

<ul>
<li><p><code>lib.moveFile(src, dst, overwrite, callback)</code></p>
<p> Copy file and then remove the source, do not overwrite existing file</p>
</li>
</ul>

<ul>
<li><p><code>lib.copyFile(src, dst, overwrite, callback)</code></p>
<p> Copy file, overwrite is optional flag, by default do not overwrite</p>
</li>
</ul>

<ul>
<li><p><code>lib.execProcess(cmd, callback)</code></p>
<p> Run the process and return all output to the callback, this a simply wrapper around child_processes.exec so the lib.runProcess
can be used without importing the child_processes module. All fatal errors are logged.</p>
</li>
</ul>

<ul>
<li><p><code>lib.spawnProcess(cmd, args, options, callback)</code></p>
<p> Run specified command with the optional arguments, this is similar to child_process.spawn with callback being called after the process exited</p>
<p>Example</p>
<pre><code>    lib.spawProcess(&quot;ls&quot;, &quot;-ls&quot;, { cwd: &quot;/tmp&quot; }, lib.log)</code></pre></li>
</ul>

<ul>
<li><p><code>lib.spawnSeries(cmds, options, callback)</code></p>
<p> Run a series of commands, <code>cmds</code> is an object where a property name is a command to execute and the value is an array of arguments or null.
if <code>options.error</code> is 1, then stop on first error or if non-zero status on a process exit.</p>
<p>Example:</p>
<pre><code>    lib.spawnSeries({&quot;ls&quot;: &quot;-la&quot;,
                      &quot;ps&quot;: &quot;augx&quot;,
                      &quot;du&quot;: { argv: &quot;-sh&quot;, stdio: &quot;inherit&quot;, cwd: &quot;/tmp&quot; },
                      &quot;uname&quot;: [&quot;-a&quot;] },
                     lib.log)</code></pre></li>
</ul>

<ul>
<li><p><code>lib.statSync(file)</code></p>
<p> Non-exception version, returns empty object,
mtime is 0 in case file does not exist or number of seconds of last modified time
mdate is a Date object with last modified time</p>
</li>
</ul>

<ul>
<li><p><code>lib.readFileSync(file, options)</code></p>
<p> Return contents of a file, empty if not exist or on error.</p>
<p>Options can specify the format:</p>
<ul>
<li>json - parse file as JSON, return an object, in case of error an empty object</li>
<li>xml - parse the file as XML, return an object</li>
<li>list - split contents with the given separator</li>
<li>encoding - file encoding when converting to string</li>
<li>logger - log level for error messages</li>
<li>missingok - if set ENOENT will not be logged</li>
<li>offset - read from the position in the file, if negative the offset is from the end of file</li>
<li>length - read only this much of the data, otherwise read till the end of file</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>lib.readFile(file, options, callback)</code></p>
<p> Same as <code>lib.readFileSync</code> but asynchronous</p>
</li>
</ul>

<ul>
<li><p><code>lib.findFilter(file, stat, options)</code></p>
<p> Filter function to be used in findFile methods</p>
</li>
</ul>

<ul>
<li><p><code>lib.findFileSync(file, options)</code></p>
<p> Return list of files than match filter recursively starting with given path, file is the starting path.</p>
<p>The options may contain the following:</p>
<ul>
<li>include - a regexp with file pattern to include</li>
<li>exclude - a regexp with file pattern to exclude</li>
<li>filter - a function(file, stat) that return 1 if the given file matches, stat is a object returned by fs.statSync</li>
<li>depth - if a number it specifies max depth to go into the subfolders, starts with 1</li>
<li>types - a string with types of files to include: d - a dir, f - a file, l - a symlink, c - char dev, b - block dev, s - socket, p - a FIFO</li>
<li>base - if set only keep base file name in the result, not full path</li>
</ul>
<p>Example:</p>
<pre><code>  lib.findFileSync(&quot;modules/&quot;, { depth: 1, types: &quot;f&quot;, include: /\.js$/ }).sort()</code></pre></li>
</ul>

<ul>
<li><p><code>lib.findFile(dir, options, callback)</code></p>
<p> Async version of find file, same options as in the sync version</p>
</li>
</ul>

<ul>
<li><p><code>lib.watchFiles(options, fileCallback, endCallback)</code></p>
<p> Watch files in a dir for changes and call the callback, the parameters:</p>
<ul>
<li>root - a string with root path</li>
<li>files - a regexp to watch files individually, if omitted watch the whole dir</li>
<li>match - a regexp to watch files when using the whole dir, only for matched files the callback will be called</li>
<li>ignore - a regexp to ignore files</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>lib.makePathSync(dir)</code></p>
<p> Recursively create all directories, return 1 if created or 0 on error or if exists, no exceptions are raised, error is logged only</p>
</li>
</ul>

<ul>
<li><p><code>lib.makePath(dir, callback)</code></p>
<p> Async version of makePath, stops on first error</p>
</li>
</ul>

<ul>
<li><p><code>lib.unlink(name, callback)</code></p>
<p> Unlink a file, no error on non-existent file, callback is optional</p>
</li>
</ul>

<ul>
<li><p><code>lib.unlinkPath(dir, callback)</code></p>
<p> Recursively remove all files and folders in the given path, returns an error to the callback if any</p>
</li>
</ul>

<ul>
<li><p><code>lib.unlinkPathSync(dir)</code></p>
<p> Recursively remove all files and folders in the given path, stops on first error</p>
</li>
</ul>

<ul>
<li><p><code>lib.chownSync(uid, gid)</code></p>
<p> Change file owner, multiples files can be specified, do not report errors about non existent files, the uid/gid must be set to non-root user
for this function to work and it is called by the root only, all the rest of the arguments are used as files names</p>
<p>Example:</p>
<pre><code>     lib.chownSync(1, 1, &quot;/path/file1&quot;, &quot;/path/file2&quot;)</code></pre></li>
</ul>

<ul>
<li><p><code>lib.mkdirSync()</code></p>
<p> Create a directories if do not exist, multiple dirs can be specified, all preceeding directories are not created</p>
<p>Example:</p>
<pre><code>       lib.mkdirSync(&quot;dir1&quot;, &quot;dir2&quot;)</code></pre></li>
</ul>

<ul>
<li><p><code>lib.Pool(options)</code></p>
<p> Create a resource pool, <code>create</code> and <code>close</code> callbacks must be given which perform allocation and deallocation of the resources like db connections.</p>
<p>Options defines the following properties:</p>
<ul>
<li>create - method to be called to return a new resource item, takes 1 argument, a callback as <code>function(err, item)</code></li>
<li>destroy - method to be called to destroy a resource item</li>
<li>reset - method to bec alled just before releasing an item back to the resource pool, this is a chance to reset the item to the initial state</li>
<li>validate - method to verify actibe resource item, return false if it needs to be destroyed</li>
<li>min - min number of active resource items</li>
<li>max - max number of active resource items</li>
<li>max_queue - how big the waiting queue can be, above this all requests will be rejected immediately</li>
<li>timeout - number of milliseconds to wait for the next available resource item, cannot be 0</li>
<li>idle - number of milliseconds before starting to destroy all active resources above the minimum, 0 to disable.</li>
</ul>
<p>If no create implementation callback is given then all operations are basically noop but still cals the callbacks.</p>
<p>Example:</p>
<pre><code>  var pool = new lib.Pool({ min: 1, max: 5,
                            create: function(cb) {
                               someDb.connect(function(err) { cb(err, this) }
                            },
                            destroy: function(client) {
                               client.close() }
                            })

  pool.aquire(function(err, client) {
     ...
     client.findItem....
     ...
     pool.release(client);

  });</code></pre></li>
</ul>

<ul>
<li><p><code>lib.Pool.prototype.init(options)</code></p>
<p> Initialize pool properties, this can be run anytime even on the active pool to override some properties</p>
</li>
</ul>

<ul>
<li><p><code>lib.Pool.prototype.acquire(callback)</code></p>
<p> Return next available resource item, if not available immediately wait for defined amount of time before calling the
callback with an error. The callback second argument is active resource item.</p>
</li>
</ul>

<ul>
<li><p><code>lib.Pool.prototype.destroy(item, callback)</code></p>
<p> Destroy the resource item calling the provided close callback</p>
</li>
</ul>

<ul>
<li><p><code>lib.Pool.prototype.release(item)</code></p>
<p> Return the resource item back to the list of available resources.</p>
</li>
</ul>

<ul>
<li><p><code>lib.Pool.prototype.destroyAll()</code></p>
<p> Close all active items</p>
</li>
</ul>

<ul>
<li><p><code>lib.Pool.prototype.stats()</code></p>
<p> Return an object with stats</p>
</li>
</ul>

<ul>
<li><p><code>lib.Pool.prototype.shutdown(callback, maxtime)</code></p>
<p> Close all connections and shutdown the pool, no more items will be open and the pool cannot be used without re-initialization,
if callback is provided then wait until all items are released and call it, optional maxtime can be used to retsrict how long to wait for
all items to be released, when expired the callback will be called</p>
</li>
</ul>

<ul>
<li><p><code>lib.Pool.prototype._call(name, callback)</code></p>
<p> Call registered method and catch exceptions, pass it to the callback if given</p>
</li>
</ul>

<ul>
<li><p><code>lib.Pool.prototype._timer()</code></p>
<p> Timer to ensure pool integrity</p>
</li>
</ul>

<h2 id="module-logger">Module: LOGGER</h2>
<ul>
<li><p><code>logger</code></p>
<p> Simple logger utility for debugging</p>
</li>
</ul>

<ul>
<li><p><code>logger.registerLevel(level, callback, options)</code></p>
<p> Register a custom level handler, must be invoked via <code>logger.logger</code> only, if no handler registered for given level
the whole message will be logger as an error. The custom hadnler is called in the context of the module which means
the options are available inside the handler.</p>
<p>The following properties are supported automatically:</p>
<ul>
<li>format - if 1 then all arguments will be formatted into one line as for the regular levels and passed
the handler as one argument, this is to support different transport and preserve the same standard logging format</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>logger.setSyslog (on)</code></p>
<p> Set or close syslog mode</p>
</li>
</ul>

<ul>
<li><p><code>logger.setFile(file, options)</code></p>
<p> Redirect logging into file</p>
</li>
</ul>

<ul>
<li><p><code>logger.setLevel(level)</code></p>
<p> Set the output level, it can be a number or one of the supported level names</p>
</li>
</ul>

<ul>
<li><p><code>logger.setDebugFilter(str)</code></p>
<p> Enable debugging level for this label, if used with the same debugging level it will be printed regardless of the global level,
a label is first argument to the <code>logger.debug</code> methods, it is used as is, usually the fist argument is
the current function name with comma, like <code>logger.debug(&quot;select:&quot;, name, args)</code></p>
</li>
</ul>

<ul>
<li><p><code>logger.printSyslog(level, msg)</code></p>
<p> syslog allows facility to be specified after log level like info:local0 for LOG_LOCAL0</p>
</li>
</ul>

<ul>
<li><p><code>logger.errorWithOptions(err, options)</code></p>
<p> Prints the given error and the rest of the arguments, the logger level to be used is determined for the given error by code,
uses <code>options</code> or <code>options.logger_error</code> as the level if a string,
if <code>options.logger_error</code> is an object, extract the level by <code>err.code</code> or use <code>*</code> as the default level for not matched codes,
the default is to use the <code>error</code> level.
In case the level is notice or info the error will only show status/code/message properties in order not to print stack trace</p>
</li>
</ul>

<ul>
<li><p><code>logger.trace()</code></p>
<p> Print stack backtrace as error</p>
</li>
</ul>

<ul>
<li><p><code>logger.logger()</code></p>
<p> A generic logger method, safe, first arg is supposed to be a logging level, if not valid the error level is used</p>
</li>
</ul>

<ul>
<li><p><code>logger.print()</code></p>
<p> Default write handler</p>
</li>
</ul>

<ul>
<li><p><code>logger.write(str)</code></p>
<p> Stream emulation</p>
</li>
</ul>

<h2 id="module-metrics">Module: METRICS</h2>
<ul>
<li><p><code>TokenBucket.prototype.configure(rate, max, interval)</code></p>
<p> Initialize existing token with numbers for rate calculations</p>
</li>
</ul>

<ul>
<li><p><code>TokenBucket.prototype.toJSON()</code></p>
<p> Return a JSON object to be serialized/saved</p>
</li>
</ul>

<ul>
<li><p><code>TokenBucket.prototype.toString()</code></p>
<p> Return a string to be serialized/saved</p>
</li>
</ul>

<ul>
<li><p><code>TokenBucket.prototype.toArray()</code></p>
<p> Return an array object to be serialized/saved</p>
</li>
</ul>

<ul>
<li><p><code>TokenBucket.prototype.equal(rate, max, interval)</code></p>
<p> Return true if this bucket uses the same rates in arguments</p>
</li>
</ul>

<ul>
<li><p><code>TokenBucket.prototype.consume(tokens)</code></p>
<p> Consume N tokens from the bucket, if no capacity, the tokens are not pulled from the bucket.</p>
<p>Refill the bucket by tracking elapsed time from the last time we touched it.</p>
<pre><code>min(totalTokens, current + (fillRate * elapsedTime))</code></pre></li>
</ul>

<ul>
<li><p><code>TokenBucket.prototype.delay(tokens)</code></p>
<p> Returns number of milliseconds to wait till number of tokens can be available again</p>
</li>
</ul>

<h2 id="module-msg">Module: MSG</h2>
<ul>
<li><p><code>Msg.prototype.init(options, callback)</code></p>
<p> Initialize supported notification services, it supports jobs arguments convention so can be used in the jobs that
need to send push notifications in the worker process.</p>
</li>
</ul>

<ul>
<li><p><code>Msg.prototype.shutdown(options, callback)</code></p>
<p> Shutdown notification services, wait till all pending messages are sent before calling the callback</p>
</li>
</ul>

<ul>
<li><p><code>Msg.prototype.send(device, options, callback)</code></p>
<p> Deliver a notification for the given device token(s).</p>
<p>The <code>device</code> is where to send the message to, can be multiple ids separated by , or |.</p>
<p>Options with the following properties:</p>
<ul>
<li>service_id - list of services to use for delivery only: default, sns, apn, gcm</li>
<li>account_id - an account id associated with this token, for debugging and invalid token management</li>
<li>app_id - send to the devices for the given app only, if none matched send to the default device tokens only</li>
<li>msg - text message to send</li>
<li>badge - badge number to show if supported by the service</li>
<li>sound - set to 1 if a sound should be produced on message receive</li>
<li>type - set type of the message, service specific</li>
<li>category - action category for APN</li>
<li>id - send id with the notification, this is application specific data, sent as is</li>
<li>name - notification group name, can be used for grouping multiple messages under this name</li>
<li>url - a launch url for the app, it show associated screen on launch if supported</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>Msg.prototype.parseDevice(device)</code></p>
<p> Parse device URN and returns an object with all parts into separate properties. A device URN can be in the following format:
  [service://]device_token[@app]</p>
<ul>
<li>service is optional, supported types: <code>apn</code>, <code>gcm</code>, <code>sns</code>, the <code>default</code> service uses APN delivery</li>
<li>app is optional and can define an application id which is used by APN for routing to the devices with corresponding APS certificate.</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>Msg.prototype.getClient(dev)</code></p>
<p> Return a client module that supports the given device</p>
</li>
</ul>

<ul>
<li><p><code>Msg.prototype.getConfig(name)</code></p>
<p> Return a list of all config cert/key parameters for the given name.
Each item in the list is an object with the following properties: key, secret, app</p>
</li>
</ul>

<ul>
<li><p><code>Msg.prototype.getAgent(mod, dev)</code></p>
<p> Return an agent for the given module for the given device</p>
</li>
</ul>

<h2 id="module-msg_apn">Module: MSG_APN</h2>
<ul>
<li><p><code>client.check(dev)</code></p>
<p> Returns true if given device is supported by APN</p>
</li>
</ul>

<ul>
<li><p><code>client.init(options)</code></p>
<p> Initialize Apple Push Notification service in the current process, Apple supports multiple connections to the APN gateway but
not too many so this should be called on the dedicated backend hosts, on multi-core servers every spawn web process will initialize a
connection to APN gateway.</p>
</li>
</ul>

<ul>
<li><p><code>client.close(callback)</code></p>
<p> Close APN agent, try to send all pending messages before closing the gateway connection</p>
</li>
</ul>

<ul>
<li><p><code>client.send(dev, options, callback)</code></p>
<p> Send push notification to an Apple device, returns true if the message has been queued.</p>
<p>The options may contain the following properties:</p>
<ul>
<li>msg - message text</li>
<li>badge - badge number</li>
<li>sound - 1, true or a sound file to play</li>
<li>category - a user notification category</li>
<li>alertAction - action to exec per Apple doc, action</li>
<li>launchImage - image to show per Apple doc, launch-image</li>
<li>contentAvailable - content indication per Apple doc, content-available</li>
<li>locKey - localization key per Apple doc, loc-key</li>
<li>locArgs - localization key per Apple doc, loc-args</li>
<li>titleLocKey - localization key per Apple doc, title-loc-key</li>
<li>titleLocArgs - localization key per Apple doc, title-loc-args</li>
<li>id - send id in the user properties</li>
<li>type - set type of the event</li>
<li>url - launch url</li>
<li>payload - an object with additional fileds to send in the message payload</li>
</ul>
</li>
</ul>

<h2 id="module-msg_fcm">Module: MSG_FCM</h2>
<ul>
<li><p><code>client.init(options)</code></p>
<p> Initialize Google Cloud Messaging service to send push notifications to mobile devices</p>
</li>
</ul>

<ul>
<li><p><code>client.close(callback)</code></p>
<p> Close GCM connection, flush the queue</p>
</li>
</ul>

<ul>
<li><p><code>client.send(dev, options, callback)</code></p>
<p> Send push notification to an Android device, return true if queued.</p>
</li>
</ul>

<ul>
<li><p><code>client.retryOnError()</code></p>
<p> Retry on server error, honor Retry-After header if present, use it only on the first error</p>
</li>
</ul>

<h2 id="module-msg_gcm">Module: MSG_GCM</h2>
<ul>
<li><p><code>client.init(options)</code></p>
<p> Initialize Google Cloud Messaging service to send push notifications to mobile devices</p>
</li>
</ul>

<ul>
<li><p><code>client.close(callback)</code></p>
<p> Close GCM connection, flush the queue</p>
</li>
</ul>

<ul>
<li><p><code>client.send(dev, options, callback)</code></p>
<p> Send push notification to an Android device, return true if queued.</p>
</li>
</ul>

<ul>
<li><p><code>client.retryOnError()</code></p>
<p> Retry on server error, honor Retry-After header if present, use it only on the first error</p>
</li>
</ul>

<h2 id="module-server">Module: SERVER</h2>
<ul>
<li><p><code>server.start()</code></p>
<p> Start the server process, call the callback to perform some initialization before launchng any server, just after core.init</p>
</li>
</ul>

<ul>
<li><p><code>server.startMonitor(options)</code></p>
<p> Start process monitor, running as root</p>
</li>
</ul>

<ul>
<li><p><code>server.startMaster(options)</code></p>
<p> Setup worker environment</p>
</li>
</ul>

<ul>
<li><p><code>server.startWeb(options)</code></p>
<p> Create Express server, setup worker environment, call supplied callback to set initial environment</p>
</li>
</ul>

<ul>
<li><p><code>server.startWebProcess()</code></p>
<p> Spawn web server from the master as a separate master with web workers, it is used when web and master processes are running on the same server</p>
</li>
</ul>

<ul>
<li><p><code>server.handleChildProcess(child, type, method)</code></p>
<p> Setup exit listener on the child process and restart it</p>
</li>
</ul>

<ul>
<li><p><code>server.startProcess()</code></p>
<p> Restart the main process with the same arguments and setup as a monitor for the spawn child</p>
</li>
</ul>

<ul>
<li><p><code>server.startWatcher()</code></p>
<p> Watch source files for modifications and restart</p>
</li>
</ul>

<ul>
<li><p><code>server.startDaemon()</code></p>
<p> Create daemon from the current process, restart node with -daemon removed in the background</p>
</li>
</ul>

<ul>
<li><p><code>server.onProcessExit()</code></p>
<p> Kill all child processes on exit</p>
</li>
</ul>

<ul>
<li><p><code>server.onProcessTerminate()</code></p>
<p> Terminates the server process, it is called on SIGTERM signal but can be called manually for graceful shitdown,
it runs <code>shutdown[Role]</code> methods before exiting</p>
</li>
</ul>

<ul>
<li><p><code>server.writePidfile()</code></p>
<p> Create a pid file for the current process</p>
</li>
</ul>

<ul>
<li><p><code>server.shutdown(options, callback)</code></p>
<p> Shutdown the system immediately, mostly to be used in the remote jobs as the last task</p>
</li>
</ul>

<ul>
<li><p><code>server.respawn(callback)</code></p>
<p> If respawning too fast, delay otherwise schedule new process after short timeout</p>
</li>
</ul>

<ul>
<li><p><code>server.spawnProcess(args, skip, opts)</code></p>
<p> Start new process reusing global process arguments, args will be added and args in the skip list will be removed</p>
</li>
</ul>

<h2 id="module-bk_account">Module: BK_ACCOUNT</h2>
<ul>
<li><p><code>accounts</code></p>
<p> Account management</p>
</li>
</ul>

<ul>
<li><p><code>Database tables</code></p>
<pre><code>      // Basic account information
      bk_account: {
          id: { primary: 1, pub: 1 },
          login: {},
          status: { type: &quot;text&quot; },
          type: { type: &quot;text&quot;, list: 1, admin: 1 },
          name: { pub: 1, notempty: 1 },
          first_name: { pub: 1 },
          last_name: { pub: 1 },
          email: { type: &quot;email&quot; },
          phone: { type: &quot;phone&quot; },
          website: {},
          company: {},
          birthday: { type: &quot;string&quot; },
          gender: {},
          street: {},
          city: {},
          county: {},
          state: {},
          zipcode: {},
          country: {},
          device_id: {},                            // Device(s) for notifications the format is: [service://]token[@appname]
          ctime: { type: &quot;now&quot;, readonly: 1 },      // Create time
          mtime: { type: &quot;now&quot; },                   // Last update time
      },
  },</code></pre><p>  };
  module.exports = accounts;</p>
<p>  // Initialize the module
  accounts.init = function(options)
  {</p>
<pre><code>  db.describeTables();</code></pre><p>  }</p>
<p>  accounts.configureMdule = function(options, callback)
  {</p>
<pre><code>  db.setProcessRow(&quot;post&quot;, &quot;bk_account&quot;, function(req, row) {
      if (row.birthday) {
          row.age = Math.floor((Date.now() - lib.toDate(row.birthday))/(86400000*365));
      }
      // If only used as alias then split manually
      if (row.name &amp;&amp; !row.first_name) {
          var name = row.name.split(&quot; &quot;);
          if (name.length &gt; 1) row.last_name = name.pop();
          row.first_name = name.join(&quot; &quot;);
      }</code></pre></li>
</ul>

<ul>
<li><p><code>accounts.configureWeb(options, callback)</code></p>
<p> Create API endpoints and routes</p>
</li>
</ul>

<ul>
<li><p><code>accounts.configureAccountsAPI()</code></p>
<p> Account management</p>
</li>
</ul>

<ul>
<li><p><code>accounts.getAccount(req, options, callback)</code></p>
<p> Returns current account, used in /account/get API call, req.account will be filled with the properties from the db</p>
</li>
</ul>

<ul>
<li><p><code>accounts.notifyAccount(options, callback)</code></p>
<p> Send Push notification to the account. The delivery is not guaranteed, if the message was queued for delivery, no errors will be returned.</p>
<p>The options may contain the following:</p>
<ul>
<li>account_id - REQUIRED, the account to who to send the notification</li>
<li>msg - message text to send</li>
<li>badge - a badge number to be sent</li>
<li>allow - the account properties to check if notifications are enabled, it must be an object with properties in the account record and values to
be a regexp, each value if starts with &quot;!&quot; means not equal, see <code>lib.isMatched</code></li>
<li>skip - Array or an object with account ids which should be skipped, this is for mass sending in order to reuse the same options</li>
<li>enable - the account properties to check if a notification parameter must be set or removed, an account property must be a boolean and
if false then a notification parameter is removed and if true a notification parameter is set to 1</li>
<li>logging - logging level about the notification send status, default is debug, can be any valid logger level, must be a string, not a number</li>
<li>service_id - name of the standard delivery service supported by the backend, it is be used instead of custom handler, one of the following: apple, google</li>
<li>app_id - the application specific device tokens should be used only or if none matched the default device tokens</li>
<li>device_id - the device to send the message to instesd of the device_id property fro the account record</li>
</ul>
<p>In addition the device_id can be saved in the format service://id where the service is one of the supported delivery services, this way the notification
system will pick the right delivery service depending on the device id, the default service is apple.</p>
<p>Example:</p>
<pre><code> bk_account.notifyAccount({ account_id: &quot;123&quot;, msg: &quot;test&quot;, badge: 1, sound: 1, allow: { notifications0: 1, type: &quot;user&quot; }, enable: { sound: &quot;sound0&quot;, badge: &quot;badge0&quot; } })</code></pre></li>
</ul>

<ul>
<li><p><code>accounts.listAccount(rows, options, callback)</code></p>
<p> Return account details for the list of rows, <code>options.account_key</code> specified the column to use for the account id in the <code>rows</code>, or <code>id</code> will be used.
The result accounts are cleaned for public columns, all original properties from the <code>rows</code> are kept as is.
If options.existing is 1 then return only record with found accounts, all other records in the rows will be deleted</p>
</li>
</ul>

<ul>
<li><p><code>accounts.selectAccount(req, options, callback)</code></p>
<p> Query accounts, used in /accout/select API call, simple wrapper around db.select but can be replaced in the apps while using the same API endpoint</p>
</li>
</ul>

<ul>
<li><p><code>accounts.addAccount(req, options, callback)</code></p>
<p> Register new account, used in /account/add API call, but the req does not have to be an Express request, it just
need to have query and options objects.</p>
</li>
</ul>

<ul>
<li><p><code>accounts.updateAccount(req, options, callback)</code></p>
<p> Update existing account, used in /account/update API call</p>
</li>
</ul>

<ul>
<li><p><code>accounts.deleteAccount(req, callback)</code></p>
<p> Delete account specified by the obj. Used in <code>/account/del</code> API call.
The options may contain <code>keep</code> array with tables to be kept, for example
delete an account but keep all messages and location: keep:[&quot;bk_message&quot;,&quot;bk_location&quot;]</p>
<p>This methods is suitable for background jobs</p>
</li>
</ul>

<ul>
<li><p><code>accounts.renameAccount(req, callback)</code></p>
<p> Rename account alias</p>
</li>
</ul>

<ul>
<li><p><code>accounts.fetchAccount(query, options, callback)</code></p>
<p> Override OAuth account management</p>
</li>
</ul>

<h2 id="module-bk_data">Module: BK_DATA</h2>
<ul>
<li><p><code>mod</code></p>
<p> Account management</p>
</li>
</ul>

<ul>
<li><p><code>mod.configureWeb(options, callback)</code></p>
<p> Create API endpoints and routes</p>
</li>
</ul>

<ul>
<li><p><code>mod.configureDataAPI()</code></p>
<p> API for full access to all tables</p>
</li>
</ul>

<h2 id="module-bk_dynamodbstreams">Module: BK_DYNAMODBSTREAMS</h2>
<ul>
<li><p><code>mod.processShardStart(req, options, callback)</code></p>
<p> Get the last processed sequence for the shard</p>
</li>
</ul>

<ul>
<li><p><code>mod.processRecords(req, options, callback)</code></p>
<p> Commit a batch of shard records processed</p>
</li>
</ul>

<h2 id="module-bk_file">Module: BK_FILE</h2>
<ul>
<li><p><code>files</code></p>
<p> Account management</p>
</li>
</ul>

<ul>
<li><p><code>files.configureWeb(options, callback)</code></p>
<p> Create API endpoints and routes</p>
</li>
</ul>

<ul>
<li><p><code>files.configureFilesAPI()</code></p>
<p> Generic file management</p>
</li>
</ul>

<h2 id="module-bk_icon">Module: BK_ICON</h2>
<ul>
<li><p><code>mod</code></p>
<p> Icons management</p>
</li>
</ul>

<ul>
<li><p><code>Database tables</code></p>
<pre><code>      bk_icon: {
          id: { primary: 1 },                         // account id
          type: {                                     // prefix:type
              primary: 1,
              pub: 1,
              join: [ &quot;prefix&quot;, &quot;type&quot; ],
              unjoin: 1,
              separator: &quot;:&quot;,
              ops: { select: &quot;begins_with&quot; }
          },
          prefix: {},                                 // icon prefix/namespace
          descr: { pub: 1 },                          // user provided caption
          acl_allow: {},                              // who can see it: all, auth, id:id...
          tags: { type: &quot;list&quot; },                     // detected or attached tags
          width: { type: &quot;int&quot; },
          height: { type: &quot;int&quot; },
          rotation: { type: &quot;int&quot; },                  // rotation angle
          ext: {},                                    // saved image extension
          latitude: { type: &quot;real&quot; },
          longitude: { type: &quot;real&quot; },
          mtime: { type: &quot;now&quot; },
      },

  },
  limit: { &quot;*&quot;: 0 },
  controls: {
      width: { type: &quot;number&quot; },
      height: { type: &quot;number&quot; },
      rotate: { type: &quot;number&quot; },
      quality: { type: &quot;number&quot; },
      brightness: { type: &quot;number&quot; },
      contrast: { type: &quot;number&quot; },
      bgcolor: { type: &quot;string&quot; },
      verify: { type: &quot;bool&quot; },
      extkeep: { type: &quot;regexp&quot; },
      autodel: { type: &quot;bool&quot; },
  }</code></pre><p>  };
  module.exports = mod;</p>
<p>  // Initialize the module
  mod.init = function(options)
  {</p>
<pre><code>  core.describeArgs(&quot;icons&quot;, [
       { name: &quot;limit&quot;, type: &quot;map&quot;, datatype: &quot;int&quot;, descr: &quot;Set the limit of how many icons by type can be uploaded by an account, type:N,type:N..., type * means global limit for any icon type&quot; },
  ]);</code></pre><p>  }</p>
<p>  mod.configureMiddleware = function(options, callback)
  {</p>
<pre><code>  api.registerControlParams(mod.controls);
  callback();</code></pre><p>  }</p>
<p>  // Create API endpoints and routes
  mod.configureWeb = function(options, callback)
  {</p>
<pre><code>  this.configureIconsAPI();
  callback()</code></pre><p>  }</p>
<p>  // Generic icon management
  mod.configureIconsAPI = function()
  {</p>
<pre><code>  api.app.all(/^\/icon\/([a-z]+)$/, function(req, res) {
      var options = api.getOptions(req);
      options.cleanup = &quot;bk_icon&quot;;

      if (!req.query.prefix) return api.sendReply(res, 400, &quot;prefix is required&quot;);
      if (!req.query.id) req.query.id = req.account.id;
      if (!req.query.type) req.query.type = &quot;&quot;;
      switch (req.params[0]) {
      case &quot;get&quot;:
          mod.send(req, options);
          break;

      case &quot;select&quot;:
          mod.select(req.query, function(err, rows) {
              api.sendJSON(req, err, rows);</code></pre></li>
</ul>

<ul>
<li><p><code>mod.upload(req, options, callback)</code></p>
<p> Process icon request, put or del, update table and deal with the actual image data, always overwrite the icon file
Verify icon limits before adding new icons</p>
</li>
</ul>

<ul>
<li><p><code>mod.select(query, options, callback)</code></p>
<p> Return list of icons for the account, used in /icon/get API call</p>
</li>
</ul>

<ul>
<li><p><code>mod.del(options, callback)</code></p>
<p> Delete an icon, only one icon at a time, options must profile id, prefix. It will try to delete
an icon file even if there is no record in the bk_icon table.</p>
</li>
</ul>

<ul>
<li><p><code>mod.send(req, options)</code></p>
<p> Return icon to the client, checks the bk_icon table for existence and permissions</p>
</li>
</ul>

<h2 id="module-bk_message">Module: BK_MESSAGE</h2>
<ul>
<li><p><code>mod</code></p>
<p> Messages management</p>
</li>
</ul>

<ul>
<li><p><code>Database tables</code></p>
<pre><code>      // New messages, the inbox
      bk_message: {
          id: { primary: 1 },                           // my account_id
          mtime: {
              primary: 1,                               // mtime:sender
              join: [&quot;mtime&quot;,&quot;sender&quot;],
              unjoin: 1,
          },
          sender: { type: &quot;text&quot; },                      // sender id
          name: {},                                      // sender name
          msg: {},                                       // Text of the message
          icon: { type: &quot;int&quot; },                         // 1 - icon present, 0 - no icon
          icon_type: {},                                 // png, gif, jpg
          read: { type: &quot;int&quot; },                         // 1 - read, 0 - unread
          flags: { type: &quot;list&quot; },
          ctime: { type: &quot;now&quot;, readonly: 1 },
      },
      // Archived messages
      bk_archive: {
          id: { primary: 1 },                            // my account_id
          mtime: {
              primary: 1,                                // mtime:sender
              join: [&quot;mtime&quot;,&quot;sender&quot;],
              unjoin: 1,
          },
          sender: { type: &quot;text&quot; },                      // sender id
          name: {},                                      // sender name
          msg: {},                                       // text of the message
          icon: { type: &quot;int&quot; },                         // 1 - icon present, 0 - no icon
          icon_type: {},                                 // png, gif, jpg
          flags: { type: &quot;list&quot; },
          ctime: { type: &quot;now&quot;, readonly: 1 },
      },
      // Messages sent
      bk_sent: {
          id: { primary: 1 },                            // my account
          mtime: {
              primary: 1,                                // mtime:recipient
              join: [&quot;mtime&quot;,&quot;recipient&quot;],
              unjoin: 1,
          },
          recipient: { type: &quot;text&quot; },                   // recipient id
          name: {},                                      // recipient name if known
          msg: {},                                       // text of the message
          icon: { type: &quot;int&quot; },                         // 1 - icon present, 0 - no icon
          icon_type: {},                                 // png, gif, jpg
          flags: { type: &quot;list&quot; },
          ctime: { type: &quot;now&quot;, readonly: 1 },
      },
  },
  controls: {
      archive: { type: &quot;bool&quot; },
      trash: { type: &quot;bool&quot; },
      nosent: { type: &quot;bool&quot; },
  },
  cacheOptions: { cacheName: &quot;messages&quot;, ttl: 3600000 },</code></pre><p>  };
  module.exports = mod;</p>
<p>  mod.init = function(options)
  {</p>
<pre><code>  core.describeArgs(mod.name, [
       { name: &quot;cache-name&quot;, obj: &quot;cacheOptions&quot;, descr: &quot;Cache name for keeping unread messages counter&quot; },
       { name: &quot;cache-ttl&quot;, type: &quot;number&quot;, obj: &quot;cacheOptions&quot;, nocamel: 1, key: &quot;ttl&quot;, min: 0, descr: &quot;How long in ms to keep unread messages counter&quot; },
  ]);</code></pre><p>  }</p>
<p>  mod.configureMiddleware = function(options, callback)
  {</p>
<pre><code>  api.registerControlParams(mod.controls);
  callback();</code></pre><p>  }</p>
<p>  mod.configureModule = function(options, callback)
  {</p>
<pre><code>  db.setProcessRow(&quot;post&quot;, &quot;bk_message&quot;, function(req, row) {
      if (row.icon) row.icon = api.iconUrl({ prefix: &#39;message&#39;, id: row.id, type: row.mtime + &quot;:&quot; + row.sender, ext: row.icon_type }); else delete row.icon;</code></pre></li>
</ul>

<ul>
<li><p><code>mod.configureWeb(options, callback)</code></p>
<p> Create API endpoints and routes</p>
</li>
</ul>

<ul>
<li><p><code>mod.configureMessagesAPI()</code></p>
<p> Messaging management</p>
</li>
</ul>

<ul>
<li><p><code>mod.getArchiveMessage(req, options, callback)</code></p>
<p> Return archived messages, used in /message/get API call</p>
</li>
</ul>

<ul>
<li><p><code>mod.getSentMessage(req, options, callback)</code></p>
<p> Return sent messages to the specified account, used in /message/get/sent API call</p>
</li>
</ul>

<ul>
<li><p><code>mod.getMessage(req, options, callback)</code></p>
<p> Return new/unread messages, used in /message/get API call</p>
</li>
</ul>

<ul>
<li><p><code>mod.addMessage(req, options, callback)</code></p>
<p> Add new message(s), used in /message/add API call</p>
<p>The following options properties can be used:</p>
<ul>
<li>nosent - do not create a record in the bk_sent table</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>mod.archiveMessage(req, options, callback)</code></p>
<p> Move matched messages to the archive, used in /message/archive API call</p>
</li>
</ul>

<ul>
<li><p><code>mod.delMessage(req, options, callback)</code></p>
<p> Delete matched messages, used in /message/del` API call</p>
</li>
</ul>

<ul>
<li><p><code>mod.delArchiveMessage(req, options, callback)</code></p>
<p> Delete matched messages in the archive, used in /message/del/archive` API call</p>
</li>
</ul>

<ul>
<li><p><code>mod.delSentMessage(req, options, callback)</code></p>
<p> Delete matched messages i sent, used in /message/del/sent` API call</p>
</li>
</ul>

<ul>
<li><p><code>mod.updateMessage(req, options, callback)</code></p>
<p> Update a message or all messages for the given account from the given sender, used in /message/del` API call</p>
</li>
</ul>

<ul>
<li><p><code>mod.updateArchiveMessage(req, options, callback)</code></p>
<p> Update a messages in the archive, used in /message/update/archive` API call</p>
</li>
</ul>

<ul>
<li><p><code>mod.readMessage(req, options, callback)</code></p>
<p> Mark matched messages as read, used in /message/read` API call</p>
</li>
</ul>

<h2 id="module-bk_shell">Module: BK_SHELL</h2>
<ul>
<li><p><code>shell.exit(err, msg)</code></p>
<p> Exit and write to the console a message or error message if non empty</p>
</li>
</ul>

<ul>
<li><p><code>shell.getUser(obj, callback)</code></p>
<p> Resolves a user from <code>obj.id</code> or <code>obj.login</code> params and return the record in the callback</p>
</li>
</ul>

<ul>
<li><p><code>shell.getQuery()</code></p>
<p> Returns an object with all command line params that do not start with dash(-), treat 2 subsequent parms without dashes as name value pair</p>
</li>
</ul>

<ul>
<li><p><code>shell.getQueryList()</code></p>
<p> Returns a list with all command line params that do not start with dash(-), only the trailing arguments will be collected</p>
</li>
</ul>

<ul>
<li><p><code>shell.getArgs(options)</code></p>
<p> Returns an object with all command line params starting with dash set with the value if the next param does not start with dash or 1.
By sefault all args are zatored as is with dahses, if <code>options.camel`` is true then all args will be stored in camel form,
if</code>options.underscor`e is true then all args will be stored with dashes convertyed into underscores.</p>
</li>
</ul>

<ul>
<li><p><code>shell.getOption(name, options)</code></p>
<p> Return an argument by name from the options, options may contain parameters in camel form or with underscores, both formats will be checked</p>
</li>
</ul>

<ul>
<li><p><code>shell.getArg(name, options, dflt)</code></p>
<p> Return first available value for the given name, options first, then command arg and then default,</p>
</li>
</ul>

<ul>
<li><p><code>shell.runShell(options)</code></p>
<p> Start REPL shell or execute any subcommand if specified in the command line.
A subcommand may return special string to indicate how to treat the flow:</p>
<ul>
<li>stop - stop processing commands and create REPL</li>
<li>continue - do not exit and continue processing other commands or end with REPL</li>
<li>all other values will result in returning from the run assuming the command will decide what to do, exit or continue running, no REPL is created</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>shell.cmdShowInfo(options)</code></p>
<p> App version</p>
</li>
</ul>

<ul>
<li><p><code>shell.cmdRunFile(options)</code></p>
<p> Load a module and optionally execute it</p>
<p>Example:</p>
<pre><code>  var bkjs = require(&quot;backendjs&quot;)
  bkjs.app.test = 123;
  exports.run = function() {
      console.log(&quot;run&quot;);
  }
  exports.newMethod = function() {
      console.log(bkjs.core.version, &quot;version&quot;);
  }</code></pre><p>Save into a file a.js and run</p>
<pre><code>  bksh -run-file a.js</code></pre><p>In the shell now it new methods can be executed</p>
<pre><code>  &gt; shell.newMethod()</code></pre></li>
</ul>

<ul>
<li><p><code>shell.cmdRunApi(options)</code></p>
<p> Run API server inside the shell</p>
</li>
</ul>

<ul>
<li><p><code>shell.cmdRunJobs(options)</code></p>
<p> Run jobs workers inside the shell</p>
</li>
</ul>

<ul>
<li><p><code>shell.cmdAccountGet(options)</code></p>
<p> Show account records by id or login</p>
</li>
</ul>

<ul>
<li><p><code>shell.cmdAccountAdd(options)</code></p>
<p> Add a user</p>
</li>
</ul>

<ul>
<li><p><code>shell.cmdAccountDel(options)</code></p>
<p> Delete a user and all its history according to the options</p>
</li>
</ul>

<ul>
<li><p><code>shell.cmdLoginGet(options)</code></p>
<p> Show account records by id or login</p>
</li>
</ul>

<ul>
<li><p><code>shell.cmdLoginAdd(options)</code></p>
<p> Add a user login</p>
</li>
</ul>

<ul>
<li><p><code>shell.cmdLoginUpdate(options)</code></p>
<p> Update a user login</p>
</li>
</ul>

<ul>
<li><p><code>shell.cmdLoginDel(options)</code></p>
<p> Delete a user login</p>
</li>
</ul>

<ul>
<li><p><code>shell.cmdLocationPut(options)</code></p>
<p> Update location</p>
</li>
</ul>

<ul>
<li><p><code>shell.cmdLogWatch(options)</code></p>
<p> Run logwatcher and exit</p>
</li>
</ul>

<ul>
<li><p><code>shell.cmdSendRequest(options)</code></p>
<p> Send API request</p>
</li>
</ul>

<ul>
<li><p><code>},(err, opts)</code></p>
<p> If executed as standalone script directly in the node</p>
</li>
</ul>

<h2 id="module-bk_shell_aws">Module: BK_SHELL_AWS</h2>
<ul>
<li><p><code>shell.awsCheckTags(obj, name)</code></p>
<p> Check all names in the tag set for given name pattern(s), all arguments after 0 are checked</p>
</li>
</ul>

<ul>
<li><p><code>shell.awsFilterSubnets(subnets, zone, name)</code></p>
<p> Return matched subnet ids by availability zone and/or name pattern</p>
</li>
</ul>

<ul>
<li><p><code>shell.awsGetSelfImages(name, callback)</code></p>
<p> Retrieve my AMIs for the given name pattern</p>
</li>
</ul>

<ul>
<li><p><code>shell.awsSearchImage(filter, appName, callback)</code></p>
<p> Return an image that matches given app name latest version</p>
</li>
</ul>

<ul>
<li><p><code>shell.awsGetAmazonImages(options, callback)</code></p>
<p> Return Amazon AMIs for the current region, HVM type only</p>
</li>
</ul>

<ul>
<li><p><code>shell.awsGetElbCount(name, equal, total, options, callback)</code></p>
<p> Wait ELB to have instance count equal or not to the expected total</p>
</li>
</ul>

<ul>
<li><p><code>shell.awsLaunchInstances(options, callback)</code></p>
<p> Launch instances by run mode and/or other criteria</p>
</li>
</ul>

<ul>
<li><p><code>shell.cmdAwsLaunchInstances(options)</code></p>
<p> Delete an AMI with the snapshot</p>
</li>
</ul>

<ul>
<li><p><code>shell.cmdAwsDeleteImage(options)</code></p>
<p> Delete an AMI with the snapshot</p>
</li>
</ul>

<ul>
<li><p><code>shell.cmdAwsCreateImage(options)</code></p>
<p> Create an AMI from the current instance of the instance by id</p>
</li>
</ul>

<ul>
<li><p><code>shell.cmdAwsRebootInstances(options)</code></p>
<p> Reboot instances by run mode and/or other criteria</p>
</li>
</ul>

<ul>
<li><p><code>shell.cmdAwsTerminateInstances(options)</code></p>
<p> Terminate instances by run mode and/or other criteria</p>
</li>
</ul>

<ul>
<li><p><code>shell.cmdAwsShowInstances(options)</code></p>
<p> Show running instances by run mode and/or other criteria</p>
</li>
</ul>

<ul>
<li><p><code>shell.cmdAwsSetRoute53(options)</code></p>
<p> Update a Route53 record with IP/names of all instances specified by the filter</p>
</li>
</ul>

<ul>
<li><p><code>shell.cmdAwsShowElb(options)</code></p>
<p> Show ELB running instances</p>
</li>
</ul>

<ul>
<li><p><code>shell.cmdAwsRebootElb(options)</code></p>
<p> Reboot instances in the ELB, one by one</p>
</li>
</ul>

<ul>
<li><p><code>shell.cmdAwsReplaceElb(options)</code></p>
<p> Deploy new version in the ELB, terminate the old version</p>
</li>
</ul>

<ul>
<li><p><code>shell.cmdAwsSetupSsh(options)</code></p>
<p> Open/close SSH access to the specified group for the current external IP address</p>
</li>
</ul>

<ul>
<li><p><code>shell.cmdAwsSetupInstance(options)</code></p>
<p> Launch an instance and setup it with provisioning script</p>
</li>
</ul>

<ul>
<li><p><code>shell.cmdAwsS3Get(options)</code></p>
<p> Get file</p>
</li>
</ul>

<ul>
<li><p><code>shell.cmdAwsS3Put(options)</code></p>
<p> Put file</p>
</li>
</ul>

<ul>
<li><p><code>shell.cmdAwsS3List(options)</code></p>
<p> List folder</p>
</li>
</ul>

<h2 id="module-bk_shell_db">Module: BK_SHELL_DB</h2>
<ul>
<li><p><code>shell.cmdDbGetConfig(options)</code></p>
<p> Show all config parameters</p>
</li>
</ul>

<ul>
<li><p><code>shell.cmdDbTables(options)</code></p>
<p> Show all tables</p>
</li>
</ul>

<ul>
<li><p><code>shell.cmdDbSelect(options)</code></p>
<p> Show record that match the search criteria, return up to <code>-count N</code> records</p>
</li>
</ul>

<ul>
<li><p><code>shell.cmdDbScan(options)</code></p>
<p> Show all records that match search criteria</p>
</li>
</ul>

<ul>
<li><p><code>shell.cmdDbBackup(options)</code></p>
<p> Save all tables to the specified directory or the server home</p>
</li>
</ul>

<ul>
<li><p><code>shell.cmdDbRestore(options)</code></p>
<p> Restore tables</p>
</li>
</ul>

<ul>
<li><p><code>shell.cmdDbPut(options)</code></p>
<p> Put a record</p>
</li>
</ul>

<ul>
<li><p><code>shell.cmdDbDel(options)</code></p>
<p> Delete a record</p>
</li>
</ul>

<ul>
<li><p><code>shell.cmdDbDelAll(options)</code></p>
<p> Delete all records</p>
</li>
</ul>

<ul>
<li><p><code>shell.cmdDbDrop(options)</code></p>
<p> Drop a table</p>
</li>
</ul>

<h2 id="module-bk_shell_test">Module: BK_SHELL_TEST</h2>
<ul>
<li><p><code>shell.assert(next, err)</code></p>
<p> To be used in the tests, this function takes the following arguments:</p>
<p>assert(next, err, ....)</p>
<ul>
<li>next is a callback to be called after printing error condition if any, it takes err as its argument</li>
<li>err - an error object from the most recent operation, can be null/undefined or any value that results in Javascript &quot;true&quot; evaluation
up to the caller, assertion happens if an err is given or this value is true</li>
<li>all other arguments are printed in case of error or result being false</li>
</ul>
<p>NOTES:</p>
<ul>
<li>In forever mode <code>-test-forever</code> any error is ignored and not reported</li>
<li>if <code>tests.test.delay</code> is set it will be used to delay calling the next callback and reset, this is for
one time delays.</li>
</ul>
<p>Example</p>
<pre><code>    function(next) {
        db.get(&quot;bk_account&quot;, { id: &quot;123&quot; }, function(err, row) {
            tests.assert(next, err || !row || row.id != &quot;123&quot;, &quot;Record not found&quot;, row)
        });
    }</code></pre></li>
</ul>

<ul>
<li><p><code>shell.cmdTestRun(options)</code></p>
<p> Run the test function which is defined in the tests module, all arguments will be taken from the options or the command line. Options
use the same names as command line arguments without preceeding <code>test-</code> prefix.</p>
<p>The common command line arguments that supported:</p>
<ul>
<li>-test-run - name of the function to run</li>
<li>-test-delay - number of milliseconds before starting the test</li>
<li>-test-workers - number of workers to run the test at the same time</li>
<li>-test-workers-delay - number of milliseconds before starting worker processes, default is 500ms</li>
<li>-test-timeout - number of milliseconds between test steps, i.e. between invocations of the check</li>
<li>-test-interval - number of milliseconds between iterations</li>
<li>-test-iterations - how many times to run this test function, default is 1</li>
<li>-test-forever - run forever without reporting any errors, for performance testing</li>
<li>-test-file - a javascript file to be loaded with additional tests</li>
</ul>
<p>All other common command line arguments are used normally, like -db-pool to specify which db to use.</p>
<p>After finish or in case of error the process exits if no callback is given.</p>
<p>Example, store it in tests/tests.js:</p>
<pre><code>    var bkjs = require(&quot;backendjs&quot;);
    var tests = bkjs.core.modules.tests;

    tests.test_mytest = function(next) {
       bkjs.db.get(&quot;bk_account&quot;, { id: &quot;123&quot; }, function(err, row) {
           tests.assert(next, err || !row || row.id != &quot;123&quot;, &quot;Record not found&quot;, row)
       });
    }

    # bksh -test-run mytest</code></pre><p>Custom tests:</p>
<ul>
<li><p>create a user for backend testing, if the API does not require authentication skip this step:</p>
<pre><code>  ./app.sh -shell -account-add login testuser secret testpw</code></pre></li>
<li><p>configure global backend credentials</p>
<pre><code>  echo &quot;backend-login=testuser&quot; &gt;&gt; etc/config.local
  echo &quot;backend-secret=testpw&quot; &gt;&gt; etc/config.local</code></pre></li>
<li><p>to start a test command in the shell using local ./tests.js</p>
<pre><code>./app.sh -shell -test-run account</code></pre></li>
<li><p>to start a test command in the shell using custom file with tests</p>
<pre><code>./app.sh -shell -test-run api -test-file tests/api.js</code></pre></li>
</ul>
</li>
</ul>

<h2 id="module-bk_status">Module: BK_STATUS</h2>
<ul>
<li><p><code>mod</code></p>
<p> Status management</p>
</li>
</ul>

<ul>
<li><p><code>Database tables</code></p>
<pre><code>      bk_status: {
          id: { primary: 1, pub: 1 },                       // account id
          name: { pub: 1 },                                 // account name
          status: { pub: 1 },                               // status, online, offline, away
          atime: { type: &quot;bigint&quot;, pub: 1 },                // last access time
          mtime: { type: &quot;now&quot;, pub: 1 },                   // last update time
      },
  },
  // Intervals between updating presence status table
  args: [
      { name: &quot;interval&quot;, type: &quot;number&quot;, min: 60000, max: 86400000, descr: &quot;Max idle period in milliseconds after which the status will be considered offline&quot; },
      { name: &quot;update-interval&quot;, type: &quot;number&quot;, min: 60000, max: 86400000, descr: &quot;Period in milliseconds between database flushing updates in the case when bk_status is cached&quot; },
  ],
  interval: 60000,
  updateInterval: 180000,</code></pre><p>  };
  module.exports = mod;</p>
<p>  // Returns status record for given account, used in /status/get API call.
  // It always returns status object even if it was never set before, on return the record contains
  // a property <code>online</code> set to true of false according to the idle period and actual status.
  //
  // If status was explicitely set to <code>offline</code> then it is considered offline until changed to other value,
  // for other cases <code>status</code> property is not used, it is supposed for the application extention.
  //
  mod.get = function(id, options, callback)
  {</p>
<pre><code>  if (typeof options == &quot;function&quot;) callback = options, options = null;
  var now = Date.now();
  db.get(&quot;bk_status&quot;, { id: id }, options, function(err, status, info) {
      if (err) return callback(err);
      if (!status) status = { id: id, status: &quot;&quot;, online: false, atime: 0, mtime: 0 };
      status.online = now - status.atime &lt; mod.interval &amp;&amp; status.status != &quot;offline&quot; ? true : false;
      status._cached = info.cached;
      callback(err, status);</code></pre></li>
</ul>

<ul>
<li><p><code>mod.select(ids, options, callback)</code></p>
<p> Return status records for specified list of account ids.</p>
</li>
</ul>

<ul>
<li><p><code>mod.update(status, options, callback)</code></p>
<p> Maintain online status, update to db every status-interval seconds, only update the db if last update happened
longer than <code>status-interval</code> milliseconds ago, keep atime up-to-date in the cache on every status update.</p>
<p>On return and if it was flushed to db the <code>atime</code> will be equal to <code>mtime</code>.</p>
<p><em>NOTE: All properties from the <code>obj</code> will be saved in the bk_status record, existing properties will be overriden</em></p>
</li>
</ul>

<ul>
<li><p><code>mod.bkDeleteAccount(req, callback)</code></p>
<p> This methods is suitable for background jobs</p>
</li>
</ul>

<h2 id="module-bk_system">Module: BK_SYSTEM</h2>
<ul>
<li><p><code>system</code></p>
<p> System management</p>
</li>
</ul>

<ul>
<li><p><code>system.init(options)</code></p>
<p> Initialize the module</p>
</li>
</ul>

<ul>
<li><p><code>system.configureWeb(options, callback)</code></p>
<p> Create API endpoints and routes</p>
</li>
</ul>

<ul>
<li><p><code>system.configureSystemAPI()</code></p>
<p> API for internal provisioning and configuration</p>
</li>
</ul>

<h2 id="module-db_cassandra">Module: DB_CASSANDRA</h2>
<ul>
<li><p><code>Pool.prototype.put(table, obj, options, callback)</code></p>
<p> No REPLACE INTO support but UPDATE creates new record if no primary key exists</p>
</li>
</ul>

<h2 id="module-db_couchdb">Module: DB_COUCHDB</h2>
<ul>
<li><p><code>pool</code></p>
<p> Create a database pool that works with CouchDB server.</p>
<p>In addition to the standard commands it can execute any CouchDB HTTP API directly</p>
<pre><code>db.query({ op: &quot;GET&quot;, text: &quot;/db/url&quot; }, { pool: &quot;couchdb&quot; }, lib.log)
db.query({ op: &quot;PUT&quot;, text: &quot;/db/url&quot;, obj: { a: 1 b: 2 } }, { pool: &quot;couchdb&quot; }, lib.log)</code></pre></li>
</ul>

<h2 id="module-db_lmdb">Module: DB_LMDB</h2>
<ul>
<li><p><code>pool</code></p>
<p> Setup LMDB/LevelDB database driver, this is simplified driver which supports only basic key-value operations,
table parameter is ignored, the object only supports the properties name and value in the record objects.</p>
<p>Because this driver supports 2 databases it requires type to be specified, possible values are: <code>lmdb, leveldb</code></p>
<p>Options are passed to the LMDB low level driver as MDB_ flags according to <a href="http://symas.com/mdb/doc/">http://symas.com/mdb/doc/</a> and
as properties for LevelDB as described in <a href="http://leveldb.googlecode.com/svn/trunk/doc/index.html">http://leveldb.googlecode.com/svn/trunk/doc/index.html</a></p>
<p>The LevelDB database can only be shared by one process so if no unique options.url is given, it will create a unique database using core.processId()</p>
</li>
</ul>

<h2 id="module-db_mongodb">Module: DB_MONGODB</h2>
<h2 id="module-db_mysql">Module: DB_MYSQL</h2>
<h2 id="module-db_pgsql">Module: DB_PGSQL</h2>
<ul>
<li><p><code>Pool.prototype.put(table, obj, opts, callback)</code></p>
<p> No REPLACE INTO support, do it manually</p>
</li>
</ul>

<ul>
<li><p><code>Pool.prototype.cacheIndexes(options, callback)</code></p>
<p> Cache indexes using the information_schema</p>
</li>
</ul>

<ul>
<li><p><code>Pool.prototype.bindValue(val, info, options)</code></p>
<p> Convert JS array into db PostgreSQL array format: {..}</p>
</li>
</ul>

<h2 id="module-db_redis">Module: DB_REDIS</h2>
<ul>
<li><p><code>pool</code></p>
<p> Redis database pool, uses Hash to store the records</p>
</li>
</ul>

<h2 id="module-db_riak">Module: DB_RIAK</h2>
<ul>
<li><p><code>pool</code></p>
<p> Create a database pool that works with the Riak database.</p>
<p>By default the driver uses simple key-value mode of operations, to enable bucket-type mode
pass bucketType in the <code>-db-riak-options</code>:</p>
<p>To use maps for the object records set <code>useMaps</code> in the <code>-db-riak-options</code></p>
<pre><code>-db-riak-options &#39;{ &quot;bucketType&quot;: &quot;bk&quot;, &quot;useMaps&quot;: 1 }&#39;</code></pre><p>In addition to the standard commands it can execute any Riak HTTP API directly</p>
<pre><code>db.query({ op: &quot;GET&quot;, text: &quot;/buckets?buckets=true&quot; }, { pool: &quot;riak&quot; }, lib.log)
db.query({ op: &quot;POST&quot;, text: &quot;/buckets/bucket/counter/name&quot;, obj: 1 }, { pool: &quot;riak&quot; }, lib.log)</code></pre></li>
</ul>

<h2 id="module-ipc_amqp">Module: IPC_AMQP</h2>
<ul>
<li><p><code>client</code></p>
<p> Queue client using RabbitMQ server</p>
</li>
</ul>

<h2 id="module-ipc_db">Module: IPC_DB</h2>
<ul>
<li><p><code>Database tables</code></p>
<pre><code>      bk_queue: {
          id: { primary: 1 },
          active: { type: &quot;int&quot;, index: 1, value: 0 },      // job status, new, running
          data: { type: &quot;json&quot;, projections: 1 },            // job definition object
          mtime: { type: &quot;now&quot;, index: 1 }
      },
  },</code></pre><p>  };</p>
<p>  ipc.modules.push(client);</p>
<p>  client.createClient = function(url, options)
  {</p>
<pre><code>  if (url.match(/^db:/)) return new IpcDbClient(url, options);</code></pre><p>  }</p>
<p>  function IpcDbClient(url, options)
  {</p>
<pre><code>  Client.call(this, url, options);
  this.options.interval = lib.toNumber(this.options.interval, { dflt: 30, min: 1 });
  this.options.count = lib.toNumber(this.options.count, { dflt: 1, min: 1 });
  this.options.visibilityTimeout = lib.toNumber(this.options.visibilityTimeout, { min: 0, dflt: this.options.interval });
  this.emit(&quot;ready&quot;);</code></pre><p>  }
  util.inherits(IpcDbClient, Client);</p>
<p>  IpcDbClient.prototype.monitorQueue = function()
  {</p>
<pre><code>  var options = { pool: this.options.pool, updateCollect: 1, sort: &quot;active_mtime&quot;, ops: { mtime: &quot;lt&quot; }, expected: { active: 1 } };
  var query = { active: 1, mtime: Date.now() - this.options.visibilityTimeout * 1000 };

  db.updateAll(&quot;bk_queue&quot;, query, { active: 0 }, options, function(err, rows) {
      for (var i in rows) logger.error(&quot;ipc.monitor:&quot;, lib.toAge(rows[i].mtime), rows[i]);</code></pre></li>
</ul>

<h2 id="module-ipc_hazelcast">Module: IPC_HAZELCAST</h2>
<ul>
<li><p><code>client</code></p>
<p> Cache client based on HazelCast server using <a href="https://github.com/hazelcast/hazelcast-nodejs-client">https://github.com/hazelcast/hazelcast-nodejs-client</a></p>
<p>To support more than one server use either one:</p>
<p>   ipc-cache=hazelcast://host?bk-servers=host2,host3</p>
<p>   ipc-cache=memcache://host1
   ipc-cache-options-servers=host1,host2</p>
<p>To pass module specific options:</p>
<p>   ipc-cache-options-map-name=defaultMap
   ipc-cache-options-map-prefix=|</p>
</li>
</ul>

<h2 id="module-msg_sns">Module: MSG_SNS</h2>
<ul>
<li><p><code>client.send(dev, options, callback)</code></p>
<p> Send push notification to a device using AWS SNS service, device_id must be a valid SNS endpoint ARN.</p>
<p>The options may contain the following properties:</p>
<ul>
<li>msg - message text</li>
<li>badge - badge number</li>
<li>type - set type of the packet</li>
<li>id - send id in the user properties</li>
</ul>
</li>
</ul>

</div></body>
